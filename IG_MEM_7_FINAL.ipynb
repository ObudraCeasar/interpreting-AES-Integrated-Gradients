{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IG_MEM_7_FINAL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4X6HAViVdJu"
      },
      "source": [
        "\r\n",
        "# SETUP AND DEPS\r\n",
        "---\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-sQ5nW4RDV2"
      },
      "source": [
        "!rm -rf calling-out-bluff/\n",
        "!git clone https://github.com/SwapnilDreams100/calling-out-bluff.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oJTBGu57Rm5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sX7AJ0Eyahx"
      },
      "source": [
        "! pip install xhtml2pdf\n",
        "# !wget http://nlp.sta/nford.edu/data/glove.6B.zip\n",
        "! cp ./drive/My\\ Drive/glove.6B.300d.txt ./calling-out-bluff/Model5-MemoryNets/glove/\n",
        "! cp ./drive/My\\ Drive/glove.6B.300d.txt ./calling-out-bluff/Model5-MemoryNets/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi1zKL26LJ8u"
      },
      "source": [
        "%cd calling-out-bluff/Model5-MemoryNets/\n",
        "%tensorflow_version 1.x\n",
        "import data_utils\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from qwk import quadratic_weighted_kappa\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "early_stop_count = 0\n",
        "max_step_count = 10\n",
        "is_regression = True\n",
        "gated_addressing = False\n",
        "essay_set_id = 7\n",
        "batch_size = 15\n",
        "embedding_size = 300\n",
        "feature_size = 100\n",
        "l2_lambda = 0.3\n",
        "hops = 3\n",
        "reader = 'bow' # gru may not work\n",
        "epochs = 100\n",
        "num_samples = 1\n",
        "num_tokens = 42\n",
        "test_batch_size = batch_size\n",
        "random_state = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTgVHKBOVzWK"
      },
      "source": [
        "# MODEL ARCH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaHPoIhEMmLA"
      },
      "source": [
        "if is_regression:\n",
        "    from memn2n_kv_regression import MemN2N_KV\n",
        "else:\n",
        "    from memn2n_kv import MemN2N_KV\n",
        "# print flags info\n",
        "orig_stdout = sys.stdout\n",
        "timestamp = time.strftime(\"%b_%d_%Y_%H:%M:%S\", time.localtime())\n",
        "folder_name = '{}'.format(essay_set_id)\n",
        "out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs/\", folder_name))\n",
        "if not os.path.exists(out_dir):\n",
        "    os.makedirs(out_dir)\n",
        "\n",
        "# save output to a file\n",
        "#f = file(out_dir+'/out.txt', 'w')\n",
        "#sys.stdout = f\n",
        "print(\"Writing to {}\\n\".format(out_dir))\n",
        "\n",
        "# hyper-parameters end here\n",
        "training_path = 'training_set_rel3.tsv'\n",
        "essay_list, resolved_scores, essay_id = data_utils.load_training_data(training_path, essay_set_id)\n",
        "\n",
        "max_score = max(resolved_scores)\n",
        "min_score = min(resolved_scores)\n",
        "if essay_set_id == 7:\n",
        "    min_score, max_score = 0, 30\n",
        "elif essay_set_id == 8:\n",
        "    min_score, max_score = 0, 60\n",
        "\n",
        "print( 'max_score is {} \\t min_score is {}\\n'.format(max_score, min_score))\n",
        "with open(out_dir+'/params', 'a') as f:\n",
        "    f.write('max_score is {} \\t min_score is {} \\n'.format(max_score, min_score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzizYq6tNYvA"
      },
      "source": [
        "# if essay_set_id == 7:\n",
        "#     min_score, max_score = 0, 30\n",
        "# elif essay_set_id == 8:\n",
        "#     min_score, max_score = 0, 60\n",
        "\n",
        "# score_range = range(min_score, max_score+1)\n",
        "\n",
        "# #word_idx, _ = data_utils.build_vocab(essay_list, vocab_limit)\n",
        "\n",
        "# load glove\n",
        "import data_utils\n",
        "word_idx, word2vec = data_utils.load_glove(42, dim=300)\n",
        "\n",
        "with open('tokenizer.pkl', 'wb') as f:\n",
        "  pickle.dump( word_idx,f)\n",
        "\n",
        "# vocab_size = len(word_idx) + 1\n",
        "# # stat info on data set\n",
        "\n",
        "# sent_size_list = list(map(len, [essay for essay in essay_list]))\n",
        "# # print(\"sent size list\", sent_size_list)\n",
        "# max_sent_size = max(sent_size_list)\n",
        "# mean_sent_size = int(np.mean(sent_size_list))\n",
        "\n",
        "# print( 'max sentence size: {} \\nmean sentence size: {}\\n'.format(max_sent_size, mean_sent_size))\n",
        "# with open(out_dir+'/params', 'a') as f:\n",
        "#     f.write('max sentence size: {} \\nmean sentence size: {}\\n'.format(max_sent_size, mean_sent_size))\n",
        "\n",
        "# print( 'The length of score range is {}'.format(len(score_range)))\n",
        "# E = data_utils.vectorize_data(essay_list, word_idx, max_sent_size)\n",
        "# # print(vocab_size)\n",
        "# labeled_data = zip(E, resolved_scores, sent_size_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yg1RBn6wNueN"
      },
      "source": [
        "def train_step(m, e, s, ma):\n",
        "    start_time = time.time()\n",
        "    feed_dict = {\n",
        "        model._query: e,\n",
        "        model._memory_key: m,\n",
        "        model._score_encoding: s,\n",
        "        model._mem_attention_encoding: ma,\n",
        "        model.keep_prob: 0.9\n",
        "        #model.w_placeholder: word2vec\n",
        "    }\n",
        "    _, step, predict_op, cost = sess.run([train_op, global_step, model.predict_op, model.cost], feed_dict)\n",
        "    end_time = time.time()\n",
        "    time_spent = end_time - start_time\n",
        "    return predict_op, cost, time_spent\n",
        "\n",
        "def test_step(e, m):\n",
        "    feed_dict = {\n",
        "        model._query: e,\n",
        "        model._memory_key: m,\n",
        "        model.keep_prob: 1\n",
        "        #model.w_placeholder: word2vec\n",
        "    }\n",
        "    preds, mem_attention_probs = sess.run([model.predict_op, model.mem_attention_probs], feed_dict)\n",
        "    if is_regression:\n",
        "        preds = np.clip(np.round(preds), min_score, max_score)\n",
        "        return preds, mem_attention_probs\n",
        "    else:\n",
        "        return preds, mem_attention_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z3jOAx8N_2z"
      },
      "source": [
        "fold_count = 0\n",
        "kf = KFold(n_splits=5, random_state=random_state)\n",
        "best_kappa_scores = []\n",
        "for train_index, test_index in kf.split(essay_id):\n",
        "    early_stop_count = 0\n",
        "    fold_count += 1\n",
        "    if fold_count>=2:\n",
        "      break\n",
        "    trainE = []\n",
        "    testE = []\n",
        "    train_scores = []\n",
        "    test_scores = []\n",
        "    train_essay_id = []\n",
        "    test_essay_id = []\n",
        "\n",
        "    for ite in train_index:\n",
        "        trainE.append(E[ite])\n",
        "        train_scores.append(resolved_scores[ite])\n",
        "        train_essay_id.append(essay_id[ite])\n",
        "    for ite in test_index:\n",
        "        testE.append(E[ite])\n",
        "        test_scores.append(resolved_scores[ite])\n",
        "        test_essay_id.append(essay_id[ite])\n",
        "    \n",
        "    memory = []\n",
        "    memory_score = []\n",
        "    memory_sent_size = []\n",
        "    memory_essay_ids = []\n",
        "    # pick sampled essay for each score\n",
        "    for i in score_range:\n",
        "    # test point: limit the number of samples in memory for 8\n",
        "        for j in range(num_samples):\n",
        "            if i in train_scores:\n",
        "                score_idx = train_scores.index(i)\n",
        "                score = train_scores.pop(score_idx)\n",
        "                essay = trainE.pop(score_idx)\n",
        "                sent_size = sent_size_list.pop(score_idx)\n",
        "                memory.append(essay)\n",
        "                memory_score.append(score)\n",
        "                memory_essay_ids.append(train_essay_id.pop(score_idx))\n",
        "                memory_sent_size.append(sent_size)\n",
        "    memory_size = len(memory)\n",
        "    if is_regression:\n",
        "    # bad naming\n",
        "        train_scores_encoding = train_scores\n",
        "    else:\n",
        "        train_scores_encoding = list(map(lambda x: score_range.index(x), train_scores))\n",
        "    \n",
        "    # data size\n",
        "    n_train = len(trainE)\n",
        "    n_test = len(testE)\n",
        "\n",
        "    print( 'The size of training data: {}'.format(n_train))\n",
        "    print( 'The size of testing data: {}'.format(n_test))\n",
        "    with open(out_dir+'/params{}'.format(fold_count), 'a') as f:\n",
        "        f.write('The size of training data: {}\\n'.format(n_train))\n",
        "        f.write('The size of testing data: {}\\n'.format(n_test))\n",
        "        f.write('\\nEssay scores in memory:\\n{}'.format(memory_score))\n",
        "        f.write('\\nEssay ids in memory:\\n{}'.format(memory_essay_ids))\n",
        "        f.write('\\nEssay ids in training:\\n{}'.format(train_essay_id))\n",
        "        f.write('\\nEssay ids in testing:\\n{}'.format(test_essay_id))\n",
        "\n",
        "    batches = zip(range(0, n_train-batch_size, batch_size), range(batch_size, n_train, batch_size))\n",
        "    batches = [(start, end) for start, end in batches]\n",
        "    print(batches)\n",
        "    x = 1\n",
        "    if x == 1:\n",
        "        with tf.Graph().as_default():\n",
        "            session_conf = tf.ConfigProto(\n",
        "                allow_soft_placement=True,\n",
        "                log_device_placement=False)\n",
        "\n",
        "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "            # decay learning rate\n",
        "            starter_learning_rate = 0.0001\n",
        "            learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 3000, 0.96, staircase=True)\n",
        "\n",
        "            # test point\n",
        "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=0.1)\n",
        "            best_kappa_so_far = 0.0\n",
        "            with tf.Session(config=session_conf) as sess:\n",
        "                model = MemN2N_KV(batch_size, vocab_size, max_sent_size, max_sent_size, memory_size,\n",
        "                                  memory_size, embedding_size, len(score_range), feature_size, hops, reader, l2_lambda)\n",
        "                grads_and_vars = optimizer.compute_gradients(model.loss_op, aggregation_method=tf.AggregationMethod.EXPERIMENTAL_TREE)\n",
        "                grads_and_vars = [(tf.clip_by_norm(g, 10.0), v)\n",
        "                                  for g, v in grads_and_vars if g is not None]\n",
        "                #grads_and_vars = [(add_gradient_noise(g, 1e-4), v) for g, v in grads_and_vars]\n",
        "                train_op = optimizer.apply_gradients(grads_and_vars, name=\"train_op\", global_step=global_step)\n",
        "                sess.run(tf.global_variables_initializer(), feed_dict={model.w_placeholder: word2vec})\n",
        "                saver = tf.train.Saver(tf.global_variables(), max_to_keep=1)\n",
        "\n",
        "                for i in range(1, epochs+1):\n",
        "                    train_cost = 0\n",
        "                    total_time = 0\n",
        "                    np.random.shuffle(batches)\n",
        "                    for start, end in batches:\n",
        "                        e = trainE[start:end]\n",
        "                        s = train_scores_encoding[start:end]\n",
        "                        s_num = train_scores[start:end]\n",
        "                        #batched_memory = []\n",
        "                        # batch sized memory\n",
        "                        #for _ in range(len(e)):\n",
        "                        #    batched_memory.append(memory)\n",
        "                        mem_atten_encoding = []\n",
        "                        for ite in s_num:\n",
        "                            mem_encoding = np.zeros(memory_size)\n",
        "                            for j_idx, j in enumerate(memory_score):\n",
        "                                if j == ite:\n",
        "                                    mem_encoding[j_idx] = 1\n",
        "                            mem_atten_encoding.append(mem_encoding)\n",
        "                        batched_memory = [memory] * (end-start)\n",
        "                        _, cost, time_spent = train_step(batched_memory, e, s, mem_atten_encoding)\n",
        "                        total_time += time_spent\n",
        "                        train_cost += cost\n",
        "                    print( 'Finish epoch {}, total training cost is {}, time spent is {}'.format(i, train_cost, total_time))\n",
        "                    # evaluation\n",
        "                    if i % 5 == 0 or i == 200:\n",
        "                        # test on training data\n",
        "                        train_preds = []\n",
        "                        for start in range(0, n_train, test_batch_size):\n",
        "                            end = min(n_train, start+test_batch_size)\n",
        "\n",
        "                            #batched_memory = []\n",
        "                            #for _ in range(end-start):\n",
        "                            #    batched_memory.append(memory)\n",
        "                            batched_memory = [memory] * (end-start)\n",
        "    #                         print(\"BM\", len(batched_memory))\n",
        "                            preds, _ = test_step(trainE[start:end], batched_memory)\n",
        "                            if type(preds) is np.float32:\n",
        "                                train_preds.append(preds)\n",
        "                            else:\n",
        "                                for ite in preds:\n",
        "                                    train_preds.append(ite)\n",
        "                        if not is_regression:\n",
        "                            train_preds = np.add(train_preds, min_score)\n",
        "                        #train_kappp_score = kappa(train_scores, train_preds, 'quadratic')\n",
        "                        train_kappp_score = quadratic_weighted_kappa(\n",
        "                            train_scores, train_preds, min_score, max_score)\n",
        "                        # test on test data\n",
        "                        test_preds = []\n",
        "                        test_atten_probs = []\n",
        "                        for start in range(0, n_test, test_batch_size):\n",
        "                            end = min(n_test, start+test_batch_size)\n",
        "\n",
        "                            #batched_memory = []\n",
        "                            #for _ in range(end-start):\n",
        "                            #    batched_memory.append(memory)\n",
        "                            batched_memory = [memory] * (end-start)\n",
        "    #                         print(\"Test\", len(testE[start:end]))\n",
        "                            \n",
        "                            preds, mem_attention_probs = test_step(testE[start:end], batched_memory)\n",
        "\n",
        "\n",
        "                            # preds2, explanations = explain_step(testE[start:end], batched_memory)\n",
        "                            # print(preds2, len(explanations), (explanations[0]).shape)\n",
        "                            if type(preds) is np.float32:\n",
        "                                test_preds.append(preds)\n",
        "                            else:\n",
        "                                for ite in preds:\n",
        "                                    test_preds.append(ite)\n",
        "                            for ite in mem_attention_probs:\n",
        "                                test_atten_probs.append(ite)\n",
        "                        if not is_regression:\n",
        "                            test_preds = np.add(test_preds, min_score)\n",
        "                        #test_kappp_score = kappa(test_scores, test_preds, 'quadratic')\n",
        "                        \n",
        "                        ##### STORE TEST DATA\n",
        "                        t = []\n",
        "                        m = []\n",
        "                        for start in range(0, n_test, test_batch_size):\n",
        "                            end = min(n_test, start+test_batch_size)\n",
        "                            batched_memory = [memory] * (end-start)\n",
        "                            t.append(testE[start:end])\n",
        "                            m.append(batched_memory)\n",
        "                        \n",
        "                        with open(str(essay_set_id)+'_'+str(fold_count)+'.pkl', 'wb') as f:\n",
        "                          pickle.dump((t,m), f)\n",
        "                        #########################\n",
        "                        test_kappp_score = quadratic_weighted_kappa(\n",
        "                            test_scores, test_preds, min_score, max_score)\n",
        "                        stat_dict = {'pred_score': test_preds}\n",
        "                        stat_df = pd.DataFrame(stat_dict)\n",
        "                        # save the model if it gets best kappa\n",
        "                        if(test_kappp_score > best_kappa_so_far):\n",
        "                            early_stop_count = 0\n",
        "                            best_kappa_so_far = test_kappp_score\n",
        "                            # stats on test\n",
        "                            # stat_df.to_csv(out_dir+'/predScore_'+str(fold_count))\n",
        "                            with open(out_dir+'/mem_atten', 'a') as f:\n",
        "                                for idx, ite in enumerate(test_essay_id):\n",
        "                                    f.write('{}\\n'.format(ite))\n",
        "                                    f.write('{}\\n'.format(test_atten_probs[idx]))\n",
        "                            saver.save(sess, out_dir+'/checkpoints_'+str(fold_count), global_step)\n",
        "                            \n",
        "                        else:\n",
        "                            early_stop_count += 1\n",
        "                        print(\"Training kappa score = {}\".format(train_kappp_score))\n",
        "                        print(\"Testing kappa score = {}\".format(test_kappp_score))\n",
        "                        with open(out_dir+'/eval_'.format(fold_count), 'a') as f:\n",
        "                            f.write(\"Training kappa score = {}\\n\".format(train_kappp_score))\n",
        "                            f.write(\"Testing kappa score = {}\\n\".format(test_kappp_score))\n",
        "                            f.write(\"Best Testing kappa score so far = {}\\n\".format(best_kappa_so_far))\n",
        "                            f.write('*'*10)\n",
        "                            f.write('\\n')\n",
        "                    if early_stop_count > max_step_count:\n",
        "                        break\n",
        "                best_kappa_scores.append(best_kappa_so_far)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1rl01419tWZ"
      },
      "source": [
        "# INFERENCE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZBjozECi01B"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "%cd /content/calling-out-bluff/Model5-MemoryNets/\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import numpy as np\n",
        "from IPython.display import display, HTML\n",
        "# from xhtml2pdf import pisa\n",
        "import gc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzRVPEa4rZVe"
      },
      "source": [
        "fold_no = 1\n",
        "step = 15170\n",
        "essay_set_id = 7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K0SWvSVsa53"
      },
      "source": [
        "ATTRS_DIR = '/content/drive/My Drive/IG RESULTS/MEMORY NET/P'+str(essay_set_id)+'/'\n",
        "ATTRS_TSV = '/content/drive/My Drive/IG RESULTS/MEMORY NET/P'+str(essay_set_id)+'/attrs.tsv'\n",
        "\n",
        "def convert_html_to_pdf(source_html, output_filename):\n",
        "  # open output file for writing (truncated binary)\n",
        "  result_file = open(output_filename, \"w+b\")\n",
        "\n",
        "  # convert HTML to PDF\n",
        "  pisa_status = pisa.CreatePDF(\n",
        "          source_html,                # the HTML to convert\n",
        "          dest=result_file)           # file handle to recieve result\n",
        "\n",
        "  # close output file\n",
        "  result_file.close()                 # close output file\n",
        "\n",
        "  # return False on success and True on errors\n",
        "  return pisa_status.err   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mXLl4KN6jaN"
      },
      "source": [
        "# GET DATA AND MEMEORY OF VALIDATION SET\n",
        "with open('/content/drive/My Drive/IG RESULTS/MEM MODELS/'+str(essay_set_id)+'_'+str(fold_no)+'.pkl', 'rb') as f:\n",
        "  (t,m,test_scores) = pickle.load(f)\n",
        "  \n",
        "# LOAD TRAINED MODEL\n",
        "saver = tf.train.import_meta_graph(\"/content/drive/My Drive/IG RESULTS/MEM MODELS/\"+str(essay_set_id)+\"/checkpoints_\"+str(fold_no)+\"-\"+str(step)+\".meta\")  \n",
        "sess=tf.Session()\n",
        "saver.restore(sess,\"/content/drive/My Drive/IG RESULTS/MEM MODELS/\"+str(essay_set_id)+\"/checkpoints_\"+str(fold_no)+\"-\"+str(step))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMQtCdRBWOSh"
      },
      "source": [
        "# IG CLASS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OfFy2AcJDt3"
      },
      "source": [
        "# INSPIRED BY : https://github.com/ankurtaly/Integrated-Gradients\n",
        "\n",
        "class integrated_gradients:\n",
        "  def __init__(self, graph, sess, min, tokenizer_file = 'tokenizer.pkl',batch_size = 20, num_reps = 20):\n",
        "    \n",
        "    self.graph = graph\n",
        "    self.sess = sess\n",
        "    w1 = self.graph.get_tensor_by_name(\"input/essay:0\")\n",
        "    w2 = self.graph.get_tensor_by_name(\"input/memory_key:0\")\n",
        "    w3 = self.graph.get_tensor_by_name(\"input/keep_prob:0\")\n",
        "    self.batch_size = batch_size\n",
        "    self.min = min\n",
        "    self.INPUT_TENSORS = [w1,w2,w3]\n",
        "    self.num_reps = num_reps\n",
        "\n",
        "    with open(tokenizer_file, 'rb') as f:\n",
        "      tokenizer = pickle.load(f)\n",
        "    self.reverse_word_map = dict(map(reversed, tokenizer.items()))\n",
        "      \n",
        "    self.OUTPUT_TENSOR =  self.graph.get_tensor_by_name('prediction/Squeeze:0')\n",
        "    self.PRED_TENSOR =  self.graph.get_tensor_by_name('prediction/Squeeze:0')\n",
        "    self.EMBEDDING_TENSOR =  self.graph.get_tensor_by_name('embedding_lookup/Identity:0')\n",
        "    GRADIENT_TENSOR = tf.gradients(self.OUTPUT_TENSOR, self.EMBEDDING_TENSOR)\n",
        "    self.get_gradients = {}\n",
        "    self.get_gradients[0] = GRADIENT_TENSOR\n",
        "  \n",
        "  def get_tensor(self, name):\n",
        "    return self.graph.get_tensor_by_name(name)\n",
        "\n",
        "  def generate_baseline(self, sample):\n",
        "    baseline = np.zeros(sample.shape)\n",
        "    return baseline\n",
        "\n",
        "  def _get_feed_dict(self, input_df):\n",
        "      feed = {}\n",
        "      for i, key in enumerate(self.INPUT_TENSORS):\n",
        "          feed[key.name] = [input_df[i]]\n",
        "      return feed\n",
        "\n",
        "  def _get_ig_error(self, integrated_gradients, baseline_prediction, prediction,\n",
        "                  debug=False):\n",
        "      sum_attributions = 0\n",
        "      sum_attributions += np.sum(integrated_gradients)\n",
        "\n",
        "      delta_prediction = prediction - baseline_prediction\n",
        "\n",
        "      error_percentage = \\\n",
        "          100 * (delta_prediction - sum_attributions) / delta_prediction\n",
        "      if debug:\n",
        "          print(f'prediction is {prediction}')\n",
        "          print(f'baseline_prediction is {baseline_prediction}')\n",
        "          print(f'delta_prediction is {delta_prediction}')\n",
        "          print(f'sum_attributions are {sum_attributions}')\n",
        "          print(f'Error percentage is {error_percentage}')\n",
        "\n",
        "      return error_percentage\n",
        "\n",
        "  def _get_scaled_inputs(self, input_val, baseline_val, num_reps):\n",
        "      list_scaled_embeddings = []\n",
        "      scaled_embeddings = \\\n",
        "          [baseline_val + (float(i) / (num_reps * self.batch_size - 1)) *\n",
        "          (input_val - baseline_val) for i in range(0, num_reps * self.batch_size)]\n",
        "\n",
        "      for i in range(num_reps):\n",
        "          list_scaled_embeddings.append(\n",
        "              np.array(scaled_embeddings[i * self.batch_size:i * self.batch_size +\n",
        "                                                        self.batch_size]))\n",
        "      return np.array(list_scaled_embeddings)\n",
        "\n",
        "  def _get_unscaled_inputs(self, input_val):\n",
        "      unscaled_embeddings = [input_val] * self.batch_size\n",
        "\n",
        "      return np.array(unscaled_embeddings)\n",
        "\n",
        "  def _calculate_integral(self, ig):\n",
        "      ig = (ig[:-1] + ig[1:]) / 2.0  # trapezoidal rule\n",
        "      integral = np.average(ig, axis=0)\n",
        "      return integral\n",
        "\n",
        "  def explain(self, x, memory, max_allowed_error=5, debug=False):\n",
        "      num_reps = self.num_reps\n",
        "      baseline = self.generate_baseline(np.array(x))\n",
        "      # baseline_memory = self.generate_baseline(np.array(memory))\n",
        "      inp = [x, memory, 1]\n",
        "      base = [baseline, memory, 1]\n",
        "      \n",
        "      # pred = self.predict(inp)\n",
        "      c =  0\n",
        "      # print('channel:', c)\n",
        "      \n",
        "      attributions, baseline_prediction, prediction = \\\n",
        "          self._compute_ig(inp, base, c, num_reps=num_reps) #### MAIN FUNC\n",
        "      if debug:\n",
        "        error_percentage = \\\n",
        "            self._get_ig_error(attributions, baseline_prediction, prediction,\n",
        "                          debug=debug)\n",
        "\n",
        "      # while abs(error_percentage) > max_allowed_error:\n",
        "      #     num_reps += 5\n",
        "      #     if debug:\n",
        "      #         print(f'Num reps is {num_reps}, abs error percentage is '\n",
        "      #               f'{error_percentage}')\n",
        "      #     integrated_gradients, baseline_prediction, prediction = \\\n",
        "      #         self._compute_ig(inp, base, c, num_reps=num_reps)\n",
        "      #     error_percentage = \\\n",
        "      #         self._get_ig_error(integrated_gradients, baseline_prediction,\n",
        "      #                       prediction, debug=debug)\n",
        "\n",
        "      igs = attributions.astype('float')\n",
        "      words = self.sequence_to_text(inp[0])\n",
        "      assert len(igs) == len(words)\n",
        "      # html_code = self.visualize_token_attrs(words , igs)\n",
        "      return igs, words\n",
        "  \n",
        "  def predict(self, inp):\n",
        "      pred = self.sess.run(self.PRED_TENSOR,\n",
        "                              self._get_feed_dict(inp))\n",
        "      pred = np.clip(np.round(pred), 0, 30)\n",
        "      return pred\n",
        "\n",
        "  def _get_feed_dict_batch(self, input_df):\n",
        "      feed = {}\n",
        "      for i, key in enumerate(self.INPUT_TENSORS):\n",
        "          feed[key.name] = input_df[i]\n",
        "      return feed\n",
        "\n",
        "  def predict_batch(self, inp):\n",
        "      pred = self.sess.run(self.PRED_TENSOR,\n",
        "                              self._get_feed_dict_batch(inp))\n",
        "      pred = [np.clip(np.round(x), 0, 30) for x in pred]\n",
        "      return pred\n",
        "  \n",
        "  def visualize_token_attrs(self, tokens, attrs):\n",
        "    def get_color(attr):\n",
        "        if attr > 0:\n",
        "            g = int(128*attr) + 127\n",
        "            b = 128 - int(64*attr)\n",
        "            r = 128 - int(64*attr)\n",
        "        else:\n",
        "            g = 128 + int(64*attr)\n",
        "            b = 128 + int(64*attr)\n",
        "            r = int(-128*attr) + 127\n",
        "        return r,g,b\n",
        "    import matplotlib as mpl\n",
        "    cmap='PiYG'\n",
        "    cmap_bound = np.abs(attrs).max()\n",
        "    norm = mpl.colors.Normalize(vmin=-cmap_bound, vmax=cmap_bound)\n",
        "    cmap = mpl.cm.get_cmap(cmap)\n",
        "\n",
        "    # bound = max(abs(attrs.max()), abs(attrs.min()))\n",
        "    # attrs = attrs/bound\n",
        "    \n",
        "    html_text = \"\"\n",
        "    for i, tok in enumerate(tokens):\n",
        "        if tok is not None:\n",
        "          # r, g, b = get_color(attrs[i])\n",
        "          color = mpl.colors.rgb2hex(cmap(norm(attrs[i])))\n",
        "          # html_text += \" <mark style='background-color:rgb(%d,%d,%d)'>%s</mark>\" % \\\n",
        "          #             (r, g, b, tok)\n",
        "          html_text += \" <mark style='background-color:{}'>{}</mark>\".format(color, tok)\n",
        "        # else:\n",
        "        #   r, g, b = get_color(attrs[i])\n",
        "        #   html_text += \" <span style='color:rgb(%d,%d,%d)'>%s</span>\" % \\\n",
        "        #               (r, g, b, \"None\")\n",
        "    return (html_text)\n",
        "  \n",
        "  def sequence_to_text(self, list_of_indices):\n",
        "      words = [self.reverse_word_map.get(letter) for letter in list_of_indices]\n",
        "      return words\n",
        "\n",
        "  def _compute_ig(self, inp, base, c, num_reps):\n",
        "      tensor_values = self.sess.run(self.EMBEDDING_TENSOR,\n",
        "                              self._get_feed_dict(inp))\n",
        "\n",
        "      tensor_baseline_values = self.sess.run(self.EMBEDDING_TENSOR,\n",
        "                              self._get_feed_dict(base))\n",
        "      \n",
        "      # print(tensor_values.shape, tensor_baseline_values.shape)\n",
        "      scaled_embeddings = self._get_scaled_inputs(tensor_values[0],\n",
        "                                            tensor_baseline_values[0],\n",
        "                                            num_reps)\n",
        "      \n",
        "      # print(scaled_embeddings) # num_reps x batch_size x emb_shape\n",
        "      scaled_input_feed = {}\n",
        "      \n",
        "      for i, key in enumerate(self.INPUT_TENSORS):\n",
        "          ui = self._get_unscaled_inputs(inp[i])\n",
        "          if 'input/keep_prob:0' in key.name: ### SCALAR VALUE\n",
        "            scaled_input_feed[ self.get_tensor(key.name) ] = 1.0\n",
        "          else:\n",
        "            scaled_input_feed[ self.get_tensor(key.name) ] = ui\n",
        "      \n",
        "      scores = []\n",
        "      path_gradients = []\n",
        "      for i in range(num_reps):\n",
        "          scaled_input_feed[self.EMBEDDING_TENSOR] = scaled_embeddings[i]\n",
        "            \n",
        "          path_gradients_rep, scores_rep = self.sess.run(\n",
        "              [self.get_gradients[c], self.OUTPUT_TENSOR], scaled_input_feed)\n",
        "          \n",
        "          path_gradients.append(path_gradients_rep[0])\n",
        "          scores.append(scores_rep)\n",
        "      \n",
        "      baseline_prediction = scores[0][0]  # first score is the baseline prediction\n",
        "      baseline_prediction = np.clip(np.round(baseline_prediction), 0, 30)\n",
        "      prediction = scores[-1][-1]  # last score is the input prediction\n",
        "      prediction = np.clip(np.round(prediction), 0, 30)\n",
        "      \n",
        "      # integrating the gradients and multiplying with the difference of the baseline and input.\n",
        "      ig = np.concatenate(path_gradients, axis=0)\n",
        "      integral = self._calculate_integral(ig)\n",
        "      igs = (tensor_values[0] - tensor_baseline_values[0]) * integral\n",
        "      igs = np.sum(igs, axis=-1)\n",
        "\n",
        "      return igs, baseline_prediction, prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCfP6voRM4K1"
      },
      "source": [
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Avl9DXrwReJW"
      },
      "source": [
        "# ! cp /content/drive/My\\ Drive/IG\\ RESULTS/MEM\\ MODELS/tokenizer.pkl ./"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EWOL_LYQTlH"
      },
      "source": [
        "# saver = tf.train.import_meta_graph(\"/content/drive/My Drive/IG RESULTS/MEM MODELS/\"+str(essay_set_id)+\"/checkpoints_\"+str(fold_no)+\"-\"+str(step)+\".meta\")  \n",
        "# sess=tf.Session()\n",
        "# saver.restore(sess,\"/content/drive/My Drive/IG RESULTS/MEM MODELS/\"+str(essay_set_id)+\"/checkpoints_\"+str(fold_no)+\"-\"+str(step))\n",
        "\n",
        "# CREATE IG MODEL\n",
        "graph = tf.get_default_graph()\n",
        "IG = integrated_gradients(graph, sess, min = min_score, batch_size= 20, num_reps=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPP4N3I7P2tc"
      },
      "source": [
        "# LOADING DATA methods\n",
        "\n",
        "import re\n",
        "import os as os\n",
        "import numpy as np\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "def load_training_data(training_df):\n",
        "    resolved_score = training_df['label_orig']\n",
        "    essays = training_df['text']\n",
        "    essay_list = []\n",
        "    # turn an essay to a list of words\n",
        "    for idx, essay in essays.iteritems():\n",
        "        essay = clean_str(essay)\n",
        "        essay_list.append(tokenize(essay))\n",
        "    return essay_list, resolved_score.tolist()\n",
        "\n",
        "def tokenize(sent):\n",
        "    '''Return the tokens of a sentence including punctuation.\n",
        "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
        "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
        "    >>> tokenize('I don't know')\n",
        "    ['I', 'don', '\\'', 'know']\n",
        "    '''\n",
        "    return [x.strip() for x in re.split('(\\W+)', sent) if x.strip()]\n",
        "\n",
        "def clean_str(string):\n",
        "    \"\"\"\n",
        "    Tokenization/string cleaning for all datasets except for SST.\n",
        "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
        "    \"\"\"\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" ( \", string)\n",
        "    string = re.sub(r\"\\)\", \" ) \", string)\n",
        "    string = re.sub(r\"\\?\", \" ? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "\n",
        "    return string.strip().lower()\n",
        "\n",
        "def build_vocab(sentences, vocab_limit):\n",
        "    \"\"\"\n",
        "    Builds a vocabulary mapping from word to index based on the sentences.\n",
        "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
        "    \"\"\"\n",
        "    # Build vocabulary\n",
        "    word_counts = Counter(itertools.chain(*sentences))\n",
        "    print( 'Total size of vocab is {}'.format(len(word_counts.most_common())))\n",
        "    # Mapping from index to word\n",
        "    # vocabulary_inv = [x[0] for x in word_counts.most_common(vocab_limit)]\n",
        "    vocabulary_inv = [x[0] for x in word_counts.most_common(vocab_limit)]\n",
        "    \n",
        "    vocabulary_inv = list(sorted(vocabulary_inv))\n",
        "    # Mapping from word to index\n",
        "    vocabulary = {x: i+1 for i, x in enumerate(vocabulary_inv)}\n",
        "    return [vocabulary, vocabulary_inv]\n",
        "\n",
        "# data is DataFrame\n",
        "def vectorize_data(data, word_idx, sentence_size):\n",
        "    E = []\n",
        "    for essay in data:\n",
        "        ls = max(0, sentence_size - len(essay))\n",
        "        wl = []\n",
        "        for w in essay:\n",
        "            if w in word_idx:\n",
        "                wl.append(word_idx[w])\n",
        "            else:\n",
        "                wl.append(0)\n",
        "        wl += [0]*ls\n",
        "        E.append(wl)\n",
        "    return E"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUG1RXCUXVt7"
      },
      "source": [
        "# Load adversarial data and compute attributions + statistics on SMALL DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gy_p6YS5PSbm"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "names = ['song_beg', 'song_end', 'false_beg','false_end','normal','shuffle', 'syn']\n",
        "adv_data_list = {}\n",
        "for name in names:\n",
        "  adv_data = pd.read_csv('/content/drive/My Drive/IG RESULTS/'+str(essay_set_id)+'_'+name+'.csv')\n",
        "  adv_data_list[name] = adv_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6kjz1CCPWN5"
      },
      "source": [
        "E_list = {}\n",
        "M_list = {}\n",
        "for adv_data in adv_data_list.keys():\n",
        "  essay_list, resolved_scores = load_training_data(adv_data_list[adv_data])\n",
        "  E = data_utils.vectorize_data(essay_list, word_idx, len(t[0][0]))\n",
        "  E_list[adv_data]= E\n",
        "  M_list[adv_data] = m[0][:len(E)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQdKagXdPQ5t"
      },
      "source": [
        "e   = E_list['normal']\n",
        "mem = M_list['normal']\n",
        "inp = [e,mem,1.0]\n",
        "labels_orig = IG.predict_batch(inp)\n",
        "labels_orig = [int(x) for x in labels_orig]\n",
        "labels_orig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txVUyuycPRR_"
      },
      "source": [
        "from xhtml2pdf import pisa\n",
        "def convert_html_to_pdf(source_html, output_filename):\n",
        "  result_file = open(output_filename, \"w+b\")\n",
        "  pisa_status = pisa.CreatePDF(source_html, dest=result_file)          \n",
        "  result_file.close()\n",
        "\n",
        "def save_stats_add(diff, diff_attr, word_list, ratio,output_filename):\n",
        "  result = open(output_filename, 'w')\n",
        "  result.write('\\ndiff in scores:  '+str(diff))\n",
        "  result.write('\\ndiff in attrs:  '+ str(diff_attr))\n",
        "  result.write('\\nnew words in top 10%:  '+ ', '.join(word_list))\n",
        "  result.write('\\npercent of top words in added words:  '+ str(ratio))\n",
        "  result.close()\n",
        "\n",
        "def save_stats_mod(diff, diff_attr, changed_no,output_filename):\n",
        "  result = open(output_filename, 'w')\n",
        "  result.write('\\ndiff in scores:  '+str(diff))\n",
        "  result.write('\\ndiff in attrs:  '+ str(diff_attr))\n",
        "  result.write('\\nno of words which changed attr:  '+ str(changed_no))\n",
        "  result.close()\n",
        "\n",
        "def save_stats_gen(babel_total, babel_unattrib, output_filename):\n",
        "  result = open(output_filename, 'w')\n",
        "  result.write('\\ntop attributed words:  '+ str(babel_total))\n",
        "  result.write('\\ntop unattributed words:  '+ str(babel_unattrib))\n",
        "  result.close()\n",
        "\n",
        "def top_k_attrs(tokens, attrs, sign = None, k=None):\n",
        "    k = min(k, len(tokens))\n",
        "    \n",
        "    if sign != None:\n",
        "      tokens_list = []\n",
        "      signs_list = []\n",
        "      for i in np.argpartition(attrs, -k)[-k:]:\n",
        "          tokens_list.append(tokens[i].strip())\n",
        "          signs_list.append(sign[i])\n",
        "      return  tokens_list , signs_list\n",
        "    \n",
        "    else:\n",
        "      return ([tokens[i].strip() for i in np.argpartition(attrs, -k)[-k:]])\n",
        "\n",
        "def bottom_k_attrs(tokens, attrs, sign = None, k=None):\n",
        "    k = min(k, len(tokens))\n",
        "\n",
        "    if sign != None:\n",
        "      tokens_list = []\n",
        "      signs_list = []\n",
        "      for i in np.argpartition(attrs, k)[:k]:\n",
        "          tokens_list.append(tokens[i].strip())\n",
        "          signs_list.append(sign[i])\n",
        "      return  tokens_list , signs_list\n",
        "    \n",
        "    else:\n",
        "      return ([tokens[i].strip() for i in np.argpartition(attrs, k)[:k]])\n",
        "\n",
        "def save_normal_attrs(data, memory, labels_orig, essay_type, is_small = True):\n",
        "  dir =  ATTRS_DIR+essay_type+'/'\n",
        "  if not os.path.exists(dir):\n",
        "    os.makedirs(dir)\n",
        "\n",
        "  a_total = []\n",
        "  w_total= [] \n",
        "  for i,essay in enumerate(data):\n",
        "    attrs, words= IG.explain(x = data[i][:659], memory= memory[0])\n",
        "    label_new = IG.predict([data[i][:659], memory[0],1.0])\n",
        "    if is_small:\n",
        "      html = IG.visualize_token_attrs(words, attrs)\n",
        "      convert_html_to_pdf(html, dir+str(i)+'_'+str(labels_orig[i])+'_'+str(int(label_new))+'.pdf')\n",
        "    a_total.append(attrs)\n",
        "    w_total.append(words)\n",
        "\n",
        "  return a_total, w_total\n",
        "\n",
        "def subfinder(l, sl):\n",
        "    sll=len(sl)\n",
        "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
        "        if l[ind:ind+sll]==sl:\n",
        "            return ind,ind+sll-1\n",
        "\n",
        "def save_attrs_pdf(data, data_normal, memory, labels_orig, essay_type, type_add = False, type_mod = False, type_gen = False):\n",
        "  dir =  ATTRS_DIR+essay_type+'/'\n",
        "  if not os.path.exists(dir):\n",
        "    os.makedirs(dir)\n",
        "\n",
        "  a,w = data_normal\n",
        "  \n",
        "  pattern_none = [None, None, None, None, None]\n",
        "  \n",
        "  babel_total = {}\n",
        "  babel_unattrib = {}\n",
        "  \n",
        "  for i,essay in enumerate(data):\n",
        "    attrs, words= IG.explain(x = data[i][:659], memory= memory[0])\n",
        "    labels_new = int(IG.predict([data[i][:659], memory[0], 1.0]))\n",
        "    \n",
        "    if type_add:\n",
        "        diff = labels_orig[i] - labels_new\n",
        "        \n",
        "        loc = subfinder(w[i], pattern_none)[0]\n",
        "        loc2 = subfinder(words, pattern_none)[0]\n",
        "          \n",
        "        if essay_type == 'song_beg' or essay_type =='false_beg' :\n",
        "          pattern = w[i][:5]\n",
        "          loc_patt = subfinder(words, pattern)[0]\n",
        "          new_w = words[:loc_patt]\n",
        "          new_a = attrs[:loc_patt]\n",
        "\n",
        "        elif essay_type == 'song_end' or essay_type =='false_end':\n",
        "          new_w = words[loc:loc2]\n",
        "          new_a = attrs[loc:loc2]\n",
        "\n",
        "        top_words_orig = top_k_attrs(w[i], a[i], k = int(0.1*loc))\n",
        "        top_words = top_k_attrs(words , attrs, k = int(0.1*loc2))\n",
        "\n",
        "        top_words_final = [x for x in top_words if x not in top_words_orig]\n",
        "        html = IG.visualize_token_attrs(words, attrs)\n",
        "        convert_html_to_pdf(html, dir+str(i)+'_'+str(labels_orig[i])+'_'+str(labels_new+0)+'.pdf')\n",
        "        save_stats_add(diff, diff_attr, top_words_final, len(top_words_final)/len(new_w), dir+'stats_'+str(i)+'_'+str(labels_orig[i])+'_'+str(labels_new+0)+'.pdf')\n",
        "    \n",
        "    elif type_gen:\n",
        "        try:\n",
        "          loc = subfinder(words, pattern_none)[0] \n",
        "        except Exception as e:\n",
        "          loc = len(words)\n",
        "        attrs_sign= []\n",
        "        for x in attrs:\n",
        "            if x>0:\n",
        "              attrs_sign.append('+')\n",
        "            else:\n",
        "              attrs_sign.append('-')\n",
        "\n",
        "        top_words, top_words_signs = top_k_attrs(words, attrs, attrs_sign, k = int(0.1*loc))\n",
        "        \n",
        "        for j,x in enumerate(top_words):\n",
        "          if x in babel_total.keys():\n",
        "            if top_words_signs[j] == '+':\n",
        "              babel_total[x][0]+=1\n",
        "            else:\n",
        "              babel_total[x][1]+=1\n",
        "          else:\n",
        "            if top_words_signs[j] == '+':\n",
        "              babel_total[x] = [1,0]\n",
        "            else:\n",
        "              babel_total[x] = [0,1]\n",
        "         \n",
        "        attrs_abs = [abs(x) for x in attrs]\n",
        "        bottom_words = bottom_k_attrs(words[:loc], attrs_abs[:loc], k = int(0.1*loc))\n",
        "        \n",
        "        for j,x in enumerate(bottom_words):\n",
        "          if x in babel_unattrib.keys():\n",
        "            babel_unattrib[x]+= 1\n",
        "          else:\n",
        "            babel_unattrib[x] = 1\n",
        "\n",
        "        html = IG.visualize_token_attrs(words, attrs)\n",
        "        convert_html_to_pdf(html, dir+str(i)+'_'+str(labels_new+0)+'.pdf')\n",
        "\n",
        "    elif type_mod:\n",
        "        diff = labels_orig[i] - int(labels_new)\n",
        "        diff_attr = sum(attrs) - sum(a[i])\n",
        "        \n",
        "        if essay_type == 'shuffle' or essay_type == 'syn':\n",
        "          loc = subfinder(w[i], pattern_none)[0]\n",
        "          attrs_abs=[]\n",
        "          attrs_sign =[]\n",
        "          for x in a[i]:\n",
        "            attrs_abs.append(abs(x))\n",
        "            if x>0:\n",
        "              attrs_sign.append('+')\n",
        "            else:\n",
        "              attrs_sign.append('-')\n",
        "\n",
        "          top_words_orig, token_signs_orig = top_k_attrs(w[i], attrs_abs, attrs_sign, k = int(0.1*loc))\n",
        "          \n",
        "          loc = subfinder(words, pattern_none)[0]\n",
        "          attrs_abs=[]\n",
        "          attrs_sign =[]\n",
        "          for x in attrs:\n",
        "            attrs_abs.append(abs(x))\n",
        "            if x>0:\n",
        "              attrs_sign.append('+')\n",
        "            else:\n",
        "              attrs_sign.append('-')\n",
        "          \n",
        "          top_words, token_signs = top_k_attrs(words, attrs_abs, attrs_sign, k = int(0.1*loc))\n",
        "          print(token_signs_orig, token_signs)\n",
        "          changed_count = 0\n",
        "\n",
        "          for orig_index,t in enumerate(top_words_orig):\n",
        "            try:\n",
        "              ind = top_words.index(t)\n",
        "            except Exception as e:\n",
        "              ind = -1\n",
        "\n",
        "            if ind!=-1:\n",
        "              if token_signs_orig[orig_index] != token_signs[ind]:\n",
        "                  changed_count+=1\n",
        "\n",
        "        html = IG.visualize_token_attrs(words, attrs)\n",
        "        convert_html_to_pdf(html, dir+str(i)+'_'+str(labels_orig[i])+'_'+str(labels_new+0)+'.pdf')\n",
        "        save_stats_mod(diff, diff_attr, changed_count, dir+'stats_'+str(i)+'_'+str(labels_orig[i])+'_'+str(labels_new+0)+'.pdf')\n",
        "  \n",
        "  if type_gen:\n",
        "    del babel_unattrib[None]\n",
        "    save_stats_gen(babel_total, babel_unattrib, dir+'stats_word_attributions.txt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhEe9eXxxs8u"
      },
      "source": [
        "e_normal = E_list['normal']\n",
        "m_normal = M_list['normal']\n",
        "data_normal = save_normal_attrs(e_normal , m_normal, labels_orig, 'normal')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjdy75fUxs_t"
      },
      "source": [
        "gc.collect()\n",
        "e_add_song = E_list['song_beg']\n",
        "memory = M_list['normal']\n",
        "save_attrs_pdf(e_add_song, data_normal, memory, labels_orig, 'song_beg', type_add=True)\n",
        "\n",
        "gc.collect()\n",
        "e_add_song = E_list['song_end']\n",
        "memory = M_list['normal']\n",
        "save_attrs_pdf(e_add_song, data_normal, memory, labels_orig, 'song_end', type_add=True)\n",
        "\n",
        "gc.collect()\n",
        "e_add_song = E_list['false_beg']\n",
        "memory = M_list['normal']\n",
        "save_attrs_pdf(e_add_song, data_normal, memory, labels_orig, 'false_beg', type_add=True)\n",
        "\n",
        "gc.collect()\n",
        "e_add_song = E_list['false_end']\n",
        "memory = M_list['normal']\n",
        "save_attrs_pdf(e_add_song, data_normal, memory, labels_orig, 'false_end', type_add=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eAPhWQVxtCc"
      },
      "source": [
        "gc.collect()\n",
        "e_add_song = E_list['syn']\n",
        "memory = M_list['normal']\n",
        "save_attrs_pdf(e_add_song, data_normal, memory, labels_orig, 'syn', type_mod = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLs4eiD-xtFJ"
      },
      "source": [
        "gc.collect()\n",
        "e_add_song = E_list['shuffle']\n",
        "memory = M_list['normal']\n",
        "save_attrs_pdf(e_add_song, data_normal, memory, labels_orig, 'shuffle', type_mod = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUB-3Dv7yHrz"
      },
      "source": [
        "# Babel\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "gdd.download_file_from_google_drive(file_id='1CIEpiDmzLmJ6LMCVSOmCKw_eOg4ocuS4', dest_path='/content/AES.zip', unzip=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcQVQCbsxtHx"
      },
      "source": [
        "gc.collect()\n",
        "babel_data = pd.read_csv('/content/AES_testcases/prompt'+str(essay_set_id)+'/prompt 7 babel - Sheet1.csv', names= ['text'])\n",
        "babel_data['label_orig'] = min_score\n",
        "\n",
        "essay_list, resolved_scores = load_training_data(babel_data)\n",
        "E_babel = data_utils.vectorize_data(essay_list, word_idx, len(t[0][0]))\n",
        "\n",
        "e_add_song = E_babel\n",
        "memory = M_list['normal']\n",
        "save_attrs_pdf(e_add_song, data_normal, memory, labels_orig, 'babel', type_gen = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5eSTdyn7Bnw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zdf6QEvooF1A"
      },
      "source": [
        "# Load adversarial attributions + statistics on BIG DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rJMlb2bPkHb"
      },
      "source": [
        "ATTRS_DIR = '/content/drive/MyDrive/IG RESULTS/MEMORY NET/P7/'\r\n",
        "\r\n",
        "### NORMAL\r\n",
        "path = ATTRS_DIR + 'attrs_normal.tsv'\r\n",
        "w = []\r\n",
        "a = []\r\n",
        "\r\n",
        "with open(path, 'r') as f:\r\n",
        "  for line in f:\r\n",
        "      line = line.strip()\r\n",
        "      question_attrs = line.split('\\t')[0]\r\n",
        "      question_tokens = []\r\n",
        "      attrs = []\r\n",
        "      for word_attr in question_attrs.split('||'): \r\n",
        "          word, attr = word_attr.split('|')\r\n",
        "          question_tokens.append(word)\r\n",
        "          attrs.append(float(attr))\r\n",
        "      question_tokens = question_tokens\r\n",
        "      attrs = attrs + [0]*(659 - len(attrs))\r\n",
        "\r\n",
        "      w.append(question_tokens)\r\n",
        "      a.append(attrs)\r\n",
        "w,a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxh2JX0E7Bkb"
      },
      "source": [
        "def save_stats_add(diff, diff_attr, percent, output_filename):\r\n",
        "  result = open(output_filename, 'w')\r\n",
        "  result.write('\\ndiff in scores:  '+str(diff))\r\n",
        "  result.write('\\ndiff in attrs:  '+ str(diff_attr))\r\n",
        "  result.write('\\npercent of top words in added words:  '+ str(ratio))\r\n",
        "  result.close()\r\n",
        "\r\n",
        "def save_stats_mod_shuffle(diff, diff_attr, changed_no,output_filename):\r\n",
        "  result = open(output_filename, 'w')\r\n",
        "  result.write('\\ndiff in scores:  '+str(diff))\r\n",
        "  result.write('\\ndiff in attrs:  '+ str(diff_attr))\r\n",
        "  result.write('\\nno of top words which changed attr:  '+ str(changed_no))\r\n",
        "  result.close()\r\n",
        "\r\n",
        "def save_stats_mod_syn(diff, diff_attr, changed_top_no,changed_bottom_no, top_words, bottom_words, output_filename):\r\n",
        "  result = open(output_filename, 'w')\r\n",
        "  result.write('\\n diff in scores:  '+str(diff))\r\n",
        "  result.write('\\n diff in attrs:  '+ str(diff_attr))\r\n",
        "  result.write('\\n no of top words which changed attr:  '+ str(changed_top_no))\r\n",
        "  result.write('\\n no of bottom words which changed attr:  '+ str(changed_bottom_no))\r\n",
        "  result.write('\\n top words which changed attr:  '+ str(top_words))\r\n",
        "  result.write('\\n bottom words which changed attr:  '+ str(bottom_words))\r\n",
        "  result.close()\r\n",
        "\r\n",
        "def save_big_attrs_pdf(memory, essay_type, type_add = False, type_mod = False, type_gen = False):\r\n",
        "  dir =  ATTRS_DIR+essay_type+'/'\r\n",
        "  if not os.path.exists(dir):\r\n",
        "    os.makedirs(dir)\r\n",
        "\r\n",
        "  pattern_none = [None, None, None, None, None]\r\n",
        "  \r\n",
        "  if type_add:\r\n",
        "    diff_array = [] \r\n",
        "    diff_attr_array = []\r\n",
        "    percent_array = []\r\n",
        "\r\n",
        "  if type_mod:\r\n",
        "    diff_array = []\r\n",
        "    diff_attr_array = []\r\n",
        "\r\n",
        "    changed_count_array = []\r\n",
        "\r\n",
        "    changed_count_top_array = []\r\n",
        "    changed_count_bottom_array = []\r\n",
        "    changed_top_words = []\r\n",
        "    changed_bottom_words = []\r\n",
        "\r\n",
        "  ATTRS_DIR = '/content/drive/MyDrive/IG RESULTS/MEMORY NETS/P7/'\r\n",
        "\r\n",
        "  ### NORMAL\r\n",
        "  path = ATTRS_DIR + 'attrs_normal.tsv'\r\n",
        "  w = []\r\n",
        "  a = []\r\n",
        "\r\n",
        "  with open(path, 'r') as f:\r\n",
        "    for line in f:\r\n",
        "        line = line.strip()\r\n",
        "        question_attrs = line.split('\\t')[0]\r\n",
        "        question_tokens = []\r\n",
        "        attrs = []\r\n",
        "        for word_attr in question_attrs.split('||'): \r\n",
        "            word, attr = word_attr.split('|')\r\n",
        "            question_tokens.append(word)\r\n",
        "            attrs.append(float(attr))\r\n",
        "        question_tokens = question_tokens\r\n",
        "        attrs = attrs + [0]*(659 - len(attrs))\r\n",
        "\r\n",
        "        w.append(question_tokens)\r\n",
        "        a.append(attrs)\r\n",
        "\r\n",
        "  ### ADV\r\n",
        "  path = ATTRS_DIR+ 'attrs_'+essay_type+'.tsv'\r\n",
        "  data = []\r\n",
        "  with open(path, 'r') as f:\r\n",
        "    for line in f:\r\n",
        "        line = line.strip()\r\n",
        "        question_attrs = line.split('\\t')[0]\r\n",
        "        question_tokens = []\r\n",
        "        attrs = []\r\n",
        "        for word_attr in question_attrs.split('||'): \r\n",
        "            word, attr = word_attr.split('|')\r\n",
        "            question_tokens.append(word)\r\n",
        "            attrs.append(float(attr))\r\n",
        "        question_tokens = question_tokens\r\n",
        "        attrs = attrs + [0]*(659 - len(attrs))\r\n",
        "\r\n",
        "        data.append((attrs, question_tokens))\r\n",
        "        print(len(attrs))\r\n",
        "\r\n",
        "  for i,essay in enumerate(data):\r\n",
        "    attrs, words= essay\r\n",
        "    E = vectorize_data([words], word_idx, 659)\r\n",
        "  \r\n",
        "    labels_new = int(IG.predict([E[0], memory[0],1.0]))\r\n",
        "    \r\n",
        "    if type_add:\r\n",
        "        loc = subfinder(w[i], pattern_none)[0]\r\n",
        "        loc2 = subfinder(words, pattern_none)[0]\r\n",
        "          \r\n",
        "        if essay_type == 'song_beg' or essay_type =='false_beg' :\r\n",
        "          pattern = w[i][:5]\r\n",
        "          loc_patt = subfinder(words, pattern)[0]\r\n",
        "          new_w = words[:loc_patt]\r\n",
        "          new_a = attrs[:loc_patt]\r\n",
        "\r\n",
        "        elif essay_type == 'song_end' or essay_type =='false_end':\r\n",
        "          new_w = words[loc:loc2]\r\n",
        "          new_a = attrs[loc:loc2]\r\n",
        "\r\n",
        "        top_words_orig = top_k_attrs(w[i], a[i], k = int(0.2*loc))\r\n",
        "        top_words = top_k_attrs(words , attrs, k = int(0.2*loc2))\r\n",
        "\r\n",
        "        top_words_final = [x for x in top_words if x not in top_words_orig]\r\n",
        "\r\n",
        "        diff = int(labels_new) - labels_orig[i]\r\n",
        "        diff_array.append(diff)\r\n",
        "\r\n",
        "        diff_attr = sum(new_a) - sum(a[i][:len(new_a)])\r\n",
        "        diff_attr_array.append(diff_attr)\r\n",
        "\r\n",
        "        percent = len(top_words_final)/len(new_w)\r\n",
        "        percent_array.append(percent)\r\n",
        "    \r\n",
        "    elif type_mod:\r\n",
        "        diff = int(labels_new) - labels_orig[i]\r\n",
        "        diff_array.append(diff)\r\n",
        "\r\n",
        "        diff_attr = sum(attrs) - sum(a[i])\r\n",
        "        diff_attr_array.append(diff_attr)\r\n",
        "\r\n",
        "        if essay_type == 'syn':\r\n",
        "          # normal\r\n",
        "          loc = subfinder(w[i], pattern_none)[0]\r\n",
        "          \r\n",
        "          attrs_abs=[]\r\n",
        "          attrs_sign =[]\r\n",
        "          for x in a[i]:\r\n",
        "            attrs_abs.append(abs(x))\r\n",
        "            if x>0:\r\n",
        "              attrs_sign.append('+')\r\n",
        "            else:\r\n",
        "              attrs_sign.append('-')\r\n",
        "\r\n",
        "          top_words_orig, token_signs_orig = top_k_attrs(w[i], attrs_abs, attrs_sign, k = int(0.2*loc))\r\n",
        "          bottom_words_orig, bottom_token_signs_orig = bottom_k_attrs(w[i], attrs_abs, attrs_sign, k = int(0.2*loc))\r\n",
        "          \r\n",
        "          # adv\r\n",
        "          loc = subfinder(words, pattern_none)[0]\r\n",
        "          attrs_abs=[]\r\n",
        "          attrs_sign =[]\r\n",
        "          for x in attrs:\r\n",
        "            attrs_abs.append(abs(x))\r\n",
        "            if x>0:\r\n",
        "              attrs_sign.append('+')\r\n",
        "            else:\r\n",
        "              attrs_sign.append('-')\r\n",
        "          \r\n",
        "          top_words, token_signs = top_k_attrs(words, attrs_abs, attrs_sign, k = int(0.2*loc))\r\n",
        "          bottom_words, bottom_token_signs = bottom_k_attrs(words, attrs_abs, attrs_sign, k = int(0.2*loc))\r\n",
        "\r\n",
        "          changed_count_top = 0\r\n",
        "          changed_count_bottom = 0\r\n",
        "          \r\n",
        "          # top    \r\n",
        "          for orig_index,t in enumerate(top_words_orig):\r\n",
        "            try:\r\n",
        "              ind = top_words.index(t)\r\n",
        "            except Exception as e:\r\n",
        "              ind = -1\r\n",
        "\r\n",
        "            if ind!=-1:\r\n",
        "              if token_signs_orig[orig_index] != token_signs[ind]:\r\n",
        "                  changed_count_top+=1\r\n",
        "                  changed_top_words.append(token_signs[ind])\r\n",
        "          \r\n",
        "          # bottom\r\n",
        "          for orig_index,t in enumerate(bottom_words_orig):\r\n",
        "            try:\r\n",
        "              ind = bottom_words.index(t)\r\n",
        "            except Exception as e:\r\n",
        "              ind = -1\r\n",
        "\r\n",
        "            if ind!=-1:\r\n",
        "              if bottom_token_signs_orig[orig_index] != bottom_token_signs[ind]:\r\n",
        "                  changed_count_bottom+=1\r\n",
        "                  changed_bottom_words.append(bottom_token_signs[ind])\r\n",
        "          \r\n",
        "          changed_count_top_array.append(changed_count_top)\r\n",
        "          changed_count_bottom_array.append(changed_count_bottom)\r\n",
        "          \r\n",
        "        if essay_type == 'shuffle':\r\n",
        "          loc = subfinder(w[i], pattern_none)[0]\r\n",
        "          attrs_abs=[]\r\n",
        "          attrs_sign =[]\r\n",
        "          for x in a[i]:\r\n",
        "            attrs_abs.append(abs(x))\r\n",
        "            if x>0:\r\n",
        "              attrs_sign.append('+')\r\n",
        "            else:\r\n",
        "              attrs_sign.append('-')\r\n",
        "\r\n",
        "          top_words_orig, token_signs_orig = top_k_attrs(w[i], attrs_abs, attrs_sign, k = int(0.2*loc))\r\n",
        "          \r\n",
        "          loc = subfinder(words, pattern_none)[0]\r\n",
        "          attrs_abs=[]\r\n",
        "          attrs_sign =[]\r\n",
        "          for x in attrs:\r\n",
        "            attrs_abs.append(abs(x))\r\n",
        "            if x>0:\r\n",
        "              attrs_sign.append('+')\r\n",
        "            else:\r\n",
        "              attrs_sign.append('-')\r\n",
        "          \r\n",
        "          top_words, token_signs = top_k_attrs(words, attrs_abs, attrs_sign, k = int(0.2*loc))\r\n",
        "          \r\n",
        "          changed_count = 0\r\n",
        "\r\n",
        "          for orig_index,t in enumerate(top_words_orig):\r\n",
        "            try:\r\n",
        "              ind = top_words.index(t)\r\n",
        "            except Exception as e:\r\n",
        "              ind = -1\r\n",
        "\r\n",
        "            if ind!=-1:\r\n",
        "              if token_signs_orig[orig_index] != token_signs[ind]:\r\n",
        "                  changed_count+=1\r\n",
        "          changed_count_array.append(changed_count)\r\n",
        "\r\n",
        "  def Average(lst): \r\n",
        "    return sum(lst) / len(lst) \r\n",
        "  \r\n",
        "  if type_add:\r\n",
        "    save_stats_add(Average(diff_array), Average(diff_attr_array), Average(percent_array),       dir+'stats_'+essay_type+'.pdf')\r\n",
        "\r\n",
        "  if type_mod and essay_type == 'syn':\r\n",
        "    save_stats_mod_syn(Average(diff_array), Average(diff_attr_array), Average(changed_count_top_array), \\\r\n",
        "                   Average(changed_count_bottom_array), Counter(changed_top_words),  Counter(changed_bottom_words), \\\r\n",
        "                   dir+'stats_'+essay_time+'.pdf')\r\n",
        "\r\n",
        "  if type_mod and essay_type == 'shuffle':\r\n",
        "    save_stats_mod_shuffle(Average(diff_array), Average(diff_attr_array), Average(changed_count_array), dir+'stats_'+essay_time+'.pdf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jyfrmUv7Bgz"
      },
      "source": [
        "save_big_attrs_pdf(m[0], 'song_beg', type_add=True)\r\n",
        "save_big_attrs_pdf(m[0], 'song_end', type_add=True)\r\n",
        "save_big_attrs_pdf(m[0], 'false_beg', type_add=True)\r\n",
        "save_big_attrs_pdf(m[0], 'false_end', type_add=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJXPclKb7Bdp"
      },
      "source": [
        "save_big_attrs_pdf(m[0], 'shuffle', type_mod=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bvx1dF87BZ9"
      },
      "source": [
        "save_big_attrs_pdf(m[0], 'syn', type_mod=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMDvDvVL7BWp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BT40BNfb6_-R"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_liPUYA6_7F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZiCC6GsoO4m"
      },
      "source": [
        "# Compute other statistics on NORMAL ATTRIBUTIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amSkrLmqxtM2"
      },
      "source": [
        "# GET TOP AND BOTTOM ATTRIBUTED WORDS\n",
        "def top_k_attrs(tokens, attrs, sign = None, k=None):\n",
        "    k = min(k, len(tokens))\n",
        "    \n",
        "    if sign != None:\n",
        "      tokens_list = []\n",
        "      signs_list = []\n",
        "      for i in np.argpartition(attrs, -k)[-k:]:\n",
        "          tokens_list.append(tokens[i].strip())\n",
        "          signs_list.append(sign[i])\n",
        "      return  tokens_list , signs_list\n",
        "    \n",
        "    else:\n",
        "      return ([tokens[i].strip() for i in np.argpartition(attrs, -k)[-k:]])\n",
        "\n",
        "# ATTRS_TSV = '/content/drive/My Drive/IG RESULTS/P1/MEMORY NET/attrs.tsv'\n",
        "lines = []\n",
        "with open(ATTRS_TSV) as f:\n",
        "    for line in f:\n",
        "      lines.append(line)\n",
        "lines = list(set(lines))\n",
        "\n",
        "def get_counts_list_normal( top_k = 10, is_abs = False, is_sign = False, is_percent = None):\n",
        "  \n",
        "  essay_list = []\n",
        "  counts_list = []\n",
        "  signs_list = []\n",
        "  for line in lines:\n",
        "      line = line.strip()\n",
        "      question_attrs = line.split('\\t')[0]\n",
        "      question_tokens = []\n",
        "      attrs = []\n",
        "      for word_attr in question_attrs.split('||'): \n",
        "          word, attr = word_attr.split('|')\n",
        "          question_tokens.append(word)\n",
        "          if is_abs:\n",
        "            attrs.append(abs(float(attr)))\n",
        "          else:\n",
        "            attrs.append(float(attr))\n",
        "\n",
        "      essay_list.append(question_tokens)\n",
        "      if is_percent!=None:\n",
        "        top_k = int(is_percent*len(question_tokens))\n",
        "  \n",
        "      if top_k == None:\n",
        "        k = len(question_tokens)\n",
        "      else:\n",
        "        k = min(top_k, len(question_tokens))\n",
        "      \n",
        "      if is_sign:\n",
        "        signs = []\n",
        "\n",
        "        for i in attrs:\n",
        "          if i>0:\n",
        "            signs.append('+')\n",
        "          else:\n",
        "            signs.append('-')\n",
        "      # get top k words by attribution \n",
        "      c_list, sign_list = top_k_attrs(question_tokens , attrs, signs,  k = k)\n",
        "      counts_list.extend(c_list)\n",
        "      signs_list.extend(sign_list)\n",
        "\n",
        "  return counts_list, signs_list, essay_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiHdLF7YxtQS"
      },
      "source": [
        "counts_list, signs_list, essay_list = get_counts_list_normal(is_percent = 0.1, is_sign = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFVq163m1aJH"
      },
      "source": [
        "signs_dict= {}\n",
        "frequent_attributions = Counter(counts_list).most_common(50)\n",
        "for i in frequent_attributions:\n",
        "  w = i[0]\n",
        "  for j,w2 in enumerate(counts_list):\n",
        "    if w ==w2 :\n",
        "      if w not in signs_dict.keys():\n",
        "        signs_dict[w]=[]\n",
        "      else:\n",
        "        signs_dict[w].append(signs_list[j])\n",
        "\n",
        "for k,v in signs_dict.items():\n",
        "  signs_dict[k] = Counter(v)\n",
        "\n",
        "with open(ATTRS_DIR+'NORMAL_word_importance.txt','w') as f:\n",
        "  f.write(str(signs_dict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fwj3vd_a1aMY"
      },
      "source": [
        "###NORMAL UNATTRIBUTED STUFF:\n",
        "def bottom_k_attrs(tokens, attrs, k=None):\n",
        "    k = min(k, len(tokens))    \n",
        "    return ([tokens[i].strip() for i in np.argpartition(attrs, k)[:k]])\n",
        "\n",
        "def neg_k_attrs(tokens, attrs, k=None):\n",
        "    k = min(k, len(tokens))    \n",
        "    return ([tokens[i].strip() for i in np.argpartition(attrs, k)[:k] if attrs[i]<0 ])\n",
        "\n",
        "lines = []\n",
        "with open(ATTRS_TSV) as f:\n",
        "    for line in f:\n",
        "      lines.append(line)\n",
        "lines = list(set(lines))\n",
        "\n",
        "def get_bottom_list_normal( top_k = 10, is_abs = True, is_percent = None, is_neg= False):\n",
        "  \n",
        "  counts_list = []\n",
        "  signs_list = []\n",
        "  for line in lines:\n",
        "      line = line.strip()\n",
        "      question_attrs = line.split('\\t')[0]\n",
        "      question_tokens = []\n",
        "      attrs = []\n",
        "      for word_attr in question_attrs.split('||'): \n",
        "          word, attr = word_attr.split('|')\n",
        "          question_tokens.append(word)\n",
        "          if is_abs:\n",
        "            attrs.append(abs(float(attr)))\n",
        "          else:\n",
        "            attrs.append(float(attr))\n",
        "\n",
        "      if is_percent!=None:\n",
        "        top_k = int(is_percent*len(question_tokens))\n",
        "  \n",
        "      if top_k == None:\n",
        "        k = len(question_tokens)\n",
        "      else:\n",
        "        k = min(top_k, len(question_tokens))\n",
        "      \n",
        "      # get top k words by attribution \n",
        "      if is_neg:\n",
        "        c_list = neg_k_attrs(question_tokens , attrs, k = k)\n",
        "      else:\n",
        "        c_list = bottom_k_attrs(question_tokens , attrs, k = k)\n",
        "      counts_list.extend(c_list)\n",
        "      \n",
        "  return counts_list\n",
        "\n",
        "counts_list_unattrib = get_bottom_list_normal(is_percent = 0.1)\n",
        "counts_list_neg = get_bottom_list_normal(is_percent = 0.1, is_abs = False, is_neg = True)\n",
        "\n",
        "with open(ATTRS_DIR+'NORMAL_bottom_attributed_words.txt','w') as f:\n",
        "  f.write(str(Counter(counts_list_unattrib).most_common(50)))\n",
        "with open(ATTRS_DIR+'NORMAL_negative_attributed_words.txt','w') as f:\n",
        "  f.write(str(Counter(counts_list_neg).most_common(50)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghdNmz9-1aPU"
      },
      "source": [
        "def plot_and_save(curve_data, filename, x = 'num. words in vocab', y= 'relative accuracy', title= 'title'):\n",
        "  import matplotlib.pyplot as plt\n",
        "  OVERSTABILITY_CURVE_FILE = filename\n",
        "  plt.plot(list(curve_data.keys()), list(curve_data.values()))\n",
        "  # plt.xscale('symlog')\n",
        "  plt.xlabel(x)\n",
        "  plt.ylabel(y)\n",
        "  plt.title(title)\n",
        "\n",
        "  plt.savefig(OVERSTABILITY_CURVE_FILE, format='eps')\n",
        "  plt.savefig(OVERSTABILITY_CURVE_FILE.replace('eps','png'), format='png')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "def plot_and_save_both(a,b, filename, x = 'num. words in vocab', y= 'relative accuracy', title= 'title'):\n",
        "  import matplotlib.pyplot as plt\n",
        "  OVERSTABILITY_CURVE_FILE = filename\n",
        "  plt.plot(a,b)\n",
        "  # plt.xscale('symlog')\n",
        "  ax = plt.gca()\n",
        "  ax.invert_xaxis()\n",
        "  plt.xlabel(x)\n",
        "  plt.title(title)\n",
        "  plt.ylabel(y)\n",
        "  plt.savefig(OVERSTABILITY_CURVE_FILE, format='eps')\n",
        "  plt.savefig(OVERSTABILITY_CURVE_FILE.replace('eps','png'), format='png')\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtsYwpez1aZS"
      },
      "source": [
        "### NORMAL ATTR - VARIATION PLOTS\n",
        "pattern_none = [None, None, None, None, None]\n",
        "data = e_normal\n",
        "memory = m_normal\n",
        "for i in range(len(data)):\n",
        "  attrs, words = IG.explain(data[i][:659], memory = memory[0])\n",
        "  loc = subfinder(words, pattern_none)[0]\n",
        "  attrs = attrs[:loc]\n",
        "  attrs = [abs(x) for x in attrs]\n",
        "  words = words[:loc]\n",
        "  s= {}\n",
        "  l = len(attrs)\n",
        "  for j in range(0, l, l//5):\n",
        "    s[(j)] = sum(attrs[j: j+ l//5])\n",
        "  \n",
        "  dir = ATTRS_DIR+'normal/'\n",
        "  plot_and_save(s, dir+str(i)+'_attrs_variation.txt', x= 'word number', y='absolute attribution')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZtwoaTZ1am9"
      },
      "source": [
        "with open('/content/drive/My Drive/IG RESULTS/MEM MODELS/tokenizer.pkl', 'rb') as f:\n",
        "    word_to_idx = pickle.load(f)\n",
        "\n",
        "idx_to_word = {v: k for k, v in word_to_idx.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I5OQZ2X1akh"
      },
      "source": [
        "lines = []\n",
        "with open(ATTRS_TSV) as f:\n",
        "    for line in f:\n",
        "      lines.append(line)\n",
        "lines = list(set(lines))\n",
        "\n",
        "def get_essay_list(tsv_lines):\n",
        "  \n",
        "  essay_list = []\n",
        "  attrs_list = []\n",
        "  l = []\n",
        "  for line in tsv_lines:\n",
        "      line = line.strip()\n",
        "      question_attrs = line.split('\\t')[0]\n",
        "      question_tokens = []\n",
        "      attrs = []\n",
        "      for word_attr in question_attrs.split('||'): \n",
        "          word, attr = word_attr.split('|')\n",
        "          question_tokens.append(word)\n",
        "          attrs.append(float(attr))\n",
        "      l.append(len(question_tokens))\n",
        "      question_tokens_idx = [word_to_idx[x] for x in question_tokens] + [0]*(659 - len(question_tokens))\n",
        "      essay_list.append(question_tokens_idx)\n",
        "      attrs_list.append(attrs)\n",
        "\n",
        "  return essay_list, attrs_list\n",
        "\n",
        "essay_list, attrs_list = get_essay_list(lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qvssfBl1ah-"
      },
      "source": [
        "w1 = graph.get_tensor_by_name(\"input/essay:0\")\n",
        "w2 = graph.get_tensor_by_name(\"input/memory_key:0\")\n",
        "w3 = graph.get_tensor_by_name(\"input/keep_prob:0\")\n",
        "INPUT_TENSORS = [w1,w2,w3]\n",
        "PRED_TENSOR = graph.get_tensor_by_name('prediction/Squeeze:0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i63Yvi_t1acy"
      },
      "source": [
        "mem_total = []\n",
        "for i in range(len(essay_list)):\n",
        "  mem_total.append(m[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fd0ebeK12NOk"
      },
      "source": [
        "feed = {}\n",
        "pred_array_orig = []\n",
        "input_df = [np.array(essay_list), np.array(mem_total), 1]\n",
        "for i, key in enumerate(INPUT_TENSORS):\n",
        "    feed[key.name] = input_df[i]\n",
        "pred = sess.run(PRED_TENSOR,feed)\n",
        "pred_array_orig.extend(pred)\n",
        "pred_array_orig = [int(x) for x in pred_array_orig]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wxd-LA6o2NMR"
      },
      "source": [
        "def npos(orig, new):\n",
        "  count = 0\n",
        "  for i in range(len(orig)):\n",
        "    if new[i]>orig[i]:\n",
        "      count+=1 \n",
        "  return (count/len(orig))*100\n",
        "\n",
        "def nneg(orig, new):\n",
        "  count = 0\n",
        "  for i in range(len(orig)):\n",
        "    if new[i]<orig[i]:\n",
        "      count+=1 \n",
        "  return (count/len(orig))*100\n",
        "\n",
        "def nsame(orig, new):\n",
        "  count = 0\n",
        "  for i in range(len(orig)):\n",
        "    if new[i]==orig[i]:\n",
        "      count+=1 \n",
        "  return (count/len(orig))*100\n",
        "\n",
        "def mu(orig, new):\n",
        "  s=0\n",
        "  n = len(orig)\n",
        "  for i in range(n):\n",
        "    s+=(orig[i] - new[i])\n",
        "  return (s/n)/30\n",
        "\n",
        "def absmu(orig, new):\n",
        "  s=0\n",
        "  n = len(orig)\n",
        "  for i in range(n):\n",
        "    s+=(orig[i] - new[i])\n",
        "  return (abs(s)/n)/30\n",
        "\n",
        "def sd(orig, new):\n",
        "  mu_val = mu(orig, new)\n",
        "  s=0\n",
        "  n = len(orig)\n",
        "  for i in range(n):\n",
        "    s+=(orig[i] - new[i] - mu_val)**2\n",
        "  return ((s/n)**(1/2))/30\n",
        "\n",
        "def mupos(orig, new):\n",
        "  s=0\n",
        "  n = len(orig)\n",
        "  for i in range(n):\n",
        "    if new[i]>orig[i]:\n",
        "      s+=(orig[i] - new[i])\n",
        "  return (s/n)/30\n",
        "\n",
        "def muneg(orig, new):\n",
        "  s=0\n",
        "  n = len(orig)\n",
        "  for i in range(n):\n",
        "    if orig[i] > new[i]:\n",
        "      s+=-(orig[i] - new[i])\n",
        "  return (s/n)/30\n",
        "\n",
        "def get_pred_stats(orig, new, filename, K):\n",
        "  b = ('NPOS',  npos(orig, new))\n",
        "  c = ('NNEG',  nneg(orig, new))\n",
        "  d = ('NSAME', nsame(orig, new))\n",
        "  e = ('MU',    mu(orig, new))\n",
        "  f = ('ABSMU', absmu(orig, new))\n",
        "  g = ('SD',    sd(orig, new))\n",
        "  h = ('MUPOS', mupos(orig, new))\n",
        "  i = ('MUNEG', muneg(orig, new))\n",
        "  with open(filename, 'a') as file:\n",
        "    file.write(str(K)+\"___\"+str([b,c,d,e,f,g,h,i])+ \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygfK61NopSK9"
      },
      "source": [
        "from xhtml2pdf import pisa\r\n",
        "\r\n",
        "def convert_html_to_pdf(source_html, output_filename):\r\n",
        "  result_file = open(output_filename, \"w+b\")\r\n",
        "  pisa_status = pisa.CreatePDF(source_html, dest=result_file)          \r\n",
        "  result_file.close()\r\n",
        "\r\n",
        "def save_attrs(data, memory, K, essay_type):\r\n",
        "  dir =  ATTRS_DIR+essay_type+'/'\r\n",
        "  if not os.path.exists(dir):\r\n",
        "    os.makedirs(dir)\r\n",
        "  \r\n",
        "  attrs, words= IG.explain(x = data[:659], memory= memory[0])\r\n",
        "  label_new = IG.predict([data[:659], memory[0],1.0])\r\n",
        "  html = IG.visualize_token_attrs(words, attrs)\r\n",
        "  convert_html_to_pdf(html, dir+str(K)+'_'+str(int(label_new))+'.pdf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLEXCzFP2NKO"
      },
      "source": [
        "# TOP WORDS ADDITION GRAPH\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from random import randint\n",
        "\n",
        "import numpy as np\n",
        "def top_k_attrs(tokens, attrs,k=None):\n",
        "    k = min(k, len(tokens))\n",
        "    return ([tokens[i] for i in np.argpartition(attrs, -k)[-k:]])\n",
        "\n",
        "d = {}\n",
        "p_list = {}\n",
        "for K in range(0,6):\n",
        "  \n",
        "  percent = K*0.2\n",
        "  preds_new = []\n",
        "  new_essay_list = []\n",
        "  avg_len = 0\n",
        "  c_list_total = []\n",
        "\n",
        "  # chosen_id = randint(0, 10)  \n",
        "  for id, essay in enumerate(essay_list):\n",
        "    \n",
        "    top_k = int(percent* len(attrs_list[id]))\n",
        "    attrs = attrs_list[id]\n",
        "    question_tokens = essay_list[id]\n",
        "    try:\n",
        "      c_list = top_k_attrs(question_tokens , attrs, k = top_k)\n",
        "    except Exception as e:\n",
        "      c_list = list(set(question_tokens))\n",
        "    if top_k==0:\n",
        "      c_list = []\n",
        "    c_list_total.extend(c_list)\n",
        "    new_essay = []\n",
        "    count = 0\n",
        "    for i in range(len(question_tokens)):  \n",
        "      if question_tokens[i] in c_list and count<top_k:\n",
        "        new_essay.append(question_tokens[i])\n",
        "        count+=1\n",
        "    \n",
        "    # print(len(new_essay)/  len(question_tokens[:len(attrs)])  )\n",
        "    new_essay = new_essay + [0]*(659 - len(new_essay))\n",
        "    # if id == chosen_id and K>1 and K<5:\n",
        "    #   save_attrs(new_essay, mem_total, K, essay_type='incomplete_data')\n",
        "    new_essay_list.append(new_essay)\n",
        "    # input_df = [np.array([new_essay]), np.array([mem_total[0]]), 1]\n",
        "    # for i, key in enumerate(INPUT_TENSORS):\n",
        "    #     feed[key.name] = input_df[i]\n",
        "    # pred = sess.run(PRED_TENSOR,feed)\n",
        "    # print(pred_array_orig[id],pred)\n",
        "    # # print(abs(pred - pred_array_orig[id]))\n",
        "    # if round(abs(round(pred) - pred_array_orig[id])) == 1:\n",
        "    #   if id not in p_list.keys():\n",
        "    #     p_list[id] = percent*100\n",
        "    #   else:\n",
        "    #     pass\n",
        "\n",
        "  avg_len/=len(new_essay_list)\n",
        "  c_len = len(set(c_list_total))\n",
        "  feed = {}\n",
        "  input_df = [np.array(new_essay_list), np.array(mem_total), 1]\n",
        "  for i, key in enumerate(INPUT_TENSORS):\n",
        "      feed[key.name] = input_df[i]\n",
        "  pred = sess.run(PRED_TENSOR,feed)\n",
        "  preds_new.extend(pred)\n",
        "  preds_new = [int(x) for x in preds_new]\n",
        "\n",
        "  acc = cohen_kappa_score(preds_new, pred_array_orig, weights='quadratic')\n",
        "  get_pred_stats(pred_array_orig, preds_new, ATTRS_DIR+'stats_top.txt', int(percent*100))\n",
        "  d[percent*100] = acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMu5Z5GqWKog"
      },
      "source": [
        "l = p_list.values()\r\n",
        "sum(l)/len(l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jCMK8eT2NHw"
      },
      "source": [
        "d\n",
        "plot_and_save(d,ATTRS_DIR+'adding_top', x = '% length of response', y ='relative QWK', title= 'iteratively adding words(in order of importance)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIGWEb_41aT7"
      },
      "source": [
        "# BOTTOM WORDS REMOVAL GRAPH\n",
        "\n",
        "def bottom_k_attrs(tokens, attrs,k=None):\n",
        "    k = min(k, len(tokens))\n",
        "    return ([tokens[i] for i in np.argpartition(attrs, k)[:k]])\n",
        "\n",
        "d2_keys = []\n",
        "d2_vals = []\n",
        "for K in range(0,6):\n",
        "  percent = K*0.2\n",
        "  preds_new = []\n",
        "  new_essay_list = []\n",
        "  avg_len = 0\n",
        "  c_list_total = []\n",
        "  for id, essay in enumerate(essay_list):\n",
        "    top_k = int(percent* len(attrs_list[id]))\n",
        "    attrs = attrs_list[id]\n",
        "    question_tokens = essay_list[id]\n",
        "    attrs = [abs(x) for x in attrs]\n",
        "    try:\n",
        "      c_list = bottom_k_attrs(question_tokens , attrs, k = top_k)\n",
        "    except Exception as e:\n",
        "      c_list = list(set(question_tokens))\n",
        "    if top_k==0:\n",
        "      c_list = []\n",
        "\n",
        "    # c_list_total.extend(c_list)\n",
        "    new_essay = []\n",
        "    count = 0\n",
        "    for i in range(len(question_tokens[:len(attrs)])):  \n",
        "      if question_tokens[:len(attrs)][i] in c_list and count<top_k:\n",
        "        count+=1\n",
        "        pass\n",
        "      else:\n",
        "        new_essay.append( question_tokens[:len(attrs)][i])\n",
        "    # avg_len+=len(new_essay)\n",
        "    print(len(new_essay)/  len(question_tokens[:len(attrs)]) , new_essay, question_tokens[:len(attrs)])\n",
        "    \n",
        "    new_essay = new_essay + [0]*(659 - len(new_essay))\n",
        "    new_essay_list.append(new_essay)\n",
        "\n",
        "    # input_df = [np.array([new_essay]), np.array([mem_total[0]]), 1]\n",
        "    # for i, key in enumerate(INPUT_TENSORS):\n",
        "    #     feed[key.name] = input_df[i]\n",
        "    # pred = sess.run(PRED_TENSOR,feed)\n",
        "    # # print(abs(pred - pred_array_orig[id]))\n",
        "    # if round(abs(pred - pred_array_orig[id])) == 1:\n",
        "    #   if id not in p_list.keys():\n",
        "    #     p_list[id] = percent*100\n",
        "    #   else:\n",
        "    #     pass\n",
        "\n",
        "  # avg_len/=len(new_essay_list)\n",
        "  # c_len = len(set(c_list_total))\n",
        "  feed = {}\n",
        "  input_df = [np.array(new_essay_list), np.array(mem_total), 1]\n",
        "  for i, key in enumerate(INPUT_TENSORS):\n",
        "      feed[key.name] = input_df[i]\n",
        "  pred = sess.run(PRED_TENSOR,feed)\n",
        "  preds_new.extend(pred)\n",
        "  preds_new = [int(x) for x in preds_new]\n",
        "  \n",
        "  acc = cohen_kappa_score(preds_new, pred_array_orig, weights='quadratic')\n",
        "  get_pred_stats(pred_array_orig, preds_new, ATTRS_DIR+'stats_bottom.txt', int(100-percent*100))\n",
        "  # print(percent*100, avg_len, acc)\n",
        "  d2_keys.append(100-percent*100)\n",
        "  d2_vals.append(acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjKI2m2eZzQW"
      },
      "source": [
        "l = p_list.values()\r\n",
        "sum(l)/len(l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MP0xdSNxtTL"
      },
      "source": [
        "plot_and_save_both(d2_keys, d2_vals, ATTRS_DIR+'removing_bottom', x = '% length of response', y ='relative QWK', title= 'iteratively removing words(in reverse order of importance)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a9Da0pg3KSX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1RDGLdwkOqi"
      },
      "source": [
        "GENERATE ATTRIBUTIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAklSYZU3KZn"
      },
      "source": [
        "import os\r\n",
        "import gc\r\n",
        "import multiprocessing\r\n",
        "\r\n",
        "def create_model_and_train(mem, attr_type = 'add'):\r\n",
        "  ATTRS_DIR = '/content/drive/MyDrive/IG RESULTS/'\r\n",
        "  ATTRS_TSV = ATTRS_DIR+'MEMORY NET/P7/attrs_'+attr_type+'.tsv'\r\n",
        "  \r\n",
        "  saver = tf.train.import_meta_graph(\"/content/drive/MyDrive/IG RESULTS/MEM MODELS/\"+str(essay_set_id)+\"/checkpoints_\"+str(fold_no)+\"-\"+str(step)+\".meta\")  \r\n",
        "  sess=tf.Session()\r\n",
        "  saver.restore(sess,\"/content/drive/MyDrive/IG RESULTS/MEM MODELS/\"+str(essay_set_id)+\"/checkpoints_\"+str(fold_no)+\"-\"+str(step))\r\n",
        "\r\n",
        "  graph = tf.get_default_graph()\r\n",
        "  IG = integrated_gradients(graph, sess, min = 0, batch_size= 20, num_reps=40)\r\n",
        "\r\n",
        "  data = pd.read_csv(ATTRS_DIR+'big_7_'+attr_type+'.csv')\r\n",
        "  essay_list, resolved_scores = load_training_data(data)\r\n",
        "  E = data_utils.vectorize_data(essay_list, word_idx, 659)\r\n",
        "  \r\n",
        "  with open(ATTRS_TSV, 'w') as outf:\r\n",
        "    ans = ''\r\n",
        "    for i,v in enumerate(E):\r\n",
        "        tsv_string = ''\r\n",
        "        attrs, words= IG.explain(x = E[i][:659], memory= mem)\r\n",
        "\r\n",
        "        question_attrs = []\r\n",
        "        for ind in range(len(words)):\r\n",
        "          if words[ind] != None and str(attrs[ind])!=None:\r\n",
        "            question_attrs.append(\r\n",
        "                '|'.join([ words[ind], str(attrs[ind]) ])\r\n",
        "                )\r\n",
        "        tsv_string = ['||'.join(question_attrs)]\r\n",
        "        ans += '\\t'.join(tsv_string) + '\\n'\r\n",
        "        del attrs, words, question_attrs, tsv_string\r\n",
        "        gc.collect()\r\n",
        "    \r\n",
        "    outf.write(ans)\r\n",
        "    outf.flush()\r\n",
        "    del ans\r\n",
        "    gc.collect()\r\n",
        "    outf.write('done')\r\n",
        "  print('DONE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jxb8o5PgcJq"
      },
      "source": [
        "attr_type = 'normal'\r\n",
        "p = multiprocessing.Process(target = create_model_and_train, args = (m[0][0], attr_type))\r\n",
        "p.start()\r\n",
        "p.join()\r\n",
        "p.terminate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdXJylpvJFX-"
      },
      "source": [
        "attr_type = 'song_beg'\r\n",
        "p = multiprocessing.Process(target = create_model_and_train, args = (m[0][0], attr_type))\r\n",
        "p.start()\r\n",
        "p.join()\r\n",
        "p.terminate()\r\n",
        "\r\n",
        "gc.collect()\r\n",
        "\r\n",
        "attr_type = 'song_end'\r\n",
        "p = multiprocessing.Process(target = create_model_and_train, args = (m[0][0], attr_type))\r\n",
        "p.start()\r\n",
        "p.join()\r\n",
        "p.terminate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RPnLmXg-o7y"
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eydr11oZJFVM"
      },
      "source": [
        "attr_type = 'false_beg'\r\n",
        "p = multiprocessing.Process(target = create_model_and_train, args = (m[0][0], attr_type))\r\n",
        "p.start()\r\n",
        "p.join()\r\n",
        "p.terminate()\r\n",
        "\r\n",
        "gc.collect()\r\n",
        "\r\n",
        "attr_type = 'false_end'\r\n",
        "p = multiprocessing.Process(target = create_model_and_train, args = (m[0][0], attr_type))\r\n",
        "p.start()\r\n",
        "p.join()\r\n",
        "p.terminate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Rd9AjZG-lbc"
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Nb-CqNG-lYo"
      },
      "source": [
        "attr_type = 'shuffle'\r\n",
        "p = multiprocessing.Process(target = create_model_and_train, args = (m[0][0], attr_type))\r\n",
        "p.start()\r\n",
        "p.join()\r\n",
        "p.terminate()\r\n",
        "\r\n",
        "gc.collect()\r\n",
        "\r\n",
        "attr_type = 'syn'\r\n",
        "p = multiprocessing.Process(target = create_model_and_train, args = (m[0][0], attr_type))\r\n",
        "p.start()\r\n",
        "p.join()\r\n",
        "p.terminate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XNsh-O5-lVf"
      },
      "source": [
        "# SYNONYM AND SHUFFLING STATS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGGI1L9sSpv9"
      },
      "source": [
        "######### SYNONYM DIFF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vodzFdhCRZV9"
      },
      "source": [
        "data = pd.read_csv(ATTRS_DIR+'big_7_syn.csv')\r\n",
        "essay_list, resolved_scores = load_training_data(data)\r\n",
        "E = data_utils.vectorize_data(essay_list, word_idx, 659)\r\n",
        "\r\n",
        "\r\n",
        "pred_array_syn=[]\r\n",
        "for i,v in enumerate(E):\r\n",
        "  pred = IG.predict([E[i][:659], m[0][0],1.0])\r\n",
        "  pred_array_syn.append(int(pred))\r\n",
        "\r\n",
        "data = pd.read_csv(ATTRS_DIR+'big_7_normal.csv')\r\n",
        "essay_list, resolved_scores = load_training_data(data)\r\n",
        "E = data_utils.vectorize_data(essay_list, word_idx, 659)\r\n",
        "\r\n",
        "\r\n",
        "pred_array_normal=[]\r\n",
        "for i,v in enumerate(E):\r\n",
        "  pred = IG.predict([E[i][:659], m[0][0],1.0])\r\n",
        "  pred_array_normal.append(int(pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnfkxlkiRcEr"
      },
      "source": [
        "get_pred_stats(pred_array_normal, pred_array_syn, filename = ATTRS_DIR+'MEMORY NET/P7/stats_synonym.txt', K = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg2bLZWtSxCn"
      },
      "source": [
        "########## SHUFFLING"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBZzK-_zSw_V"
      },
      "source": [
        "import math\r\n",
        "data = pd.read_csv(ATTRS_DIR+'big_7_shuffle.csv')\r\n",
        "essay_list, resolved_scores = load_training_data(data)\r\n",
        "E = data_utils.vectorize_data(essay_list, word_idx, 659)\r\n",
        "\r\n",
        "pred_array_shuffle=[]\r\n",
        "for i,v in enumerate(E):\r\n",
        "  pred = IG.predict([E[i][:659], m[0][0],1.0])\r\n",
        "  pred = np.clip(np.round(pred), 0, 30)\r\n",
        "  pred_array_shuffle.append((pred))\r\n",
        "\r\n",
        "data = pd.read_csv(ATTRS_DIR+'big_7_normal.csv')\r\n",
        "essay_list, resolved_scores = load_training_data(data)\r\n",
        "E = data_utils.vectorize_data(essay_list, word_idx, 659)\r\n",
        "\r\n",
        "pred_array_normal=[]\r\n",
        "for i,v in enumerate(E):\r\n",
        "  pred = IG.predict([E[i][:659], m[0][0],1.0])\r\n",
        "  pred = np.clip(np.round(pred), 0, 30)\r\n",
        "  pred_array_normal.append((pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTcVjnspSw8Y"
      },
      "source": [
        "get_pred_stats(pred_array_normal, pred_array_shuffle, filename = ATTRS_DIR+'MEMORY NET/P7/stats_shuffled.txt', K = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAPXKAtESw0r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}