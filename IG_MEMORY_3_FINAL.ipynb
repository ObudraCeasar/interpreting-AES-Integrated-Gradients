{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IG_MEMORY_3_Final_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-sQ5nW4RDV2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e1c3282-d721-41b0-b41c-9b7251a3293c"
      },
      "source": [
        "!rm -rf calling-out-bluff/\n",
        "!git clone https://github.com/SwapnilDreams100/calling-out-bluff.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'calling-out-bluff'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 3471 (delta 3), reused 12 (delta 2), pack-reused 3455\u001b[K\n",
            "Receiving objects: 100% (3471/3471), 908.76 MiB | 37.76 MiB/s, done.\n",
            "Resolving deltas: 100% (711/711), done.\n",
            "Checking out files: 100% (3481/3481), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oJTBGu57Rm5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc3175a5-b107-43ef-869b-c65869e30753"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sX7AJ0Eyahx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd221baf-7079-4f1c-8ac8-5f392f4982e1"
      },
      "source": [
        "! pip install xhtml2pdf\n",
        "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "! cp ./drive/My\\ Drive/glove.6B.300d.txt ./calling-out-bluff/Model5-MemoryNets/\n",
        "! cp ./drive/My\\ Drive/glove.6B.300d.txt ./calling-out-bluff/Model5-MemoryNets/glove"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting xhtml2pdf\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/a3/6d4c760bb08e7669f8c8a565ef62fe5a42d28aebf8dbc44476d9dd13782c/xhtml2pdf-0.2.5.tar.gz (100kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: html5lib>=1.0 in /usr/local/lib/python3.6/dist-packages (from xhtml2pdf) (1.0.1)\n",
            "Collecting pyPdf2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/01/68fcc0d43daf4c6bdbc6b33cc3f77bda531c86b174cac56ef0ffdb96faab/PyPDF2-1.26.0.tar.gz (77kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from xhtml2pdf) (7.0.0)\n",
            "Collecting reportlab>=3.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/6e/1b1b0202e176ad3acc3894d8d2dd4cf855a380d2405ec52dc7a96c05a601/reportlab-3.5.57-cp36-cp36m-manylinux2010_x86_64.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 12.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from xhtml2pdf) (1.15.0)\n",
            "Collecting python-bidi>=0.4.2\n",
            "  Downloading https://files.pythonhosted.org/packages/33/b0/f942d146a2f457233baaafd6bdf624eba8e0f665045b4abd69d1b62d097d/python_bidi-0.4.2-py2.py3-none-any.whl\n",
            "Collecting arabic-reshaper>=2.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/0a/1e/b78cacd22bd55c43f2711115d67ece3ed9616ba82d2d0e0914db2a6db985/arabic_reshaper-2.1.1.tar.gz\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from html5lib>=1.0->xhtml2pdf) (0.5.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from arabic-reshaper>=2.1.0->xhtml2pdf) (0.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from arabic-reshaper>=2.1.0->xhtml2pdf) (51.0.0)\n",
            "Building wheels for collected packages: xhtml2pdf, pyPdf2, arabic-reshaper\n",
            "  Building wheel for xhtml2pdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for xhtml2pdf: filename=xhtml2pdf-0.2.5-cp36-none-any.whl size=234491 sha256=c416d4bc559f55da78dc156cdc36446cb0a84dc77c5380b25d0e8ab78ab138c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/0f/15/6f8008b18ca84c08c198445b465c8038f745e444d7251a8266\n",
            "  Building wheel for pyPdf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyPdf2: filename=PyPDF2-1.26.0-cp36-none-any.whl size=61087 sha256=101b77986e83a5a1ad60d186fc2414de2e2b5ef4c9ff5073ebe13a20c2b9acf4\n",
            "  Stored in directory: /root/.cache/pip/wheels/53/84/19/35bc977c8bf5f0c23a8a011aa958acd4da4bbd7a229315c1b7\n",
            "  Building wheel for arabic-reshaper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for arabic-reshaper: filename=arabic_reshaper-2.1.1-cp36-none-any.whl size=16504 sha256=488e33e0688b257985ece8a2cb9dd428cb804656dfb43d13b39327b7fb218c5b\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/52/d8/bd0dcbf00f9e77e3bd0184285ed77dfa9c475dac494a5353d1\n",
            "Successfully built xhtml2pdf pyPdf2 arabic-reshaper\n",
            "Installing collected packages: pyPdf2, reportlab, python-bidi, arabic-reshaper, xhtml2pdf\n",
            "Successfully installed arabic-reshaper-2.1.1 pyPdf2-1.26.0 python-bidi-0.4.2 reportlab-3.5.57 xhtml2pdf-0.2.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi1zKL26LJ8u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf01da7e-271f-4267-bab7-5ffbfc0b652d"
      },
      "source": [
        "%cd calling-out-bluff/Model5-MemoryNets/\n",
        "%tensorflow_version 1.x\n",
        "import data_utils\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from qwk import quadratic_weighted_kappa\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "early_stop_count = 0\n",
        "max_step_count = 10\n",
        "is_regression =  False\n",
        "gated_addressing = False\n",
        "essay_set_id = 3\n",
        "batch_size = 15\n",
        "embedding_size = 300\n",
        "feature_size = 100\n",
        "l2_lambda = 0.3\n",
        "hops = 3\n",
        "reader = 'bow' # gru may not work\n",
        "epochs = 100\n",
        "num_samples = 1\n",
        "num_tokens = 42\n",
        "test_batch_size = batch_size\n",
        "random_state = 0"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/calling-out-bluff/Model5-MemoryNets\n",
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaHPoIhEMmLA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f10d9239-73e1-4ba4-d429-6acaaf52227e"
      },
      "source": [
        "if is_regression:\n",
        "    from memn2n_kv_regression import MemN2N_KV\n",
        "else:\n",
        "    from memn2n_kv import MemN2N_KV\n",
        "# print flags info\n",
        "orig_stdout = sys.stdout\n",
        "timestamp = time.strftime(\"%b_%d_%Y_%H:%M:%S\", time.localtime())\n",
        "folder_name = '{}'.format(essay_set_id)\n",
        "out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs/\", folder_name))\n",
        "if not os.path.exists(out_dir):\n",
        "    os.makedirs(out_dir)\n",
        "\n",
        "# save output to a file\n",
        "#f = file(out_dir+'/out.txt', 'w')\n",
        "#sys.stdout = f\n",
        "print(\"Writing to {}\\n\".format(out_dir))\n",
        "\n",
        "# hyper-parameters end here\n",
        "training_path = 'training_set_rel3.tsv'\n",
        "essay_list, resolved_scores, essay_id = data_utils.load_training_data(training_path, essay_set_id)\n",
        "\n",
        "max_score = max(resolved_scores)\n",
        "min_score = min(resolved_scores)\n",
        "if essay_set_id == 7:\n",
        "    min_score, max_score = 0, 30\n",
        "elif essay_set_id == 8:\n",
        "    min_score, max_score = 0, 60\n",
        "\n",
        "print( 'max_score is {} \\t min_score is {}\\n'.format(max_score, min_score))\n",
        "with open(out_dir+'/params', 'a') as f:\n",
        "    f.write('max_score is {} \\t min_score is {} \\n'.format(max_score, min_score))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing to /content/calling-out-bluff/Model5-MemoryNets/runs/3\n",
            "\n",
            "max_score is 3 \t min_score is 0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzizYq6tNYvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65d59f1b-73e5-4349-ccd8-15bff5f230e0"
      },
      "source": [
        "# if essay_set_id == 7:\n",
        "#     min_score, max_score = 0, 30\n",
        "# elif essay_set_id == 8:\n",
        "#     min_score, max_score = 0, 60\n",
        "\n",
        "score_range = range(min_score, max_score+1)\n",
        "\n",
        "# #word_idx, _ = data_utils.build_vocab(essay_list, vocab_limit)\n",
        "\n",
        "# load glove\n",
        "import data_utils\n",
        "word_idx, word2vec = data_utils.load_glove(42, dim=300)\n",
        "\n",
        "with open('tokenizer.pkl', 'wb') as f:\n",
        "  pickle.dump( word_idx,f)\n",
        "\n",
        "# vocab_size = len(word_idx) + 1\n",
        "# # stat info on data set\n",
        "\n",
        "# sent_size_list = list(map(len, [essay for essay in essay_list]))\n",
        "# # print(\"sent size list\", sent_size_list)\n",
        "# max_sent_size = max(sent_size_list)\n",
        "# mean_sent_size = int(np.mean(sent_size_list))\n",
        "\n",
        "# print( 'max sentence size: {} \\nmean sentence size: {}\\n'.format(max_sent_size, mean_sent_size))\n",
        "# with open(out_dir+'/params', 'a') as f:\n",
        "#     f.write('max sentence size: {} \\nmean sentence size: {}\\n'.format(max_sent_size, mean_sent_size))\n",
        "\n",
        "# print( 'The length of score range is {}'.format(len(score_range)))\n",
        "# E = data_utils.vectorize_data(essay_list, word_idx, max_sent_size)\n",
        "# # print(vocab_size)\n",
        "# labeled_data = zip(E, resolved_scores, sent_size_list)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> glove is loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yg1RBn6wNueN"
      },
      "source": [
        "def train_step(m, e, s, ma):\n",
        "    start_time = time.time()\n",
        "    feed_dict = {\n",
        "        model._query: e,\n",
        "        model._memory_key: m,\n",
        "        model._score_encoding: s,\n",
        "        model._mem_attention_encoding: ma,\n",
        "        model.keep_prob: 0.9\n",
        "        #model.w_placeholder: word2vec\n",
        "    }\n",
        "    _, step, predict_op, cost = sess.run([train_op, global_step, model.predict_op, model.cost], feed_dict)\n",
        "    end_time = time.time()\n",
        "    time_spent = end_time - start_time\n",
        "    return predict_op, cost, time_spent\n",
        "\n",
        "def test_step(e, m):\n",
        "    feed_dict = {\n",
        "        model._query: e,\n",
        "        model._memory_key: m,\n",
        "        model.keep_prob: 1\n",
        "        #model.w_placeholder: word2vec\n",
        "    }\n",
        "    preds, mem_attention_probs = sess.run([model.predict_op, model.mem_attention_probs], feed_dict)\n",
        "    if is_regression:\n",
        "        preds = np.clip(np.round(preds), min_score, max_score)\n",
        "        return preds, mem_attention_probs\n",
        "    else:\n",
        "        return preds, mem_attention_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z3jOAx8N_2z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "230669db-30d1-4875-a02b-a7b3ad597537"
      },
      "source": [
        "fold_count = 0\n",
        "kf = KFold(n_splits=5, random_state=random_state)\n",
        "best_kappa_scores = []\n",
        "for train_index, test_index in kf.split(essay_id):\n",
        "    early_stop_count = 0\n",
        "    fold_count += 1\n",
        "    if fold_count>=2:\n",
        "      break\n",
        "    trainE = []\n",
        "    testE = []\n",
        "    train_scores = []\n",
        "    test_scores = []\n",
        "    train_essay_id = []\n",
        "    test_essay_id = []\n",
        "\n",
        "    for ite in train_index:\n",
        "        trainE.append(E[ite])\n",
        "        train_scores.append(resolved_scores[ite])\n",
        "        train_essay_id.append(essay_id[ite])\n",
        "    for ite in test_index:\n",
        "        testE.append(E[ite])\n",
        "        test_scores.append(resolved_scores[ite])\n",
        "        test_essay_id.append(essay_id[ite])\n",
        "    \n",
        "    memory = []\n",
        "    memory_score = []\n",
        "    memory_sent_size = []\n",
        "    memory_essay_ids = []\n",
        "    # pick sampled essay for each score\n",
        "    for i in score_range:\n",
        "    # test point: limit the number of samples in memory for 8\n",
        "        for j in range(num_samples):\n",
        "            if i in train_scores:\n",
        "                score_idx = train_scores.index(i)\n",
        "                score = train_scores.pop(score_idx)\n",
        "                essay = trainE.pop(score_idx)\n",
        "                sent_size = sent_size_list.pop(score_idx)\n",
        "                memory.append(essay)\n",
        "                memory_score.append(score)\n",
        "                memory_essay_ids.append(train_essay_id.pop(score_idx))\n",
        "                memory_sent_size.append(sent_size)\n",
        "    memory_size = len(memory)\n",
        "    if is_regression:\n",
        "    # bad naming\n",
        "        train_scores_encoding = train_scores\n",
        "    else:\n",
        "        train_scores_encoding = list(map(lambda x: score_range.index(x), train_scores))\n",
        "    \n",
        "    # data size\n",
        "    n_train = len(trainE)\n",
        "    n_test = len(testE)\n",
        "\n",
        "    print( 'The size of training data: {}'.format(n_train))\n",
        "    print( 'The size of testing data: {}'.format(n_test))\n",
        "    with open(out_dir+'/params{}'.format(fold_count), 'a') as f:\n",
        "        f.write('The size of training data: {}\\n'.format(n_train))\n",
        "        f.write('The size of testing data: {}\\n'.format(n_test))\n",
        "        f.write('\\nEssay scores in memory:\\n{}'.format(memory_score))\n",
        "        f.write('\\nEssay ids in memory:\\n{}'.format(memory_essay_ids))\n",
        "        f.write('\\nEssay ids in training:\\n{}'.format(train_essay_id))\n",
        "        f.write('\\nEssay ids in testing:\\n{}'.format(test_essay_id))\n",
        "\n",
        "    batches = zip(range(0, n_train-batch_size, batch_size), range(batch_size, n_train, batch_size))\n",
        "    batches = [(start, end) for start, end in batches]\n",
        "    print(batches)\n",
        "    x = 1\n",
        "    if x == 1:\n",
        "        with tf.Graph().as_default():\n",
        "            session_conf = tf.ConfigProto(\n",
        "                allow_soft_placement=True,\n",
        "                log_device_placement=False)\n",
        "\n",
        "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "            # decay learning rate\n",
        "            starter_learning_rate = 0.0001\n",
        "            learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 3000, 0.96, staircase=True)\n",
        "\n",
        "            # test point\n",
        "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=0.1)\n",
        "            best_kappa_so_far = 0.0\n",
        "            with tf.Session(config=session_conf) as sess:\n",
        "                model = MemN2N_KV(batch_size, vocab_size, max_sent_size, max_sent_size, memory_size,\n",
        "                                  memory_size, embedding_size, len(score_range), feature_size, hops, reader, l2_lambda)\n",
        "                grads_and_vars = optimizer.compute_gradients(model.loss_op, aggregation_method=tf.AggregationMethod.EXPERIMENTAL_TREE)\n",
        "                grads_and_vars = [(tf.clip_by_norm(g, 10.0), v)\n",
        "                                  for g, v in grads_and_vars if g is not None]\n",
        "                #grads_and_vars = [(add_gradient_noise(g, 1e-4), v) for g, v in grads_and_vars]\n",
        "                train_op = optimizer.apply_gradients(grads_and_vars, name=\"train_op\", global_step=global_step)\n",
        "                sess.run(tf.global_variables_initializer(), feed_dict={model.w_placeholder: word2vec})\n",
        "                saver = tf.train.Saver(tf.global_variables(), max_to_keep=1)\n",
        "\n",
        "                for i in range(1, epochs+1):\n",
        "                    train_cost = 0\n",
        "                    total_time = 0\n",
        "                    np.random.shuffle(batches)\n",
        "                    for start, end in batches:\n",
        "                        e = trainE[start:end]\n",
        "                        s = train_scores_encoding[start:end]\n",
        "                        s_num = train_scores[start:end]\n",
        "                        #batched_memory = []\n",
        "                        # batch sized memory\n",
        "                        #for _ in range(len(e)):\n",
        "                        #    batched_memory.append(memory)\n",
        "                        mem_atten_encoding = []\n",
        "                        for ite in s_num:\n",
        "                            mem_encoding = np.zeros(memory_size)\n",
        "                            for j_idx, j in enumerate(memory_score):\n",
        "                                if j == ite:\n",
        "                                    mem_encoding[j_idx] = 1\n",
        "                            mem_atten_encoding.append(mem_encoding)\n",
        "                        batched_memory = [memory] * (end-start)\n",
        "                        _, cost, time_spent = train_step(batched_memory, e, s, mem_atten_encoding)\n",
        "                        total_time += time_spent\n",
        "                        train_cost += cost\n",
        "                    print( 'Finish epoch {}, total training cost is {}, time spent is {}'.format(i, train_cost, total_time))\n",
        "                    # evaluation\n",
        "                    if i % 5 == 0 or i == 200:\n",
        "                        # test on training data\n",
        "                        train_preds = []\n",
        "                        for start in range(0, n_train, test_batch_size):\n",
        "                            end = min(n_train, start+test_batch_size)\n",
        "\n",
        "                            #batched_memory = []\n",
        "                            #for _ in range(end-start):\n",
        "                            #    batched_memory.append(memory)\n",
        "                            batched_memory = [memory] * (end-start)\n",
        "    #                         print(\"BM\", len(batched_memory))\n",
        "                            preds, _ = test_step(trainE[start:end], batched_memory)\n",
        "                            if type(preds) is np.float32:\n",
        "                                train_preds.append(preds)\n",
        "                            else:\n",
        "                                for ite in preds:\n",
        "                                    train_preds.append(ite)\n",
        "                        if not is_regression:\n",
        "                            train_preds = np.add(train_preds, min_score)\n",
        "                        #train_kappp_score = kappa(train_scores, train_preds, 'quadratic')\n",
        "                        train_kappp_score = quadratic_weighted_kappa(\n",
        "                            train_scores, train_preds, min_score, max_score)\n",
        "                        # test on test data\n",
        "                        test_preds = []\n",
        "                        test_atten_probs = []\n",
        "                        for start in range(0, n_test, test_batch_size):\n",
        "                            end = min(n_test, start+test_batch_size)\n",
        "\n",
        "                            #batched_memory = []\n",
        "                            #for _ in range(end-start):\n",
        "                            #    batched_memory.append(memory)\n",
        "                            batched_memory = [memory] * (end-start)\n",
        "    #                         print(\"Test\", len(testE[start:end]))\n",
        "                            \n",
        "                            preds, mem_attention_probs = test_step(testE[start:end], batched_memory)\n",
        "\n",
        "\n",
        "                            # preds2, explanations = explain_step(testE[start:end], batched_memory)\n",
        "                            # print(preds2, len(explanations), (explanations[0]).shape)\n",
        "                            if type(preds) is np.float32:\n",
        "                                test_preds.append(preds)\n",
        "                            else:\n",
        "                                for ite in preds:\n",
        "                                    test_preds.append(ite)\n",
        "                            for ite in mem_attention_probs:\n",
        "                                test_atten_probs.append(ite)\n",
        "                        if not is_regression:\n",
        "                            test_preds = np.add(test_preds, min_score)\n",
        "                        #test_kappp_score = kappa(test_scores, test_preds, 'quadratic')\n",
        "                        \n",
        "                        ##### STORE TEST DATA\n",
        "                        t = []\n",
        "                        m = []\n",
        "                        for start in range(0, n_test, test_batch_size):\n",
        "                            end = min(n_test, start+test_batch_size)\n",
        "                            batched_memory = [memory] * (end-start)\n",
        "                            t.append(testE[start:end])\n",
        "                            m.append(batched_memory)\n",
        "                        \n",
        "                        with open(str(essay_set_id)+'_'+str(fold_count)+'.pkl', 'wb') as f:\n",
        "                          pickle.dump((t,m), f)\n",
        "                        #########################\n",
        "                        test_kappp_score = quadratic_weighted_kappa(\n",
        "                            test_scores, test_preds, min_score, max_score)\n",
        "                        stat_dict = {'pred_score': test_preds}\n",
        "                        stat_df = pd.DataFrame(stat_dict)\n",
        "                        # save the model if it gets best kappa\n",
        "                        if(test_kappp_score > best_kappa_so_far):\n",
        "                            early_stop_count = 0\n",
        "                            best_kappa_so_far = test_kappp_score\n",
        "                            # stats on test\n",
        "                            # stat_df.to_csv(out_dir+'/predScore_'+str(fold_count))\n",
        "                            with open(out_dir+'/mem_atten', 'a') as f:\n",
        "                                for idx, ite in enumerate(test_essay_id):\n",
        "                                    f.write('{}\\n'.format(ite))\n",
        "                                    f.write('{}\\n'.format(test_atten_probs[idx]))\n",
        "                            saver.save(sess, out_dir+'/checkpoints_'+str(fold_count), global_step)\n",
        "                            \n",
        "                        else:\n",
        "                            early_stop_count += 1\n",
        "                        print(\"Training kappa score = {}\".format(train_kappp_score))\n",
        "                        print(\"Testing kappa score = {}\".format(test_kappp_score))\n",
        "                        with open(out_dir+'/eval_'.format(fold_count), 'a') as f:\n",
        "                            f.write(\"Training kappa score = {}\\n\".format(train_kappp_score))\n",
        "                            f.write(\"Testing kappa score = {}\\n\".format(test_kappp_score))\n",
        "                            f.write(\"Best Testing kappa score so far = {}\\n\".format(best_kappa_so_far))\n",
        "                            f.write('*'*10)\n",
        "                            f.write('\\n')\n",
        "                    if early_stop_count > max_step_count:\n",
        "                        break\n",
        "                best_kappa_scores.append(best_kappa_so_far)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The size of training data: 1376\n",
            "The size of testing data: 346\n",
            "[(0, 15), (15, 30), (30, 45), (45, 60), (60, 75), (75, 90), (90, 105), (105, 120), (120, 135), (135, 150), (150, 165), (165, 180), (180, 195), (195, 210), (210, 225), (225, 240), (240, 255), (255, 270), (270, 285), (285, 300), (300, 315), (315, 330), (330, 345), (345, 360), (360, 375), (375, 390), (390, 405), (405, 420), (420, 435), (435, 450), (450, 465), (465, 480), (480, 495), (495, 510), (510, 525), (525, 540), (540, 555), (555, 570), (570, 585), (585, 600), (600, 615), (615, 630), (630, 645), (645, 660), (660, 675), (675, 690), (690, 705), (705, 720), (720, 735), (735, 750), (750, 765), (765, 780), (780, 795), (795, 810), (810, 825), (825, 840), (840, 855), (855, 870), (870, 885), (885, 900), (900, 915), (915, 930), (930, 945), (945, 960), (960, 975), (975, 990), (990, 1005), (1005, 1020), (1020, 1035), (1035, 1050), (1050, 1065), (1065, 1080), (1080, 1095), (1095, 1110), (1110, 1125), (1125, 1140), (1140, 1155), (1155, 1170), (1170, 1185), (1185, 1200), (1200, 1215), (1215, 1230), (1230, 1245), (1245, 1260), (1260, 1275), (1275, 1290), (1290, 1305), (1305, 1320), (1320, 1335), (1335, 1350), (1350, 1365)]\n",
            "WARNING:tensorflow:From /content/calling-out-bluff/Model5-MemoryNets/memn2n_kv.py:195: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/calling-out-bluff/Model5-MemoryNets/memn2n_kv.py:116: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/calling-out-bluff/Model5-MemoryNets/memn2n_kv.py:220: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/calling-out-bluff/Model5-MemoryNets/memn2n_kv.py:181: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/clip_ops.py:172: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Finish epoch 1, total training cost is 38221.46487426758, time spent is 2.399930238723755\n",
            "Finish epoch 2, total training cost is 8530.107620239258, time spent is 0.5698964595794678\n",
            "Finish epoch 3, total training cost is 6270.6053075790405, time spent is 0.5866327285766602\n",
            "Finish epoch 4, total training cost is 5860.547685623169, time spent is 0.5800943374633789\n",
            "Finish epoch 5, total training cost is 4717.790766716003, time spent is 0.625119686126709\n",
            "Training kappa score = 0.6261322494246221\n",
            "Testing kappa score = 0.5988268994680126\n",
            "Finish epoch 6, total training cost is 3937.3922290802, time spent is 0.6369366645812988\n",
            "Finish epoch 7, total training cost is 3651.114429473877, time spent is 0.6267116069793701\n",
            "Finish epoch 8, total training cost is 3631.284599304199, time spent is 0.5889184474945068\n",
            "Finish epoch 9, total training cost is 3424.8001251220703, time spent is 0.5846817493438721\n",
            "Finish epoch 10, total training cost is 3213.8027839660645, time spent is 0.5797491073608398\n",
            "Training kappa score = 0.5857650902727047\n",
            "Testing kappa score = 0.49892189360538086\n",
            "Finish epoch 11, total training cost is 3409.7076930999756, time spent is 0.5810089111328125\n",
            "Finish epoch 12, total training cost is 3109.6401805877686, time spent is 0.5742881298065186\n",
            "Finish epoch 13, total training cost is 2939.427577495575, time spent is 0.5713720321655273\n",
            "Finish epoch 14, total training cost is 2908.0142364501953, time spent is 0.5771498680114746\n",
            "Finish epoch 15, total training cost is 2802.085135936737, time spent is 0.5761008262634277\n",
            "Training kappa score = 0.6187248942824265\n",
            "Testing kappa score = 0.5430306047656586\n",
            "Finish epoch 16, total training cost is 2702.83016204834, time spent is 0.5796535015106201\n",
            "Finish epoch 17, total training cost is 2585.5694456100464, time spent is 0.5864622592926025\n",
            "Finish epoch 18, total training cost is 2597.606210708618, time spent is 0.576263427734375\n",
            "Finish epoch 19, total training cost is 2582.8685159683228, time spent is 0.5901861190795898\n",
            "Finish epoch 20, total training cost is 2470.2227172851562, time spent is 0.5783214569091797\n",
            "Training kappa score = 0.553724165936438\n",
            "Testing kappa score = 0.5054020810478375\n",
            "Finish epoch 21, total training cost is 2402.1216492652893, time spent is 0.6090438365936279\n",
            "Finish epoch 22, total training cost is 2235.6122941970825, time spent is 0.6035501956939697\n",
            "Finish epoch 23, total training cost is 2337.6228461265564, time spent is 0.589301347732544\n",
            "Finish epoch 24, total training cost is 2182.0067615509033, time spent is 0.5867424011230469\n",
            "Finish epoch 25, total training cost is 2211.1071796417236, time spent is 0.6032099723815918\n",
            "Training kappa score = 0.5906567974222361\n",
            "Testing kappa score = 0.558854084684401\n",
            "Finish epoch 26, total training cost is 2071.2478618621826, time spent is 0.5950973033905029\n",
            "Finish epoch 27, total training cost is 1921.5253462791443, time spent is 0.5863687992095947\n",
            "Finish epoch 28, total training cost is 1943.3336362838745, time spent is 0.573768138885498\n",
            "Finish epoch 29, total training cost is 1804.4614930152893, time spent is 0.6142194271087646\n",
            "Finish epoch 30, total training cost is 1864.7993078231812, time spent is 0.5778636932373047\n",
            "Training kappa score = 0.6356547500268623\n",
            "Testing kappa score = 0.5671256589818239\n",
            "Finish epoch 31, total training cost is 1871.3427577018738, time spent is 0.5785753726959229\n",
            "Finish epoch 32, total training cost is 1887.5956401824951, time spent is 0.6603264808654785\n",
            "Finish epoch 33, total training cost is 1928.9747676849365, time spent is 0.5848948955535889\n",
            "Finish epoch 34, total training cost is 1730.8067779541016, time spent is 0.5778231620788574\n",
            "Finish epoch 35, total training cost is 1758.582926750183, time spent is 0.6085939407348633\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "Training kappa score = 0.6485593142287973\n",
            "Testing kappa score = 0.6505530745448299\n",
            "Finish epoch 36, total training cost is 1727.5160007476807, time spent is 0.6013360023498535\n",
            "Finish epoch 37, total training cost is 1676.222586631775, time spent is 0.5685834884643555\n",
            "Finish epoch 38, total training cost is 1732.6908612251282, time spent is 0.6006605625152588\n",
            "Finish epoch 39, total training cost is 1684.0376119613647, time spent is 0.5745518207550049\n",
            "Finish epoch 40, total training cost is 1562.2976174354553, time spent is 0.5801339149475098\n",
            "Training kappa score = 0.6810283657554204\n",
            "Testing kappa score = 0.6192330483365783\n",
            "Finish epoch 41, total training cost is 1690.1784167289734, time spent is 0.6144604682922363\n",
            "Finish epoch 42, total training cost is 1732.4573187828064, time spent is 0.5877912044525146\n",
            "Finish epoch 43, total training cost is 1662.0298409461975, time spent is 0.5727095603942871\n",
            "Finish epoch 44, total training cost is 1652.316298007965, time spent is 0.6289041042327881\n",
            "Finish epoch 45, total training cost is 1618.6402311325073, time spent is 0.6310586929321289\n",
            "Training kappa score = 0.6876402093793398\n",
            "Testing kappa score = 0.6497647043050397\n",
            "Finish epoch 46, total training cost is 1607.4893579483032, time spent is 0.5792372226715088\n",
            "Finish epoch 47, total training cost is 1666.4278182983398, time spent is 0.5692002773284912\n",
            "Finish epoch 48, total training cost is 1629.6501026153564, time spent is 0.5967838764190674\n",
            "Finish epoch 49, total training cost is 1590.5270128250122, time spent is 0.5742189884185791\n",
            "Finish epoch 50, total training cost is 1524.5067343711853, time spent is 0.6469063758850098\n",
            "Training kappa score = 0.6553751661891655\n",
            "Testing kappa score = 0.6042492113093014\n",
            "Finish epoch 51, total training cost is 1558.756501674652, time spent is 0.579909086227417\n",
            "Finish epoch 52, total training cost is 1528.2354884147644, time spent is 0.6056420803070068\n",
            "Finish epoch 53, total training cost is 1548.990927696228, time spent is 0.5668017864227295\n",
            "Finish epoch 54, total training cost is 1569.3664298057556, time spent is 0.6116468906402588\n",
            "Finish epoch 55, total training cost is 1613.3531212806702, time spent is 0.5769925117492676\n",
            "Training kappa score = 0.6600157109190887\n",
            "Testing kappa score = 0.5924885514052876\n",
            "Finish epoch 56, total training cost is 1608.3346405029297, time spent is 0.6062769889831543\n",
            "Finish epoch 57, total training cost is 1750.626326084137, time spent is 0.5908725261688232\n",
            "Finish epoch 58, total training cost is 1613.0090446472168, time spent is 0.5777289867401123\n",
            "Finish epoch 59, total training cost is 1533.4010019302368, time spent is 0.5747451782226562\n",
            "Finish epoch 60, total training cost is 1586.7146348953247, time spent is 0.5835671424865723\n",
            "Training kappa score = 0.6792187417370303\n",
            "Testing kappa score = 0.619286762216454\n",
            "Finish epoch 61, total training cost is 1512.7292802333832, time spent is 0.5970122814178467\n",
            "Finish epoch 62, total training cost is 1592.3096170425415, time spent is 0.5812890529632568\n",
            "Finish epoch 63, total training cost is 1437.3305277824402, time spent is 0.5787186622619629\n",
            "Finish epoch 64, total training cost is 1569.944712638855, time spent is 0.5941026210784912\n",
            "Finish epoch 65, total training cost is 1455.8646636009216, time spent is 0.6467573642730713\n",
            "Training kappa score = 0.6657301326190441\n",
            "Testing kappa score = 0.594037893273075\n",
            "Finish epoch 66, total training cost is 1430.1326122283936, time spent is 0.5834352970123291\n",
            "Finish epoch 67, total training cost is 1412.9791584014893, time spent is 0.5923175811767578\n",
            "Finish epoch 68, total training cost is 1403.4639286994934, time spent is 0.5748274326324463\n",
            "Finish epoch 69, total training cost is 1355.2099618911743, time spent is 0.58561110496521\n",
            "Finish epoch 70, total training cost is 1351.170479774475, time spent is 0.5974569320678711\n",
            "Training kappa score = 0.696029533047952\n",
            "Testing kappa score = 0.6580714407256639\n",
            "Finish epoch 71, total training cost is 1354.700162410736, time spent is 0.6071763038635254\n",
            "Finish epoch 72, total training cost is 1352.4615759849548, time spent is 0.580230712890625\n",
            "Finish epoch 73, total training cost is 1324.8930668830872, time spent is 0.6066174507141113\n",
            "Finish epoch 74, total training cost is 1275.2798109054565, time spent is 0.5778703689575195\n",
            "Finish epoch 75, total training cost is 1364.0166854858398, time spent is 0.5814468860626221\n",
            "Training kappa score = 0.6811020767517737\n",
            "Testing kappa score = 0.6174855029133234\n",
            "Finish epoch 76, total training cost is 1320.3022804260254, time spent is 0.5763380527496338\n",
            "Finish epoch 77, total training cost is 1282.8743572235107, time spent is 0.6364214420318604\n",
            "Finish epoch 78, total training cost is 1246.6229286193848, time spent is 0.5832841396331787\n",
            "Finish epoch 79, total training cost is 1366.8645396232605, time spent is 0.5875332355499268\n",
            "Finish epoch 80, total training cost is 1335.9492683410645, time spent is 0.5764551162719727\n",
            "Training kappa score = 0.6860521891866207\n",
            "Testing kappa score = 0.6204071862657748\n",
            "Finish epoch 81, total training cost is 1312.0148015022278, time spent is 0.6259148120880127\n",
            "Finish epoch 82, total training cost is 1293.9798669815063, time spent is 0.6604418754577637\n",
            "Finish epoch 83, total training cost is 1325.4007534980774, time spent is 0.5988645553588867\n",
            "Finish epoch 84, total training cost is 1268.2475762367249, time spent is 0.5722699165344238\n",
            "Finish epoch 85, total training cost is 1289.049967765808, time spent is 0.5830154418945312\n",
            "Training kappa score = 0.662214750023552\n",
            "Testing kappa score = 0.6157884110640016\n",
            "Finish epoch 86, total training cost is 1259.3111691474915, time spent is 0.5697102546691895\n",
            "Finish epoch 87, total training cost is 1184.044156074524, time spent is 0.5759453773498535\n",
            "Finish epoch 88, total training cost is 1250.0112895965576, time spent is 0.6698300838470459\n",
            "Finish epoch 89, total training cost is 1205.246087551117, time spent is 0.6681396961212158\n",
            "Finish epoch 90, total training cost is 1238.5007615089417, time spent is 0.6497068405151367\n",
            "Training kappa score = 0.6931051969272954\n",
            "Testing kappa score = 0.6650977415069272\n",
            "Finish epoch 91, total training cost is 1232.8717827796936, time spent is 0.6168954372406006\n",
            "Finish epoch 92, total training cost is 1264.058792591095, time spent is 0.5646457672119141\n",
            "Finish epoch 93, total training cost is 1265.686378955841, time spent is 0.5764241218566895\n",
            "Finish epoch 94, total training cost is 1209.5481672286987, time spent is 0.5791497230529785\n",
            "Finish epoch 95, total training cost is 1285.5944437980652, time spent is 0.5628652572631836\n",
            "Training kappa score = 0.6455256837375067\n",
            "Testing kappa score = 0.5780422252947848\n",
            "Finish epoch 96, total training cost is 1255.4314031600952, time spent is 0.5699536800384521\n",
            "Finish epoch 97, total training cost is 1246.997784614563, time spent is 0.5740690231323242\n",
            "Finish epoch 98, total training cost is 1261.0949516296387, time spent is 0.579810619354248\n",
            "Finish epoch 99, total training cost is 1268.5039196014404, time spent is 0.5751252174377441\n",
            "Finish epoch 100, total training cost is 1265.4179558753967, time spent is 0.576141357421875\n",
            "Training kappa score = 0.6874318528749\n",
            "Testing kappa score = 0.6372390581384477\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1uCXZhl_Ggc"
      },
      "source": [
        "! cp -r /content/calling-out-bluff/Model5-MemoryNets/runs/3/ /content/drive/MyDrive/IG\\ RESULTS/MEM\\ MODELS/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1rl01419tWZ"
      },
      "source": [
        "# INFERENCE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZBjozECi01B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30a6c6a6-2620-4918-af05-021a764922c2"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "%cd /content/calling-out-bluff/Model5-MemoryNets/\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import numpy as np\n",
        "from IPython.display import display, HTML\n",
        "# from xhtml2pdf import pisa\n",
        "import gc"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/calling-out-bluff/Model5-MemoryNets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzRVPEa4rZVe"
      },
      "source": [
        "fold_no = 1\n",
        "step = 8190\n",
        "essay_set_id = 3"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K0SWvSVsa53"
      },
      "source": [
        "ATTRS_DIR = '/content/drive/My Drive/IG RESULTS/MEMORY NET/P'+str(essay_set_id)+'/'\n",
        "ATTRS_TSV = '/content/drive/My Drive/IG RESULTS/MEMORY NET/P'+str(essay_set_id)+'/attrs.tsv'\n",
        "\n",
        "def convert_html_to_pdf(source_html, output_filename):\n",
        "  # open output file for writing (truncated binary)\n",
        "  result_file = open(output_filename, \"w+b\")\n",
        "\n",
        "  # convert HTML to PDF\n",
        "  pisa_status = pisa.CreatePDF(\n",
        "          source_html,                # the HTML to convert\n",
        "          dest=result_file)           # file handle to recieve result\n",
        "\n",
        "  # close output file\n",
        "  result_file.close()                 # close output file\n",
        "\n",
        "  # return False on success and True on errors\n",
        "  return pisa_status.err   "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mXLl4KN6jaN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "597811ff-0667-42ca-9b32-8162b6f326dd"
      },
      "source": [
        "with open('/content/drive/My Drive/IG RESULTS/MEM MODELS/'+str(essay_set_id)+'_'+str(fold_no)+'.pkl', 'rb') as f:\n",
        "  (t,m,test_scores) = pickle.load(f)\n",
        "  \n",
        "saver = tf.train.import_meta_graph(\"/content/drive/My Drive/IG RESULTS/MEM MODELS/\"+str(essay_set_id)+\"/checkpoints_\"+str(fold_no)+\"-\"+str(step)+\".meta\")  \n",
        "sess=tf.Session()\n",
        "saver.restore(sess,\"/content/drive/My Drive/IG RESULTS/MEM MODELS/\"+str(essay_set_id)+\"/checkpoints_\"+str(fold_no)+\"-\"+str(step))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/IG RESULTS/MEM MODELS/3/checkpoints_1-8190\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OfFy2AcJDt3"
      },
      "source": [
        "from tqdm import tqdm\n",
        "class integrated_gradients:\n",
        "  def __init__(self, graph, sess, min, tokenizer_file = 'tokenizer.pkl',batch_size = 20, num_reps = 20):\n",
        "    \n",
        "    self.graph = graph\n",
        "    self.sess = sess\n",
        "    w1 = self.graph.get_tensor_by_name(\"input/question:0\")\n",
        "    w2 = self.graph.get_tensor_by_name(\"input/memory_key:0\")\n",
        "    w3 = self.graph.get_tensor_by_name(\"input/keep_prob:0\")\n",
        "    self.batch_size = batch_size\n",
        "    self.min = min\n",
        "    self.INPUT_TENSORS = [w1,w2,w3]\n",
        "    self.num_reps = num_reps\n",
        "\n",
        "    with open(tokenizer_file, 'rb') as f:\n",
        "      tokenizer = pickle.load(f)\n",
        "    self.reverse_word_map = dict(map(reversed, tokenizer.items()))\n",
        "      \n",
        "    self.OUTPUT_TENSOR =  self.graph.get_tensor_by_name('prediction/Softmax:0')\n",
        "    self.PRED_TENSOR =  self.graph.get_tensor_by_name('prediction/predict_op:0')\n",
        "    self.EMBEDDING_TENSOR =  self.graph.get_tensor_by_name('embedding_lookup/Identity:0')\n",
        "    self.outchannels = self.OUTPUT_TENSOR.shape[1]._value\n",
        "    self.get_gradients = {}\n",
        "    print(self.outchannels)\n",
        "    for c in range(self.outchannels):\n",
        "      GRADIENT_TENSOR = tf.gradients(self.OUTPUT_TENSOR[:,c], self.EMBEDDING_TENSOR)\n",
        "      self.get_gradients[c] = GRADIENT_TENSOR\n",
        "  \n",
        "  def get_tensor(self, name):\n",
        "    return self.graph.get_tensor_by_name(name)\n",
        "\n",
        "  def generate_baseline(self, sample):\n",
        "    baseline = np.zeros(sample.shape)\n",
        "    return baseline\n",
        "\n",
        "  def _get_feed_dict(self, input_df):\n",
        "      feed = {}\n",
        "      for i, key in enumerate(self.INPUT_TENSORS):\n",
        "          feed[key.name] = [input_df[i]]\n",
        "      return feed\n",
        "\n",
        "  def _get_ig_error(self, integrated_gradients, baseline_prediction, prediction,\n",
        "                  debug=False):\n",
        "      sum_attributions = 0\n",
        "      sum_attributions += np.sum(integrated_gradients)\n",
        "\n",
        "      delta_prediction = prediction - baseline_prediction\n",
        "\n",
        "      error_percentage = \\\n",
        "          100 * (delta_prediction - sum_attributions) / delta_prediction\n",
        "      if debug:\n",
        "          print(f'prediction is {prediction}')\n",
        "          print(f'baseline_prediction is {baseline_prediction}')\n",
        "          print(f'delta_prediction is {delta_prediction}')\n",
        "          print(f'sum_attributions are {sum_attributions}')\n",
        "          print(f'Error percentage is {error_percentage}')\n",
        "\n",
        "      return error_percentage\n",
        "\n",
        "  def _get_scaled_inputs(self, input_val, baseline_val, num_reps):\n",
        "      list_scaled_embeddings = []\n",
        "      scaled_embeddings = \\\n",
        "          [baseline_val + (float(i) / (num_reps * self.batch_size - 1)) *\n",
        "          (input_val - baseline_val) for i in range(0, num_reps * self.batch_size)]\n",
        "\n",
        "      for i in range(num_reps):\n",
        "          list_scaled_embeddings.append(\n",
        "              np.array(scaled_embeddings[i * self.batch_size:i * self.batch_size +\n",
        "                                                        self.batch_size]))\n",
        "      return np.array(list_scaled_embeddings)\n",
        "\n",
        "  def _get_unscaled_inputs(self, input_val):\n",
        "      unscaled_embeddings = [input_val] * self.batch_size\n",
        "\n",
        "      return np.array(unscaled_embeddings)\n",
        "\n",
        "  def _calculate_integral(self, ig):\n",
        "      ig = (ig[:-1] + ig[1:]) / 2.0  # trapezoidal rule\n",
        "      integral = np.average(ig, axis=0)\n",
        "      return integral\n",
        "\n",
        "  def explain(self, x, memory, max_allowed_error=5, debug=False):\n",
        "      num_reps = self.num_reps\n",
        "      baseline = self.generate_baseline(np.array(x))\n",
        "      # baseline_memory = self.generate_baseline(np.array(memory))\n",
        "      inp = [x, memory, 1]\n",
        "      base = [baseline, memory, 1]\n",
        "      \n",
        "      pred = self.predict(inp)\n",
        "      c =  pred\n",
        "      # print('channel:', c)\n",
        "      \n",
        "      attributions, baseline_prediction, prediction = \\\n",
        "          self._compute_ig(inp, base, c, num_reps=num_reps) #### MAIN FUNC\n",
        "      if debug:\n",
        "        error_percentage = \\\n",
        "            self._get_ig_error(attributions, baseline_prediction, prediction,\n",
        "                          debug=debug)\n",
        "\n",
        "      igs = attributions.astype('float')\n",
        "      words = self.sequence_to_text(inp[0])\n",
        "      assert len(igs) == len(words)\n",
        "      # html_code = self.visualize_token_attrs(words , igs)\n",
        "      return igs, words\n",
        "  \n",
        "  def predict(self, inp):\n",
        "      pred = self.sess.run(self.PRED_TENSOR,\n",
        "                              self._get_feed_dict(inp))\n",
        "      return pred[0]\n",
        "\n",
        "  def _get_feed_dict_batch(self, input_df):\n",
        "      feed = {}\n",
        "      for i, key in enumerate(self.INPUT_TENSORS):\n",
        "          feed[key.name] = input_df[i]\n",
        "      return feed\n",
        "\n",
        "  def predict_batch(self, inp):\n",
        "      pred = self.sess.run(self.PRED_TENSOR,\n",
        "                              self._get_feed_dict_batch(inp))\n",
        "      return pred\n",
        "\n",
        "  def visualize_token_attrs(self, tokens, attrs):\n",
        "    import matplotlib as mpl\n",
        "    cmap='PiYG'\n",
        "    cmap_bound = np.abs(attrs).max()\n",
        "    norm = mpl.colors.Normalize(vmin=-cmap_bound, vmax=cmap_bound)\n",
        "    cmap = mpl.cm.get_cmap(cmap)\n",
        "\n",
        "    html_text = \"\"\n",
        "    for i, tok in enumerate(tokens):\n",
        "        if tok is not None:\n",
        "          color = mpl.colors.rgb2hex(cmap(norm(attrs[i])))\n",
        "          html_text += \" <mark style='background-color:{}'>{}</mark>\".format(color, tok)\n",
        "    return (html_text)\n",
        "  \n",
        "  def sequence_to_text(self, list_of_indices):\n",
        "      words = [self.reverse_word_map.get(letter) for letter in list_of_indices]\n",
        "      return words\n",
        "\n",
        "  def _compute_ig(self, inp, base, c, num_reps):\n",
        "      tensor_values = self.sess.run(self.EMBEDDING_TENSOR,\n",
        "                              self._get_feed_dict(inp))\n",
        "\n",
        "      tensor_baseline_values = self.sess.run(self.EMBEDDING_TENSOR,\n",
        "                              self._get_feed_dict(base))\n",
        "      \n",
        "      # print(tensor_values.shape, tensor_baseline_values.shape)\n",
        "      scaled_embeddings = self._get_scaled_inputs(tensor_values[0],\n",
        "                                            tensor_baseline_values[0],\n",
        "                                            num_reps)\n",
        "      \n",
        "      # print(scaled_embeddings) # num_reps x batch_size x emb_shape\n",
        "      scaled_input_feed = {}\n",
        "      \n",
        "      for i, key in enumerate(self.INPUT_TENSORS):\n",
        "          ui = self._get_unscaled_inputs(inp[i])\n",
        "          if 'input/keep_prob:0' in key.name: ### SCALAR VALUE\n",
        "            scaled_input_feed[ self.get_tensor(key.name) ] = 1.0\n",
        "          else:\n",
        "            scaled_input_feed[ self.get_tensor(key.name) ] = ui\n",
        "      \n",
        "      scores = []\n",
        "      path_gradients = []\n",
        "      # print(c)\n",
        "      for i in range(num_reps):\n",
        "          scaled_input_feed[self.EMBEDDING_TENSOR] = scaled_embeddings[i]\n",
        "            \n",
        "          path_gradients_rep, scores_rep = self.sess.run(\n",
        "              [self.get_gradients[c], self.OUTPUT_TENSOR[:,c]], scaled_input_feed)\n",
        "          \n",
        "          path_gradients.append(path_gradients_rep[0])\n",
        "          scores.append(scores_rep)\n",
        "      \n",
        "      # print(scores[0])\n",
        "      baseline_prediction = scores[0][0]  # first score is the baseline prediction\n",
        "      prediction = scores[-1][-1]  # last score is the input prediction\n",
        "\n",
        "      # integrating the gradients and multiplying with the difference of the baseline and input.\n",
        "      ig = np.concatenate(path_gradients, axis=0)\n",
        "      integral = self._calculate_integral(ig)\n",
        "      igs = (tensor_values[0] - tensor_baseline_values[0]) * integral\n",
        "      igs = np.sum(igs, axis=-1)\n",
        "\n",
        "      return igs, baseline_prediction, prediction"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Avl9DXrwReJW"
      },
      "source": [
        "! cp /content/drive/My\\ Drive/IG\\ RESULTS/MEM\\ MODELS/tokenizer.pkl ./"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EWOL_LYQTlH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8e9cba7-f3f4-4246-ebb0-1ea4f09f2799"
      },
      "source": [
        "graph = tf.get_default_graph()\n",
        "IG = integrated_gradients(graph, sess, min = 0, batch_size= 20, num_reps=40)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSUaBs0_aD7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "004adbc7-78ae-462c-ba6e-f2987cc098c3"
      },
      "source": [
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "gdd.download_file_from_google_drive(file_id='1CIEpiDmzLmJ6LMCVSOmCKw_eOg4ocuS4', dest_path='/content/AES.zip', unzip=True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 1CIEpiDmzLmJ6LMCVSOmCKw_eOg4ocuS4 into /content/AES.zip... Done.\n",
            "Unzipping...Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzxogfPLaD97"
      },
      "source": [
        "import re\n",
        "import os as os\n",
        "import numpy as np\n",
        "import itertools\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "def load_training_data(training_df):\n",
        "    resolved_score = training_df['label_orig']\n",
        "    essays = training_df['text']\n",
        "    essay_list = []\n",
        "    # turn an essay to a list of words\n",
        "    for idx, essay in essays.iteritems():\n",
        "        essay = clean_str(essay)\n",
        "        essay_list.append(tokenize(essay))\n",
        "    return essay_list, resolved_score.tolist()\n",
        "\n",
        "def tokenize(sent):\n",
        "    '''Return the tokens of a sentence including punctuation.\n",
        "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
        "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
        "    >>> tokenize('I don't know')\n",
        "    ['I', 'don', '\\'', 'know']\n",
        "    '''\n",
        "    return [x.strip() for x in re.split('(\\W+)', sent) if x.strip()]\n",
        "\n",
        "def clean_str(string):\n",
        "    \"\"\"\n",
        "    Tokenization/string cleaning for all datasets except for SST.\n",
        "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
        "    \"\"\"\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" ( \", string)\n",
        "    string = re.sub(r\"\\)\", \" ) \", string)\n",
        "    string = re.sub(r\"\\?\", \" ? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "\n",
        "    return string.strip().lower()\n",
        "\n",
        "def build_vocab(sentences, vocab_limit):\n",
        "    \"\"\"\n",
        "    Builds a vocabulary mapping from word to index based on the sentences.\n",
        "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
        "    \"\"\"\n",
        "    # Build vocabulary\n",
        "    word_counts = Counter(itertools.chain(*sentences))\n",
        "    print( 'Total size of vocab is {}'.format(len(word_counts.most_common())))\n",
        "    # Mapping from index to word\n",
        "    # vocabulary_inv = [x[0] for x in word_counts.most_common(vocab_limit)]\n",
        "    vocabulary_inv = [x[0] for x in word_counts.most_common(vocab_limit)]\n",
        "    \n",
        "    vocabulary_inv = list(sorted(vocabulary_inv))\n",
        "    # Mapping from word to index\n",
        "    vocabulary = {x: i+1 for i, x in enumerate(vocabulary_inv)}\n",
        "    return [vocabulary, vocabulary_inv]\n",
        "\n",
        "# data is DataFrame\n",
        "def vectorize_data(data, word_idx, sentence_size):\n",
        "    E = []\n",
        "    for essay in data:\n",
        "        ls = max(0, sentence_size - len(essay))\n",
        "        wl = []\n",
        "        for w in essay:\n",
        "            if w in word_idx:\n",
        "                wl.append(word_idx[w])\n",
        "            else:\n",
        "                wl.append(0)\n",
        "        wl += [0]*ls\n",
        "        E.append(wl)\n",
        "    return E"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l-z1gSraEAj"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "names = ['song_beg', 'song_end', 'false_beg','false_end','normal','shuffle', 'syn']\n",
        "adv_data_list = {}\n",
        "for name in names:\n",
        "  adv_data = pd.read_csv('/content/drive/My Drive/IG RESULTS/'+str(essay_set_id)+'_'+name+'.csv')\n",
        "  adv_data_list[name] = adv_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xD0r17OaEC_"
      },
      "source": [
        "E_list = {}\n",
        "M_list = {}\n",
        "for adv_data in adv_data_list.keys():\n",
        "  essay_list, resolved_scores = load_training_data(adv_data_list[adv_data])\n",
        "  E = data_utils.vectorize_data(essay_list, word_idx, len(t[0][0]))\n",
        "  E_list[adv_data]= E\n",
        "  M_list[adv_data] = m[0][:len(E)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xxEvMD2aEQ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31e50f2f-67b8-4d5f-859f-080e65a009a2"
      },
      "source": [
        "e   = E_list['normal']\n",
        "mem = M_list['normal']\n",
        "inp = [e,mem,1.0]\n",
        "labels_orig = IG.predict_batch(inp)\n",
        "labels_orig = [x+0 for x in labels_orig]\n",
        "labels_orig"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 1, 1, 3, 3, 3, 3, 2, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_leBQgU5aETy"
      },
      "source": [
        "from xhtml2pdf import pisa\n",
        "def convert_html_to_pdf(source_html, output_filename):\n",
        "  result_file = open(output_filename, \"w+b\")\n",
        "  pisa_status = pisa.CreatePDF(source_html, dest=result_file)          \n",
        "  result_file.close()\n",
        "\n",
        "def save_stats_add(diff, diff_attr, word_list, ratio,output_filename):\n",
        "  result = open(output_filename, 'w')\n",
        "  result.write('\\ndiff in scores:  '+str(diff))\n",
        "  result.write('\\ndiff in attrs:  '+ str(diff_attr))\n",
        "  result.write('\\nnew words in top 10%:  '+ ', '.join(word_list))\n",
        "  result.write('\\npercent of top words in added words:  '+ str(ratio))\n",
        "  result.close()\n",
        "\n",
        "def save_stats_mod(diff, diff_attr, changed_no,output_filename):\n",
        "  result = open(output_filename, 'w')\n",
        "  result.write('\\ndiff in scores:  '+str(diff))\n",
        "  result.write('\\ndiff in attrs:  '+ str(diff_attr))\n",
        "  result.write('\\nno of words which changed attr:  '+ str(changed_no))\n",
        "  result.close()\n",
        "\n",
        "def save_stats_gen(babel_total, babel_unattrib, output_filename):\n",
        "  result = open(output_filename, 'w')\n",
        "  result.write('\\ntop attributed words:  '+ str(babel_total))\n",
        "  result.write('\\ntop unattributed words:  '+ str(babel_unattrib))\n",
        "  result.close()\n",
        "\n",
        "def top_k_attrs(tokens, attrs, sign = None, k=None):\n",
        "    k = min(k, len(tokens))\n",
        "    \n",
        "    if sign != None:\n",
        "      tokens_list = []\n",
        "      signs_list = []\n",
        "      for i in np.argpartition(attrs, -k)[-k:]:\n",
        "          tokens_list.append(tokens[i].strip())\n",
        "          signs_list.append(sign[i])\n",
        "      return  tokens_list , signs_list\n",
        "    \n",
        "    else:\n",
        "      return ([tokens[i].strip() for i in np.argpartition(attrs, -k)[-k:]])\n",
        "\n",
        "def bottom_k_attrs(tokens, attrs, k):\n",
        "    k = min(k, len(tokens))\n",
        "    return [tokens[i] for i in np.argpartition(attrs, k)[:k]]\n",
        "\n",
        "def save_normal_attrs(data, memory, labels_orig, essay_type):\n",
        "  dir =  ATTRS_DIR+essay_type+'/'\n",
        "  if not os.path.exists(dir):\n",
        "    os.makedirs(dir)\n",
        "\n",
        "  a_total = []\n",
        "  w_total= [] \n",
        "  for i,essay in enumerate(data):\n",
        "    attrs, words= IG.explain(x = data[i][:395], memory= memory[0])\n",
        "    label_new = IG.predict([data[i][:395], memory[0],1.0])\n",
        "    html = IG.visualize_token_attrs(words, attrs)\n",
        "    convert_html_to_pdf(html, dir+str(i)+'_'+str(labels_orig[i])+'_'+str(label_new+0)+'.pdf')\n",
        "    a_total.append(attrs)\n",
        "    w_total.append(words)\n",
        "\n",
        "  return a_total, w_total\n",
        "\n",
        "def subfinder(l, sl):\n",
        "    sll=len(sl)\n",
        "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
        "        if l[ind:ind+sll]==sl:\n",
        "            return ind,ind+sll-1\n",
        "\n",
        "def save_attrs_pdf(data, data_normal, memory, labels_orig, essay_type, type_add = False, type_mod = False, type_gen = False):\n",
        "  dir =  ATTRS_DIR+essay_type+'/'\n",
        "  if not os.path.exists(dir):\n",
        "    os.makedirs(dir)\n",
        "\n",
        "  a,w = data_normal\n",
        "  babel_total = {}\n",
        "  babel_unattrib = {}\n",
        "  pattern_none = [None, None, None, None, None]\n",
        "          \n",
        "  for i,essay in enumerate(data):\n",
        "    attrs, words= IG.explain(x = data[i][:395], memory= memory[0])\n",
        "    labels_new = IG.predict([data[i][:395], memory[0],1.0])\n",
        "    \n",
        "    if type_gen:\n",
        "        try:\n",
        "          loc = subfinder(words, pattern_none)[0] \n",
        "        except Exception as e:\n",
        "          loc = len(words)\n",
        "        attrs_sign= []\n",
        "        for x in attrs:\n",
        "            if x>0:\n",
        "              attrs_sign.append('+')\n",
        "            else:\n",
        "              attrs_sign.append('-')\n",
        "\n",
        "        top_words, top_words_signs = top_k_attrs(words, attrs, attrs_sign, k = int(0.1*loc))\n",
        "        \n",
        "        for j,x in enumerate(top_words):\n",
        "          if x in babel_total.keys():\n",
        "            if top_words_signs[j] == '+':\n",
        "              babel_total[x][0]+=1\n",
        "            else:\n",
        "              babel_total[x][1]+=1\n",
        "          else:\n",
        "            if top_words_signs[j] == '+':\n",
        "              babel_total[x] = [1,0]\n",
        "            else:\n",
        "              babel_total[x] = [0,1]\n",
        "\n",
        "        attrs_abs = [abs(x) for x in attrs]\n",
        "        bottom_words = bottom_k_attrs(words[:loc], attrs_abs[:loc], k = int(0.1*loc))\n",
        "        \n",
        "        for j,x in enumerate(bottom_words):\n",
        "          if x in babel_unattrib.keys():\n",
        "            babel_unattrib[x]+= 1\n",
        "          else:\n",
        "            babel_unattrib[x] = 1\n",
        " \n",
        "        html = IG.visualize_token_attrs(words, attrs)\n",
        "        convert_html_to_pdf(html, dir+str(i)+'_'+str(labels_new+0)+'.pdf')\n",
        "\n",
        "    else:\n",
        "      html = IG.visualize_token_attrs(words, attrs)\n",
        "      convert_html_to_pdf(html, dir+str(i)+'_'+str(labels_orig[i])+'_'+str(labels_new+0)+'.pdf')\n",
        "    \n",
        "  if type_gen:\n",
        "    del babel_unattrib[None]\n",
        "    save_stats_gen(babel_total, babel_unattrib, dir+'stats_word_attributions.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpIZop3ptRdF"
      },
      "source": [
        "e_normal = E_list['normal']\n",
        "m_normal = M_list['normal']\n",
        "data_normal = save_normal_attrs(e_normal , m_normal, labels_orig, 'normal')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LktF5chJ6GW6"
      },
      "source": [
        "gc.collect()\n",
        "e_add_song = E_list['song_beg']\n",
        "memory = M_list['normal']\n",
        "save_attrs_pdf(e_add_song, data_normal, memory, labels_orig, 'song_beg')\n",
        "\n",
        "gc.collect()\n",
        "e_add_song = E_list['song_end']\n",
        "memory = M_list['normal']\n",
        "save_attrs_pdf(e_add_song, data_normal, memory, labels_orig, 'song_end')\n",
        "\n",
        "gc.collect()\n",
        "e_add_song = E_list['false_beg']\n",
        "memory = M_list['normal']\n",
        "save_attrs_pdf(e_add_song, data_normal, memory, labels_orig, 'false_beg')\n",
        "\n",
        "gc.collect()\n",
        "e_add_song = E_list['false_end']\n",
        "memory = M_list['normal']\n",
        "save_attrs_pdf(e_add_song, data_normal, memory, labels_orig, 'false_end')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEiCOA9byWyH"
      },
      "source": [
        "gc.collect()\n",
        "e_add_song = E_list['syn']\n",
        "memory = M_list['normal']\n",
        "save_attrs_pdf(e_add_song, data_normal, memory, labels_orig, 'syn', type_mod = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6Td7-V0AQvh"
      },
      "source": [
        "gc.collect()\n",
        "e_add_song = E_list['shuffle']\n",
        "memory = M_list['normal']\n",
        "save_attrs_pdf(e_add_song, data_normal, memory, labels_orig, 'shuffle', type_mod = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2E1wdIMaAQy5"
      },
      "source": [
        "gc.collect()\n",
        "babel_data = pd.read_csv('/content/AES_testcases/prompt'+str(essay_set_id)+'/prompt 3 babel - Sheet1.csv', names= ['text'])\n",
        "babel_data['label_orig'] = min_score\n",
        "\n",
        "essay_list, resolved_scores = load_training_data(babel_data)\n",
        "E_babel = data_utils.vectorize_data(essay_list, word_idx, len(t[0][0]))\n",
        "\n",
        "e_add_song = E_babel\n",
        "memory = M_list['normal']\n",
        "save_attrs_pdf(e_add_song, data_normal, memory, labels_orig, 'babel', type_gen = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAnXvkJKHWq8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrDrXneCdQiJ"
      },
      "source": [
        "###NORMAL STUFF:\n",
        "def top_k_attrs(tokens, attrs, sign = None, k=None):\n",
        "    k = min(k, len(tokens))\n",
        "    \n",
        "    if sign != None:\n",
        "      tokens_list = []\n",
        "      signs_list = []\n",
        "      for i in np.argpartition(attrs, -k)[-k:]:\n",
        "          tokens_list.append(tokens[i].strip())\n",
        "          signs_list.append(sign[i])\n",
        "      return  tokens_list , signs_list\n",
        "    \n",
        "    else:\n",
        "      return ([tokens[i].strip() for i in np.argpartition(attrs, -k)[-k:]])\n",
        "\n",
        "# ATTRS_TSV = '/content/drive/My Drive/IG RESULTS/P1/MEMORY NET/attrs.tsv'\n",
        "lines = []\n",
        "with open(ATTRS_TSV) as f:\n",
        "    for line in f:\n",
        "      lines.append(line)\n",
        "lines = list(set(lines))\n",
        "\n",
        "def get_counts_list_normal( top_k = 10, is_abs = False, is_sign = False, is_percent = None):\n",
        "  \n",
        "  essay_list = []\n",
        "  counts_list = []\n",
        "  signs_list = []\n",
        "  for line in lines:\n",
        "      line = line.strip()\n",
        "      question_attrs = line.split('\\t')[0]\n",
        "      question_tokens = []\n",
        "      attrs = []\n",
        "      for word_attr in question_attrs.split('||'): \n",
        "          word, attr = word_attr.split('|')\n",
        "          question_tokens.append(word)\n",
        "          if is_abs:\n",
        "            attrs.append(abs(float(attr)))\n",
        "          else:\n",
        "            attrs.append(float(attr))\n",
        "\n",
        "      essay_list.append(question_tokens)\n",
        "      if is_percent!=None:\n",
        "        top_k = int(is_percent*len(question_tokens))\n",
        "  \n",
        "      if top_k == None:\n",
        "        k = len(question_tokens)\n",
        "      else:\n",
        "        k = min(top_k, len(question_tokens))\n",
        "      \n",
        "      if is_sign:\n",
        "        signs = []\n",
        "\n",
        "        for i in attrs:\n",
        "          if i>0:\n",
        "            signs.append('+')\n",
        "          else:\n",
        "            signs.append('-')\n",
        "      # get top k words by attribution \n",
        "      c_list, sign_list = top_k_attrs(question_tokens , attrs, signs,  k = k)\n",
        "      counts_list.extend(c_list)\n",
        "      signs_list.extend(sign_list)\n",
        "\n",
        "  return counts_list, signs_list, essay_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1and40sGemqi"
      },
      "source": [
        "counts_list, signs_list, essay_list = get_counts_list_normal(is_percent = 0.1, is_sign = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6bfJONHeu4D"
      },
      "source": [
        "signs_dict= {}\n",
        "frequent_attributions = Counter(counts_list).most_common(50)\n",
        "for i in frequent_attributions:\n",
        "  w = i[0]\n",
        "  for j,w2 in enumerate(counts_list):\n",
        "    if w ==w2 :\n",
        "      if w not in signs_dict.keys():\n",
        "        signs_dict[w] = []\n",
        "      else:\n",
        "        signs_dict[w].append(signs_list[j])\n",
        "\n",
        "for k,v in signs_dict.items():\n",
        "  signs_dict[k] = Counter(v)\n",
        "\n",
        "with open(ATTRS_DIR+'NORMAL_word_importance.txt','w') as f:\n",
        "  f.write(str(signs_dict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtC6OMC9t5lF"
      },
      "source": [
        "###NORMAL UNATTRIBUTED STUFF:\n",
        "def bottom_k_attrs(tokens, attrs, k=None):\n",
        "    k = min(k, len(tokens))    \n",
        "    return ([tokens[i].strip() for i in np.argpartition(attrs, k)[:k]])\n",
        "\n",
        "def neg_k_attrs(tokens, attrs, k=None):\n",
        "    k = min(k, len(tokens))    \n",
        "    return ([tokens[i].strip() for i in np.argpartition(attrs, k)[:k] if attrs[i]<0 ])\n",
        "\n",
        "lines = []\n",
        "with open(ATTRS_TSV) as f:\n",
        "    for line in f:\n",
        "      lines.append(line)\n",
        "lines = list(set(lines))\n",
        "\n",
        "def get_bottom_list_normal( top_k = 10, is_abs = True, is_percent = None, is_neg= False):\n",
        "  \n",
        "  counts_list = []\n",
        "  signs_list = []\n",
        "  for line in lines:\n",
        "      line = line.strip()\n",
        "      question_attrs = line.split('\\t')[0]\n",
        "      question_tokens = []\n",
        "      attrs = []\n",
        "      for word_attr in question_attrs.split('||'): \n",
        "          word, attr = word_attr.split('|')\n",
        "          question_tokens.append(word)\n",
        "          if is_abs:\n",
        "            attrs.append(abs(float(attr)))\n",
        "          else:\n",
        "            attrs.append(float(attr))\n",
        "\n",
        "      if is_percent!=None:\n",
        "        top_k = int(is_percent*len(question_tokens))\n",
        "  \n",
        "      if top_k == None:\n",
        "        k = len(question_tokens)\n",
        "      else:\n",
        "        k = min(top_k, len(question_tokens))\n",
        "      \n",
        "      # get top k words by attribution \n",
        "      if is_neg:\n",
        "        c_list = neg_k_attrs(question_tokens , attrs, k = k)\n",
        "      else:\n",
        "        c_list = bottom_k_attrs(question_tokens , attrs, k = k)\n",
        "      counts_list.extend(c_list)\n",
        "      \n",
        "  return counts_list\n",
        "\n",
        "counts_list_unattrib = get_bottom_list_normal(is_percent = 0.1)\n",
        "counts_list_neg = get_bottom_list_normal(is_percent = 0.1, is_abs = False, is_neg = True)\n",
        "\n",
        "with open(ATTRS_DIR+'NORMAL_bottom_attributed_words.txt','w') as f:\n",
        "  f.write(str(Counter(counts_list_unattrib).most_common(50)))\n",
        "with open(ATTRS_DIR+'NORMAL_negative_attributed_words.txt','w') as f:\n",
        "  f.write(str(Counter(counts_list_neg).most_common(50)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceBjk59xEw96"
      },
      "source": [
        "def plot_and_save(curve_data, filename, x = 'num. words in vocab', y= 'relative accuracy', title= 'title'):\n",
        "  import matplotlib.pyplot as plt\n",
        "  OVERSTABILITY_CURVE_FILE = filename\n",
        "  plt.plot(list(curve_data.keys()), list(curve_data.values()))\n",
        "  # plt.xscale('symlog')\n",
        "  plt.xlabel(x)\n",
        "  plt.ylabel(y)\n",
        "  plt.title(title)\n",
        "\n",
        "  plt.savefig(OVERSTABILITY_CURVE_FILE, format='eps')\n",
        "  plt.savefig(OVERSTABILITY_CURVE_FILE.replace('eps','png'), format='png')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "def plot_and_save_both(a,b, filename, x = 'num. words in vocab', y= 'relative accuracy', title= 'title'):\n",
        "  import matplotlib.pyplot as plt\n",
        "  OVERSTABILITY_CURVE_FILE = filename\n",
        "  plt.plot(a,b)\n",
        "  # plt.xscale('symlog')\n",
        "  ax = plt.gca()\n",
        "  ax.invert_xaxis()\n",
        "  plt.xlabel(x)\n",
        "  plt.title(title)\n",
        "  plt.ylabel(y)\n",
        "  plt.savefig(OVERSTABILITY_CURVE_FILE, format='eps')\n",
        "  plt.savefig(OVERSTABILITY_CURVE_FILE.replace('eps','png'), format='png')\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaISuSXEuaKG"
      },
      "source": [
        "# ### NORMAL ATTR %%%%\n",
        "# pattern_none = [None, None, None, None, None]\n",
        "# data = e_normal\n",
        "# memory = m_normal\n",
        "# for i in range(len(data)):\n",
        "#   attrs, words = IG.explain(data[i][:395], memory = memory[0])\n",
        "#   loc = subfinder(words, pattern_none)[0]\n",
        "#   attrs = attrs[:loc]\n",
        "#   attrs = [abs(x) for x in attrs]\n",
        "#   words = words[:loc]\n",
        "#   s= {}\n",
        "#   l = len(attrs)\n",
        "#   for j in range(0, l, l//5):\n",
        "#     s[(j)] = sum(attrs[j: j+ l//5])\n",
        "  \n",
        "#   dir = ATTRS_DIR+'normal/'\n",
        "#   plot_and_save(s, dir+str(i)+'_attrs_variation.txt', title = str(i)+'_attrib_graph')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vvyaXK3Qd_r"
      },
      "source": [
        "with open('/content/drive/My Drive/IG RESULTS/MEM MODELS/tokenizer.pkl', 'rb') as f:\n",
        "    word_to_idx = pickle.load(f)\n",
        "\n",
        "idx_to_word = {v: k for k, v in word_to_idx.items()}"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyWbOpENvRex"
      },
      "source": [
        "lines = []\n",
        "with open(ATTRS_TSV) as f:\n",
        "    for line in f:\n",
        "      lines.append(line)\n",
        "lines = lines[-1097:]\n",
        "lines = list(set(lines))\n",
        "\n",
        "def get_essay_list(tsv_lines):\n",
        "  \n",
        "  essay_list = []\n",
        "  attrs_list = []\n",
        "  for line in tsv_lines:\n",
        "      line = line.strip()\n",
        "      question_attrs = line.split('\\t')[0]\n",
        "      question_tokens = []\n",
        "      attrs = []\n",
        "      for word_attr in question_attrs.split('||'): \n",
        "          word, attr = word_attr.split('|')\n",
        "          question_tokens.append(word)\n",
        "          attrs.append(float(attr))\n",
        "      question_tokens_idx = [word_to_idx[x] for x in question_tokens] + [0]*(395 - len(question_tokens))\n",
        "      essay_list.append(question_tokens_idx)\n",
        "      attrs_list.append(attrs)\n",
        "\n",
        "  return essay_list, attrs_list\n",
        "\n",
        "essay_list, attrs_list = get_essay_list(lines)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2rQgXimR36y"
      },
      "source": [
        "w1 = graph.get_tensor_by_name(\"input/question:0\")\n",
        "w2 = graph.get_tensor_by_name(\"input/memory_key:0\")\n",
        "w3 = graph.get_tensor_by_name(\"input/keep_prob:0\")\n",
        "INPUT_TENSORS = [w1,w2,w3]\n",
        "PRED_TENSOR = graph.get_tensor_by_name('prediction/predict_op:0')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vKGj0DStBVz"
      },
      "source": [
        "mem_total = []\n",
        "for i in range(len(essay_list)):\n",
        "  mem_total.append(m[0][0])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nt7zobGVjlHA"
      },
      "source": [
        "feed = {}\n",
        "pred_array_orig = []\n",
        "input_df = [np.array(essay_list), np.array(mem_total), 1]\n",
        "for i, key in enumerate(INPUT_TENSORS):\n",
        "    feed[key.name] = input_df[i]\n",
        "pred = sess.run(PRED_TENSOR,feed)\n",
        "pred_array_orig.extend(pred)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdPo0NC34nX_"
      },
      "source": [
        "def npos(orig, new):\n",
        "  count = 0\n",
        "  for i in range(len(orig)):\n",
        "    if new[i]>orig[i]:\n",
        "      count+=1 \n",
        "  return (count/len(orig))*100\n",
        "\n",
        "def nneg(orig, new):\n",
        "  count = 0\n",
        "  for i in range(len(orig)):\n",
        "    if new[i]<orig[i]:\n",
        "      count+=1 \n",
        "  return (count/len(orig))*100\n",
        "\n",
        "def nsame(orig, new):\n",
        "  count = 0\n",
        "  for i in range(len(orig)):\n",
        "    if new[i]==orig[i]:\n",
        "      count+=1 \n",
        "  return (count/len(orig))*100\n",
        "\n",
        "def mu(orig, new):\n",
        "  s=0\n",
        "  n = len(orig)\n",
        "  for i in range(n):\n",
        "    s+=(orig[i] - new[i])\n",
        "  return (s/n)/4\n",
        "\n",
        "def absmu(orig, new):\n",
        "  s=0\n",
        "  n = len(orig)\n",
        "  for i in range(n):\n",
        "    s+=(orig[i] - new[i])\n",
        "  return (abs(s)/n)/4\n",
        "\n",
        "def sd(orig, new):\n",
        "  mu_val = mu(orig, new)\n",
        "  s=0\n",
        "  n = len(orig)\n",
        "  for i in range(n):\n",
        "    s+=(orig[i] - new[i] - mu_val)**2\n",
        "  return ((s/n)**(1/2))/4\n",
        "\n",
        "def mupos(orig, new):\n",
        "  s=0\n",
        "  n = len(orig)\n",
        "  for i in range(n):\n",
        "    if new[i]>orig[i]:\n",
        "      s+=(orig[i] - new[i])\n",
        "  return (s/n)/4\n",
        "\n",
        "def muneg(orig, new):\n",
        "  s=0\n",
        "  n = len(orig)\n",
        "  for i in range(n):\n",
        "    if orig[i] > new[i]:\n",
        "      s+=-(orig[i] - new[i])\n",
        "  return (s/n)/4\n",
        "\n",
        "def get_pred_stats(orig, new, filename, K):\n",
        "  b = ('NPOS',  npos(orig, new))\n",
        "  c = ('NNEG',  nneg(orig, new))\n",
        "  d = ('NSAME', nsame(orig, new))\n",
        "  e = ('MU',    mu(orig, new))\n",
        "  f = ('ABSMU', absmu(orig, new))\n",
        "  g = ('SD',    sd(orig, new))\n",
        "  h = ('MUPOS', mupos(orig, new))\n",
        "  i = ('MUNEG', muneg(orig, new))\n",
        "  with open(filename, 'a') as file:\n",
        "    file.write(str(K)+\"___\"+str([b,c,d,e,f,g,h,i])+ \"\\n\")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3996U1BjlKX"
      },
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "from random import randint\n",
        "\n",
        "import numpy as np\n",
        "def top_k_attrs(tokens, attrs,k=None):\n",
        "    k = min(k, len(tokens))\n",
        "    return ([tokens[i] for i in np.argpartition(attrs, -k)[-k:]])\n",
        "\n",
        "d = {}\n",
        "p_list = {}\n",
        "for K in range(0,6):\n",
        "  \n",
        "  percent = K*0.2\n",
        "  preds_new = []\n",
        "  new_essay_list = []\n",
        "  avg_len = 0\n",
        "  c_list_total = []\n",
        "\n",
        "  # chosen_id = randint(0, 10)  \n",
        "  for id, essay in enumerate(essay_list):\n",
        "    \n",
        "    top_k = int(percent* len(attrs_list[id]))\n",
        "    attrs = attrs_list[id]\n",
        "    question_tokens = essay_list[id]\n",
        "    try:\n",
        "      c_list = top_k_attrs(question_tokens , attrs, k = top_k)\n",
        "    except Exception as e:\n",
        "      c_list = list(set(question_tokens))\n",
        "    if top_k==0:\n",
        "      c_list = []\n",
        "    c_list_total.extend(c_list)\n",
        "    new_essay = []\n",
        "    count = 0\n",
        "    for i in range(len(question_tokens)):  \n",
        "      if question_tokens[i] in c_list and count<top_k:\n",
        "        new_essay.append(question_tokens[i])\n",
        "        count+=1\n",
        "    \n",
        "    # print(len(new_essay)/  len(question_tokens[:len(attrs)])  )\n",
        "    new_essay = new_essay + [0]*(395 - len(new_essay))\n",
        "    # if id == chosen_id and K>1 and K<5:\n",
        "    #   save_attrs(new_essay, mem_total, K, essay_type='incomplete_data')\n",
        "    new_essay_list.append(new_essay)\n",
        "    \n",
        "    # input_df = [np.array([new_essay]), np.array([mem_total[0]]), 1]\n",
        "    # for i, key in enumerate(INPUT_TENSORS):\n",
        "    #     feed[key.name] = input_df[i]\n",
        "    # pred = sess.run(PRED_TENSOR,feed)[0]\n",
        "    # print(pred_array_orig[id],pred)\n",
        "    # # print(abs(pred - pred_array_orig[id]))\n",
        "    # if round(abs(round(pred) - pred_array_orig[id])) == 1:\n",
        "    #   if id not in p_list.keys():\n",
        "    #     p_list[id] = percent*100\n",
        "    #   else:\n",
        "    #     pass\n",
        "\n",
        "  avg_len/=len(new_essay_list)\n",
        "  c_len = len(set(c_list_total))\n",
        "  feed = {}\n",
        "  input_df = [np.array(new_essay_list), np.array(mem_total), 1]\n",
        "  for i, key in enumerate(INPUT_TENSORS):\n",
        "      feed[key.name] = input_df[i]\n",
        "  pred = sess.run(PRED_TENSOR,feed)\n",
        "  preds_new.extend(pred)\n",
        "  preds_new = [int(x) for x in preds_new]\n",
        "\n",
        "  acc = cohen_kappa_score(preds_new, pred_array_orig, weights='quadratic')\n",
        "  get_pred_stats(pred_array_orig, preds_new, ATTRS_DIR+'stats_top.txt', int(percent*100))\n",
        "  d[percent*100] = acc"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCS7HIoq-pz4",
        "outputId": "039820d7-bfd7-40a1-ddd8-daa38c1e4d5e"
      },
      "source": [
        "l = p_list.values()\r\n",
        "sum(l)/len(l)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19.81981981981982"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "WsASTuCnvJ66",
        "outputId": "e773d322-9126-467d-fca4-f16e316c8ede"
      },
      "source": [
        "d\n",
        "plot_and_save(d,ATTRS_DIR+'adding_top', x = '% length of response', y ='relative QWK', title= 'iteratively adding words(in order of importance)')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWjgOi1ToZ7q"
      },
      "source": [
        "def bottom_k_attrs(tokens, attrs,k=None):\n",
        "    k = min(k, len(tokens))\n",
        "    return ([tokens[i] for i in np.argpartition(attrs, k)[:k]])\n",
        "\n",
        "d2_keys = []\n",
        "d2_vals = []\n",
        "for K in range(0,6):\n",
        "  percent = K*0.2\n",
        "  preds_new = []\n",
        "  new_essay_list = []\n",
        "  avg_len = 0\n",
        "  c_list_total = []\n",
        "  for id, essay in enumerate(essay_list):\n",
        "    top_k = int(percent* len(attrs_list[id]))\n",
        "    attrs = attrs_list[id]\n",
        "    question_tokens = essay_list[id]\n",
        "    attrs = [abs(x) for x in attrs]\n",
        "    try:\n",
        "      c_list = bottom_k_attrs(question_tokens , attrs, k = top_k)\n",
        "    except Exception as e:\n",
        "      c_list = list(set(question_tokens))\n",
        "    if top_k==0:\n",
        "      c_list = []\n",
        "\n",
        "    # c_list_total.extend(c_list)\n",
        "    new_essay = []\n",
        "    count = 0\n",
        "    for i in range(len(question_tokens[:len(attrs)])):  \n",
        "      if question_tokens[:len(attrs)][i] in c_list and count<top_k:\n",
        "        count+=1\n",
        "        pass\n",
        "      else:\n",
        "        new_essay.append( question_tokens[:len(attrs)][i])\n",
        "    # avg_len+=len(new_essay)\n",
        "    # print(len(new_essay)/  len(question_tokens[:len(attrs)]) , new_essay, question_tokens[:len(attrs)])\n",
        "    \n",
        "    new_essay = new_essay + [0]*(395 - len(new_essay))\n",
        "    new_essay_list.append(new_essay)\n",
        "\n",
        "    # input_df = [np.array([new_essay]), np.array([mem_total[0]]), 1]\n",
        "    # for i, key in enumerate(INPUT_TENSORS):\n",
        "    #     feed[key.name] = input_df[i]\n",
        "    # pred = sess.run(PRED_TENSOR,feed)[0]\n",
        "    # # print(abs(pred - pred_array_orig[id]))\n",
        "    # if round(abs(pred - pred_array_orig[id])) == 1:\n",
        "    #   if id not in p_list.keys():\n",
        "    #     p_list[id] = percent*100\n",
        "    #   else:\n",
        "    #     pass\n",
        "\n",
        "  avg_len/=len(new_essay_list)\n",
        "  c_len = len(set(c_list_total))\n",
        "  feed = {}\n",
        "  input_df = [np.array(new_essay_list), np.array(mem_total), 1]\n",
        "  for i, key in enumerate(INPUT_TENSORS):\n",
        "      feed[key.name] = input_df[i]\n",
        "  pred = sess.run(PRED_TENSOR,feed)\n",
        "  preds_new.extend(pred)\n",
        "  preds_new = [int(x) for x in preds_new]\n",
        "  \n",
        "  acc = cohen_kappa_score(preds_new, pred_array_orig, weights='quadratic')\n",
        "  get_pred_stats(pred_array_orig, preds_new, ATTRS_DIR+'stats_bottom.txt', int(100-percent*100))\n",
        "  # print(percent*100, avg_len, acc)\n",
        "  d2_keys.append(100-percent*100)\n",
        "  d2_vals.append(acc)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKLXBjRZ-7eY",
        "outputId": "711249c4-23b4-4c15-a927-359d0438ff21"
      },
      "source": [
        "l = p_list.values()\r\n",
        "sum(l)/len(l)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICe4ejhmoZ5R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "1e168f25-f261-4fdd-92f3-16a8d9ebefc6"
      },
      "source": [
        "plot_and_save_both(d2_keys, d2_vals, ATTRS_DIR+'removing_bottom', x = '% length of response', y ='relative QWK', title= 'iteratively removing words(in reverse order of importance)')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ4AAAEYCAYAAABslZDKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd5gUVdbA4d+ZQBwyA5KHjCQFBiSuoKtgxCxGTBhRWfN+u67o6gZdd02YExjAuAiKugYwEJRBCZKjgKQhJ0kz5/vj3oGmndDAdNdM93mfp5+pXKdqqur0rbpdV1QVY4wxJlaSgg7AGGNMYrHEY4wxJqYs8RhjjIkpSzzGGGNiyhKPMcaYmLLEY4wxJqbiIvGIyGwR6R3g+huKyHYRST7C5fQWkZXFFVdJICLPisi9QcdxJERERaRZBNOVFZE5IlLH95f6bQ9SrM8HEXlQRNaLyJp8xvUSkfmxiqU0EJGbReSfhzVvvP2OR0SGAs1U9dIormMZcI2qfl7My+0NvK6q9YtzuebIiIgCzVV1URHT3Qy0UdXrYxNZfIvl+SAiDYH5QCNVXRft9R2JknKdEJFywCKg46Hus7go8RQnEUkJOoYjFQ/bEIRi2G/XA68VRyyhxInJuRrUsVPc6z2M5TUENpSCpFNizm1V3QV8DFx+ODOX+g+wDPg90A/YA+wFtgMz/PgqwEvAauAX4EEg2Y+7ApgI/AfY4Mc1Bb70/euBN4CqfvrXgFzgV7+Ou4AMQIEU4EIgKyy+PwBjfHdZ4F/AcmAt8CxQ3o/rDaz03XcC74Ut5wng8UL2wd3ATGC3j6UrMAnYDMwAeodMP8Fv6yS/HWOBGn5btwJTgYyQ6bv7YVv83+5+eFHb+yrwYOj2AbcD6/z/48qQ+Wr4OPLW/yDwbQHbOxy43XfX8/v/Jt/fFNgIJPn+QbhvZhuBMUDdkOUocBOwEFgasu9XA6uAq/w0zfy4U4E5wDbcsXSHH97QHxMpIcuOeNvz2b4JwEO4Y/NXoBnQCvjMb8d84AI/7XHAGvwx7YedDcz03UnAPcBi3DH9NlDdj8vw23c17pj8GigHvO6n3ez/F7WLOpfy2YaywGN+P67y3WXD9sfdPvbXgPJ+n23y+/hO/Png56kLvAdkA0uBW0LGDQXe9XFvxd2RCI+nCjDCz/8z8Ge/b37v93Eu7lx4NZ95e4fFsszHNxPY4fdJbdyFeBvwOVAtbB9f6/fDavxxcxj76Z2wWLf7/dIFmOz/X6uBp4AyYcf59bjjfDMwDH/HK+Qcmetjn4MrxRS6z/34S4Dxh3zNjkViiPbHHwS/DzkAXw8b/1/gOaAiUAv4HrjOj7sC2AfcjLtYl8ed5Cf5AyIddzI+lt/6wg6sFKCC/+c1Dxk/FRjgu/+Du/hVByrhLrR/Dz+4gTr+gM5LeCm4C1anQvbBdKCB34Z6uAvHqbiT6yTfnx5yYVuEu0hX8QfbAtxJmII7QV/x01bHXQwu8+Mu8v01ItjeVzn44rsPeABI9bHt5MAJOsp/KgCtgRUUnHiuAsb67otxF9W3QsZ94LtPwH156Oj/n08CX4edkJ/5bSyP+/KyFmiLO17e5ODEsxro5burceAEPQ2YHRZjxNuez/ZNwCWCNn6fV/H740rf38FvV2s//WLgpJD53wHu8d23AlOA+n4fPAeMDDt2R/jtLQ9chzsuKwDJQCegclHnUj7b8IBfby3ceTQJ+GvY/vinj6k88A/gG/+/aAD8xIHzIQmYBvwFKAM0AZYAfUPO+73AWX7a8vnEMwL4AHfeZeCO96vDz70CtuWg8bjzbQou2dTDnZs/+P9LOdwX1/vC9vFIv9/a4S7kvz/M/fSbWP3/qCvu2MjAJZEhYcf5h0BV3JekbKCfH3c+7ktEZ0Bw179GRe1zP29HYOMhX7NjmSCi9aGQxOMPjN2hByLuwjned18BLC9i+WcBP+a3vrADK8X3vw78xXc3x12YK/h/6g6gaci83TjwTTv84P4YGOS7TwfmFLEPrgrpvxt4LWyaT4GBvnsC8KeQcY8CH4f0nwFM992XAd+HLWsycEVh2+v7X+Xgi294qWCdP2GScReOliHjCivxNMUlvyRcqfE6DlykhgO3+e6XgIdD5kvz68nQAyfkCSHjXwb+EdLfgoMTz3K/rsph8VwCTAkbFtG2F7B9E4AHQvovBL4Jm+Y5DlzcHgRe9t2V/HHWyPfPBU4Mma+O3wd5FykFmoSMvwp38Wsftr5Cz6V8tmExcGpIf19gWcj+2AOUCxm/BH8x9P3XhvxPjyPsPAX+yIEvR0MJ+UKRTyzJfn2tQ4ZdB0zI79zLZ/6DxuPOt0tC+t8DngnpvxkYHXZ9aBUy/mHgpcPcT4XG6qcZAvw3pF+BniH9b3Pgi8mnwK35LKPQfR5yvucUFkt+n0R4xtMI9w1ztYhsFpHNuBO2Vsg0K0JnEJHaIjJKRH4Rka24C2vNQ1jnm7gTEty38dGquhP3baYCMC0klk/88PwMB/IqSVxK0c8PQrejEXB+3nr8unriLjp51oZ0/5pPf5rvrou7NRHqZ9w3PSh4e/OzQVX3hfTv9OtJx10IQ7fhoP9LKFVdjLu4Hgv0wn2bWyUiLYHjga/yi11Vt+NKfvVCFhe6nrph/eHbfS6utPKziHwlIt388E24C35hCtr2goT/P48L+39eAhzlx78JnCMiZYFzgB9U9eeQef8bMt9cIAeXSPJb12u4i9EoEVklIg+LSCqRnUuhwo+bn/2wPNnqnhOETl/Qvm8E1A3b/v8rZBvC1fSxh8dTL//JIxLp+ZNffKH74lD302+ISAsR+VBE1vhr1t/47TUrtLZe6LHXAJf8wkWyzyvhbr8fknhMPBrWvwL3La2mqlb1n8qq2qaQef7mh7VT1cq4i74UMn24z4B0ETkWd0F+0w9fjzsg24TEUkVVC7r4jAbai0hbXInnjSLWGxrXClyJp2rIp6Kq/qOIZeRnFe4gDNUQVzyHgrf3UGTjbimE1tRpUMQ8XwHn4e5l/+L7B+JugU3PL3YRqYi7RfhLyHJC99vqsPU2DF2hqk5V1f64i+1o3DdHcPf6Gxfzw9/w/+dXYf/PNFW9wcc1B3fBOgWX/N8Mm/eUsHnL+X32m3Wp6l5VvV9VW+Oe7Z2Oe4AcybkUKvy4aeiH5bd9UPi+X4G7MxC6DZVU9dRClhdqPa6UFx7PL/lPHhXh25a3Lw51P+W3nc8A83C3vCvjEoTkM11+VuDuIOQ3vKh9fjTu+fEhicfEsxbIyKsFpKqrgf8Bj4pIZRFJEpGmInJ8IcuohHtot0VE6uEeIoavo0lBM6vqXtw99kdw96s/88NzgReA/4hILQARqScifQtYzi7cA9M3cbe6lhe+6Qd5HThDRPqKSLKIlPO/izicKpjjgBYicrGIpIjIhbhnMB8Wtr2HQlVzgPeBoSJSQURaUXRtma+AwbhncOBuTw3G3Z7L8cNGAleKyLG+NPA34DtVXVbAMt8GrhCR1iJSAbgvb4SIlBGRS0Skit/mrbiHvKjqStwzsy6Hst2H4EPc/+AyEUn1n84icnTING/inuf8Dvf/yPMs8JCINPLbkS4i/QtakYj0EZF2/ndpW3EX7NzDOJdGAn/266uJe1bweiHb+DbwRxGp5o/Tm0PGfQ9sE5G7RaS8P6bbikjnQpa3nz8e3vb7oZLfF7cVEU9xu9cf221wz+re8sMPdT+tBWqISJWQYZVw/6vt/ty54RDiehG4Q0Q6+RqUzfz+iWSfH497JHBI4jHx5J1wG0TkB999Oe7h2BzcLZF3OfiWU7j7cQ/NtgAf4S6Iof6OO1A2i8gdBSzjTdyD+nfCbq/cjbtATfFF4s+BloXEMhz3MPKQqumq6gqgP+6bTzbu28udHMb/XFU34L713o67TXUXcLqqrg+ZrKDtPRSDcQ/R82o5jcR9wy7IV7gTLi/xfIu7lZnXj7rfWt2Luwe/GvfNbkBBC1TVj3G1ir7E/Z++DJvkMmCZ/99dj7vdlec5P77Yqeo24GRc7Ktw+yjvgXOekbgLwZdh/5vHcRVa/ici23APso8rZHVH4c6Rrbjbcl9x4Pg7lHPpQSALVxqchXv4/mAh670fV2pbiktw+495nzhOx91aXYorwbyIO14idTPu9uwS3LHyJu6ZXqx8hTumvgD+par/88MPaT+p6jzc/3qJvwbVBe7AlXS34b7cvlXQ/Pks7x1cDco3/fyjcbUeC93n4n7HcyruGnVI4u4HpPFG3A/b5gFHqerWoOOJJXG/ij5KVQcGHUskfInqR9yD/NVBx2NKBhHJwF24U4/gS1mJI+4H0w1U9a5DntcST8nlbxf+G1eD6qqg44k2f4ugDO5bX2fcLb5rVHV0oIEZcwTiNfEciRLzK1hzMP8QfC3u1kO/gMOJlUq4Wwh1cdv+KO53F8aYOGIlHmOMMTEVj5ULjDHGlGCl7lZbzZo1NSMjI+gwjDHGFGHatGnrVfU3P5AvdYknIyODrKysoMMwxhhTBBEJf/MHYLfajDHGxJglHmOMMTFliccYY0xMWeIxxhgTU5Z4jDHGxFTUEo+IvCwi60TkpwLGi4g8ISKLRGSmiHSMVizGGGNKjmiWeF6l8Fe9nIJrva45rqXBZ6IYizHGmBIiaolHVb8GNhYySX9ghDpTgKoiUlhTBUcsJ1d5deJSlmRvx14VZIwxwQjyB6T1OLgp2JV+2G9eJy8i1+JKRTRs2DB8dMTmrNrK0LFzAGhYvQK9W6bTp2UtujapQfkyyYe9XGOMMZErFW8uUNXngecBMjMzD7uo0q5+Fb65qw8TFmQzYd463slayYjJP1MmJYmuTWrQp2U6vVvWonHNisUWuzHGmIMFmXh+4eA2yOsTg/bPG1SvwGVdG3FZ10bs2pvD1GUbGT8vmwkL1nH/2DncP3YOjWpUoE/LWhzfMp1uTWpQLtVKQ8YYU1yCTDxjgMEiMgrXDO+WWLfaWC41mV7N0+nVPJ2/0JrlG3YyYcE6JszPZtTU5bw6aRllU5Lo1rQGvVu40lCGlYaMMeaIRK09HhEZCfQGauIa9boPSAVQ1WdFRICncDXfdgJXqmqRb//MzMzUWLwkdNfeHL5bupEJ810iWrp+BwCNa1bk+Bbp9GlVi+MaV7fSkDHGFEBEpqlq5m+Gl7baXbFKPOF+3rCDCfOzGT9/HZMXb2D3vlzKpSbRrUkN+rSqRe8WtWhYo0LM4zLGmJLKEk8x2rU3hylLNjBhfjYT5q9j2YadADSpWZHeLWvRu2U6Xaw0ZIxJcJZ4omjp+h37b8lNWeJKQ+VTk+netAa9fU25BtWtNGSMSSyWeGLk1z15paF1jJ+fzfKNrjTUNN2Vhvq0rEXnxtUom2KlIWNMfLPEEwBV9aUh92zou6Ub2bMvlwpl8kpD7rZc/WpWGjLGxJ+CEk+p+AFpaSUiNElPo0l6Glf1bMzOPfuYsmTD/t8NfT53HQDNaqXt//FqZoaVhowx8c1KPAFRVZas38H4eev4akE23y3ZyJ4cVxrq0azm/mdD9aqWDzpUY4w5LFbiKWFEhKbpaTRNT+OaXk3YsXsfkxdvYMKCdYyfl81nc9YC0KJ2mrsl1yKdzIzqlEmxJpSMMaWblXhKIFVlcfb2/c+Gvl+6kb05SkVfGurTyj0bqlPFSkPGmJLLSjyliIjQrFYlmtWqtL80NGnxBsbPX8dX87P5ny8Ntaxdid6t0undwj0bSk220pAxpuSzEk8po6osXLd9/++Gpi5zpaG0sin0DHk2dFSVckGHaoxJcFadOk5t372PiYvW73+LwuotuwBodVSl/dW1OzWy0pAxJvYs8SQAVWXB2u3+x6vryFq2iX25SqWyKfRsfqA0VLuylYaMMdFniScBbdu1l4mLNuy/LbdmqysNHV2nMn1apjOwe4YlIWNM1FjiSXCqyvy129yPV+evY9rPm6hYNoW/n9OOU9vVCTo8Y0wcssRjDrIkezt/eGs6M1Zu4bxO9bnvjNZUKpcadFjGmDhSUOKxJ84Jqkl6Gu/e0J1bTmjG+z+s5NQnviFr2cagwzLGJABLPAksNTmJ205uyTvXdwPggucm8+j/5rM3JzfgyIwx8cwSj6FTo+p8fOvvOLdjfZ78chHnPjOJxdnbgw7LGBOnLPEYANLKpvDI+cfwzCUdWb5xJ6c98Q2vT/mZ0vYM0BhT8lniMQc5pV0dPh3yOzpnVOfPo3/i6uFZZG/bHXRYxpg4YonH/EbtyuUYfmUX7jujNd8uWk+/x77mc/9+OGOMOVKWeEy+kpKEK3s05sObe1KrcjmuGZHF//13Fjv37As6NGNMKWeJxxSqRe1KjL6pO9cd34SR3y/ntCe+ZcaKzUGHZYwpxSzxmCKVTUnmj6cczZvXdGX33hzOfWYST36xkH1W7doYcxgs8ZiIdWtag4+H/I5T29Xh0c8WcOHzU1i+YWfQYRljShlLPOaQVCmfyhMXdeDxAceyYO02Tnn8a97JWmHVro0xEbPEYw5L/2Pr8fGtvWhbrwp3vjuTG9/4gU079gQdljGmFLDEYw5b/WoVeHNQV+45pRWfz11L38e+5usF2UGHZYwp4SzxmCOSnCRcf3xTRt/UgyrlU7n85e+5f+xsdu3NCTo0Y0wJZYnHFIs2dasw9uaeXNE9g1cmLuPMp75lzqqtQYdljCmBLPGYYlMuNZmhZ7Zh+FVd2LRzL2cNm8jzXy8mN9cqHhhjDohq4hGRfiIyX0QWicg9+YxvKCLjReRHEZkpIqdGMx4TG8e3SOfTIb+jT6t0/jZuHhe/OIVVm38NOixjTAkRtcQjIsnAMOAUoDVwkYi0Dpvsz8DbqtoBGAA8Ha14TGxVr1iGZy/txMPntWfWyi30fexrxsxYFXRYxpgSIJolni7AIlVdoqp7gFFA/7BpFKjsu6sAdmWKIyLCBZkNGHdrL5rXSuOWkT8yZNSPbPl1b9ChGWMCFM3EUw9YEdK/0g8LNRS4VERWAuOAm/NbkIhcKyJZIpKVnW3VdUubRjUq8vZ13bjtpBaMnbmaUx//hilLNgQdljEmIEFXLrgIeFVV6wOnAq+JyG9iUtXnVTVTVTPT09NjHqQ5cinJSdxyYnPeu6E7qcnCRS9M4R8fz2PPPnvfmzGJJpqJ5xegQUh/fT8s1NXA2wCqOhkoB9SMYkwmYMc2qMpHt/RiQOeGPPvVYs4aNpGFa7cFHZYxJoaimXimAs1FpLGIlMFVHhgTNs1y4EQAETkal3jsXlqcq1g2hb+f044XLs9kzdZdnP7kt7w6cam9782YBBG1xKOq+4DBwKfAXFzttdki8oCInOknux0YJCIzgJHAFWpXn4RxUuvafDKkF92b1mDo2DkMfGUq67buCjosY0yUSWm7zmdmZmpWVlbQYZhipKq8/t1yHvpoDuVTk/n7Oe3p1/aooMMyxhwhEZmmqpnhw4OuXGAMIsJlXRvx0S29qF+tAte/Po273p3B9t3WzLYx8cgSjykxmqan8d4N3RncpxnvTlvJqY9/w7SfNwUdljGmmFniMSVKmZQk7ujbkreu60auKuc/O4l/f7aAvdbMtjFxwxKPKZE6Z1Rn3K29OKtDPZ74YiHnPTuZpet3BB2WMaYYWOIxJVblcqn8+4JjeeriDixbv4NTH/+Gkd8vt2rXxpRylnhMiXd6+7p8MqQXHRtV5Y/vz2LQiGls2L476LCMMYfJEo8pFepUKc9rVx3Hn087mq8XZtP3sW8YP29d0GEZYw6DJR5TaiQlCdf0asKYwT2omVaGK1+dyr2jf+LXPdbMtjGliSUeU+q0Oqoyo2/qwaBejXltys+c9uQ3zFq5JeiwjDERssRjSqVyqcn86bTWvHHNcezcncPZT09k2PhF5Fgz28aUeJZ4TKnWo1lNPhnSi75tj+KRT+cz4PnJrNi4M+iwjDGFsMRjSr2qFcrw1EUd+M+FxzBv9TZOefwb3v9hpVW7NqaEssRj4oKIcHaH+oy7tRet61TmtrdnMHjkj2zeuSfo0IwxYSzxmLjSoHoFRl7blbv6teTTn9bQ77FvmLhofdBhGWNCWOIxcSc5SbixdzNG39SDimWTueTF73jwwzns2mvVro0pCSzxmLjVtl4VPry5F5d3a8SL3y7lrGETmbdma9BhGZPwLPGYuFa+TDIP9G/LK1d0Zv32PZz55ERe/GYJuVbt2pjAWOIxCaFPq1p8OqQXx7dM58GP5nLpS9+xesuvQYdlTEKyxGMSRo20sjx/WSf+cU47pq/YTN//fM2HM1cFHZYxCccSj0koIsKALg0Zd0svmqSnMfjNH3ns8wVBh2VMQrHEYxJSRs2KvHt9N87tWJ/HPl/I2BlW8jEmVizxmISVkpzE385pS+eMatzxzgxmrtwcdEjGJITDSjwiUqa4AzEmCGVTknnm0k7UTCvLoBFZrN26K+iQjIl7BSYeEflLAcOrAP+LWkTGxFjNtLK8ODCTbbv2ce2ILPuhqTFRVliJp6eIPBQ6QERqA18BX0Y1KmNi7Og6lXl8QAdm/rKFu96daS8YNSaKCks8ZwLHiMi/AUSkOTAReFZVH4hFcMbE0kmta3Nn35aMmbGKpycsDjocY+JWgYlHVXcBZwMZIjIS+By4U1WfjVVwxsTaDcc35axj6/LIp/P55Kc1QYdjTFwq7BnPbcDNwHfAycCPQGMRuc2PMybuiAj/OLc9xzSoyh/ems7sVdaktjHFrbBbbZX8pxzwBC7xVAr5GBOXyqUm88JlnahSPpVBw7PI3rY76JCMiStS0ENUEammqptiHE+RMjMzNSsrK+gwTAKYtXIL5z83iTZ1q/DmoOMom5IcdEjGlCoiMk1VM8OHF1bimS8ic0TkBRG5UkRaRDE+Y0qcdvWr8Oj5xzLt50386b8/WU03Y4pJYZULagFn4WqydQPeF5G1IvKBiNwVycJFpJ+IzBeRRSJyTwHTXOAT3GwRefNwNsKYaDmtfR1uPbE5705byYvfLA06HGPiQkphI1V1AbAAeFVEmgKnArfiKhs8XNi8IpIMDANOAlYCU0VkjKrOCZmmOfBHoIeqbhKRWkeyMcZEw60nNmfhum387eO5NKuVRp9WdpgacyQKq9XWXUTuEJH3ROR74CEgGbgUqBLBsrsAi1R1iaruAUYB/cOmGQQMy3uWpKrrDmcjjImmpCThX+cfQ+s6lbl55I8sXLst6JCMKdUKe8bzLTAAeB/oraoDVPUxVZ3iE0lR6gErQvpX+mGhWgAtRGSiiEwRkX75LUhErhWRLBHJys7OjmDVxhSvCmVSeOHyTMqlJnP18Cw27YjkFDDG5KewxFMX+BvQAfhERCaJyFMicomINCmm9acAzYHewEXACyJSNXwiVX1eVTNVNTM9Pb2YVm3MoalbtTwvXN6JNVt3ccMb09ibkxt0SMaUSoVVLlijqu+r6h2q+jvg98A84H5gYQTL/gVoENJf3w8LtRIYo6p7VXUp7nlS80PZAGNiqUPDavzz3HZMWbKR+8bMtppuxhyGAisX+LdQdwO6+08HXMIZi6vpVpSpQHMRaYxLOAOAi8OmGY0r6bwiIjVxt96WHOI2GBNTZ3eoz4K123lmwmJaHVWJy7tlBB2SMaVKYbXaFgGT/ecBYKqq/hrpglV1n4gMBj7FVUp4WVVni8gDQJaqjvHjThaROUAO7l1wGw5zW4yJmTtPbsnCtdu4f+wcGtesSK/mdgvYmEgV+OaCgyYSqQygqlujHlER7M0FpqTYvnsf5z49idVbfmX0TT1okp4WdEjGlCiH8+YCRORWEfkFWAosE5EFIjLAj2tQ2LzGxLu0sim8ODCTlOQkrhmexZade4MOyZhSobDf8QwF+gK9VLWGqlYH+gGXiMjdwPjYhGhMydWgegWeuaQjKzbtZPDIH9hnNd2MKVJhJZ5LgHNUdf/Dft99AXAvYE0jGAMc16QGD57Vlm8WruehcXODDseYEq+wygU5vjG4g6jqryLyi68cYIwBLuzckPlrtvPyxKW0qF2Ji7o0DDokY0qswko8v4jIieEDReQEfvt7HGMS3v+d2orftUjn3tE/MWWJVc40piCFJZ5bgOdE5FURudl/hgPPA4NjE54xpUdKchJPXtSBhjUqcMPr01ixcWfQIRlTIhX25oLZQFvgayDDf74G2oa+YdoYc0CV8qm8NLAzuQpXD5/Ktl1W082YcIVWp1bVXar6sqre7j8v5ffcxxhzQOOaFXn6ko4szt7BkFHTycm11+oYE6rQxGOMOTw9mtVk6Bmt+WLeOh75dH7Q4RhTohTaEJwx5vBd1i2D+Wu38exXi2lRO41zOtYPOiRjSoSISjwiUl5EWkY7GGPizX1ntKFbkxrc894spv28KehwjCkRikw8InIGMB34xPcfKyL2Gx5jIpCanMTTl3SkTtVyXPdaFr9sjvg9u8bErUhKPENxzVhvBlDV6UDjKMZkTFypVrEMLw3MZPfeXAYNz2Lnnn1Bh2RMoCJJPHtVdUvYMKumY8whaFarEk9c3IF5a7Zy+9szyLWabiaBRZJ4ZovIxUCyiDQXkSeBSVGOy5i406dlLf7v1KP5+Kc1PPZFJI34GhOfIkk8NwNtgN3Am8AWYEg0gzImXl3dszHnd6rPE18sZOyMVUGHY0wgIqlO3UpV/wT8KdrBGBPvRIQHz27Lsg07uOOdGTSqUYH29asGHZYxMRVJiedREZkrIn8VkbZRj8iYOFc2JZlnLu1EzbSyDBqRxdqt9jIQk1iKTDyq2gfoA2TjXho6S0T+HPXIjIljNdPK8uLATLbt2se1I7LYtTcn6JCMiZmIfkCqqmtU9Qngetxvev4S1aiMSQBH16nMYxcey8xftnD3ezNRtZpuJjFE8gPSo0VkqIjMAvJqtNm7P4wpBie3OYo7Tm7JB9NX8fSExUGHY0xMRFK54GXgLaCvqlo1HGOK2Y29m7Jg7TYe+XQ+zWql0bfNUUGHZExURfKMp5uqPmZJx5joEBH+eW57jmlQlT+8NZ05q7YGHZIxUVVg4hGRt/3fWSIyM+QzS0Rmxi5EY+JfudRkXrisE5XLpTJoRBbrt+8OOiRjoqawEs+t/u/pwBkhn7x+Y0wxqlW5HC9cnsmGHbu5/rVp7N5nNfOwjaEAABc5SURBVN1MfCqs6evVvvNGVf059APcGJvwjEks7epX4V/nH0PWz5v4839/sppuJi5FUp36pHyGnVLcgRhjnNPb1+WWE5vzzrSVvPTt0qDDMabYFVirTURuwJVsmoQ906kETIx2YMYksiEnNmfh2m38bdxcmqan0adVraBDMqbYFFbieRP3LGcMBz/j6aSql8YgNmMSVlKS8OgFx3B0ncrcMvJHFq7dFnRIxhSbwp7xbFHVZap6kX+u8yuuHZ40EWkYswiNSVAVyqTwwuWZlE1N5poRWWzasSfokIwpFhE1fS0iC4GlwFfAMuDjKMdljAHqVi3P85d3YvWWXdz4xg/szckNOiRjjlgklQseBLoCC1S1MXAiMCWShYtIPxGZLyKLROSeQqY7V0RURDIjitqYBNKxYTX+cU47Ji/ZwP1jZwcdjjFHLNKmrzcASSKSpKrjgSIThIgkA8NwNeBaAxeJSOt8pquE+83Qd4cUuTEJ5JyO9bn++Ka8PmU5r01eFnQ4xhyRSBLPZhFJA74G3hCRx4EdEczXBVikqktUdQ8wCuifz3R/Bf4JWKMkxhTizr4t+f3RtRg6dg7fLlwfdDjGHLZIEk9/XMWCPwCfAIuJ7M0F9YAVIf0r/bD9RKQj0EBVPypsQSJyrYhkiUhWdnZ2BKs2Jv4kJwmPDehAs/Q0bnxjGkvXR/L9z5iSJ5KXhO5Q1RxV3aeqw1X1CX/r7YiISBLwb+D2CGJ4XlUzVTUzPT39SFdtTKmVVjaFFwdmkpwkXD18Klt+3Rt0SMYcssJeErpNRLaGfLaF/o1g2b8ADUL66/theSoBbYEJIrIMV4FhjFUwMKZwDapX4NlLO7F8w05uHvkj+6ymmyllCvsdTyVVrRzyqRT6N4JlTwWai0hjESkDDMD9GDVv+VtUtaaqZqhqBq6m3JmqmnWE22RM3DuuSQ0ePKstXy/I5m/j5gUdjjGHJKKmr0Wkp4hc6btrikjjouZR1X3AYOBTYC7wtqrOFpEHROTMIwnaGAMDujTkyh4ZvDxxKaO+Xx50OMZErMgWSEXkPlz16ZbAK0AZ4HWgR1Hzquo4YFzYsL8UMG3vosM1xoT606lHszh7B/d+8BONa1bkuCY1gg7JmCJFUuI5GzgTX4Xat0RaKZpBGWMik5KcxJMXdaBB9Qrc8MYPrNi4M+iQjClSJIlnj7pGQRRARCpGNyRjzKGoUj6VlwZ2JidXuWZ4Ftt37ws6JGMKFUnieVtEngOqisgg4HPgheiGZYw5FI1rVmTYxR1ZlL2dIaN+JCfXGpAzJVehiUdEBHgLeBd4D/ec5y+q+mQMYjPGHIKezWty3xmt+XzuOv71v/lBh2NMgQqtXKCqKiLjVLUd8FmMYjLGHKbLujZi/pptPDNhMS1qp3F2h/pBh2TMb0Ryq+0HEekc9UiMMUdMRBh6Zhu6NqnO3e/N4oflm4IOyZjfiCTxHAdMFpHFIjJTRGaFNYVtjClBUpOTeOaSThxVuRzXjpjGqs2/Bh2SMQeJJPH0BZoCJ+BeDno6kb0k1BgTkGoVy/DSwEx27c1h0Igsdu6xmm6m5IjkJaE/5/eJRXDGmMPXvHYlnryoA3NWb+X2t2eQazXdTAkR0StzjDGlU59Wtfi/U47m45/W8PgXC4MOxxggglfmGGNKt2t6NWbB2m08/sVCmtdO4/T2dYMOySQ4K/EYE+dEhAfPbktmo2rc8c4MZq3cEnRIJsFZ4jEmAZRNSebZyzpRo2JZBo3IYt1Wa2neBMcSjzEJomZaWV64PJOtu/Yy6LVp7NqbE3RIJkFZ4jEmgbSuW5n/XHgsM1Zs5p73ZuLe/2tMbFniMSbB9G1zFHf2bcno6at45qvFQYdjEpDVajMmAd3Yuynz12zjkU/n0yw9jZPbHBV0SCaBWInHmAQkIjx8Xnva16vCkLemM3f11qBDMgnEEo8xCapcajLPX55JpXIpXDM8i/XbdwcdkkkQlniMSWC1K5fjhcszWb99Nxc8O5lF67YHHZJJAJZ4jElw7etX5bWrj2PLr3s5e9hEvpy3NuiQTJyzxGOMoUvj6oy5uSeNalbg6uFZDBu/yKpam6ixxGOMAaBe1fK8c113zmhfl0c+nc/gkT9acwomKizxGGP2K18mmccHHMsfT2nFuFmrOfeZyazYuDPosEycscRjjDmIiHDd8U155YrOrNy0kzOf+pbJizcEHZaJI5Z4jDH56t2yFmMG96RGWlkufek7Xp241J77mGJhiccYU6DGNSvy3xu706dlOkPHzuGud2eye5+9XNQcGUs8xphCVSqXyvOXZXLLCc14Z9pKLnxuCmutWQVzBCzxGGOKlJQk3HZyS569tCML1m7jjCe/5cflm4IOy5RSlniMMRHr17YO79/YnbKpSVz43BTezloRdEimFLLEY4w5JK2OqsyYm3rSuXE17np3JkPHzGZvTm7QYZlSJKqJR0T6ich8EVkkIvfkM/42EZkjIjNF5AsRaRTNeIwxxaNaxTIMv7ILV/dszKuTlnH5S9+zcceeoMMypUTUEo+IJAPDgFOA1sBFItI6bLIfgUxVbQ+8CzwcrXiMMcUrJTmJe09vzaPnH8O05Zs486lvmbPKmlcwRYtmiacLsEhVl6jqHmAU0D90AlUdr6p5P4ueAtSPYjzGmCg4t1N93r6uG3tzcjn3mUl8NHN10CGZEi6aiaceEPrkcaUfVpCrgY/zGyEi14pIlohkZWdnF2OIxpjicGyDqowd3JOj61Tipjd/4JFP55Gbaz82NfkrEZULRORSIBN4JL/xqvq8qmaqamZ6enpsgzPGRKRW5XKMvLYrAzo3YNj4xQwakcXWXXuDDsuUQNFMPL8ADUL66/thBxGR3wN/As5UVWsC0ZhSrGxKMn8/px1/7d+GrxZkc9awiSzOtsblzMGimXimAs1FpLGIlAEGAGNCJxCRDsBzuKSzLoqxGGNiRES4rFsGr19zHJt37uWspyYyfp6d3uaAqCUeVd0HDAY+BeYCb6vqbBF5QETO9JM9AqQB74jIdBEZU8DijDGlTNcmNRgzuAcNqlfgquFTeXqCNS5nHCltB0JmZqZmZWUFHYYxJkK/7snhzndn8OHM1Zzevg4Pn9eeCmVSgg7LxICITFPVzPDhJaJygTEmfpUvk8yTF3Xg7n6t+MgalzNY4jHGxICIcEPvprzsG5frP2yiNS6XwCzxGGNipk/LWnxwUw+qVUjl0pe+Y/ikZfbcJwFZ4jHGxFST9DRG39SDPi3TuW/MbO55b5Y1LpdgLPEYY2Iur3G5m09oxltZK7jo+Smss8blEoYlHmNMIJKShNtPbsnTl3Rk7uptnPHUt0xfsTnosEwMWOIxxgTq1HaucbnU5CQueG4y705bGXRIJsos8RhjAnd0ncqMGdyTzEbVuOOdGdw/djb7rHG5uGWJxxhTIlSvWIYRV3Xhyh4ZvDJxGZe//D2brHG5uGSJxxhTYqQkJ3HfGW341/nHkPXzJs4c9i1zV1vjcvHGEo8xpsQ5zzcut2dfLuc8PYlxs6xxuXhiiccYUyLlNS7Xqk4lbnzjBx7933xrXC5OWOIxxpRYtSqXY9S1XbkwswFPfrmIa1/LYps1LlfqWeIxxpRoZVOS+ce57bj/zDaMn5/N2U9PYok1LleqWeIxxpR4IsLA7hm8fvVxbNyxh/7DJjJ+vjUuV1pZ4jHGlBrdmtbgg5t60KBaBa56dSrPTFhsLxkthSzxGGNKlQbVK/DeDd05rV0d/vnJPG4ZNZ1f99hLRksTSzzGmFInr3G5u/q15MOZqzjv2Ums3GSNy5UWlniMMaWSiHBj72a8PLAzyzfu5MynJjJliTUuVxpY4jHGlGp9WtVi9E09qFohlUtf/I7XJlvjciWdJR5jTKnX1Dcu97sW6dz7wWz++L41LleSWeIxxsSFyuVSeeHyTG7q05RRU1dw8QvfsW6bNS5XElniMcbEjeQk4c6+rRh2cUfmrNrKmU9OZIY1LlfiWOIxxsSd09rX4b0bupOSLJz/3GTes8blShRLPMaYuNS6rmtcrlPDatz+zgweGDvHGpcrISzxGGPiVvWKZRhxdReu6J7ByxOXMvAVa1yuJLDEY4yJa6nJSQw9sw0Pn9eeqUtd43Lz1ljjckGyxGOMSQgXZDZg1HVd2b3XNS73yU/WuFxQLPEYYxJGx4bVGHtzT1oeVYnrX/+Bf1vjcoGwxGOMSSi1feNyF2TW54kvF3Hta9OscbkYs8RjjEk4ZVOS+ee57X3jcus4++lJLF2/I+iwEkZUE4+I9BOR+SKySETuyWd8WRF5y4//TkQyohmPMcbkyWtc7rWru7Bh+276P/UtE6xxuZiQaL1MT0SSgQXAScBKYCpwkarOCZnmRqC9ql4vIgOAs1X1wsKWm5mZqVlZWVGJ2RiTmFZs3MmgEVksWLuNbk1rkJqcRLIISUlCSpL7myxCcpKQJEJyEiHd/pM33ncfmIeD5s/77J83b9okSBIhJSlpf3dyvus+eJ1JPpaD11nQeg6OOyVJEJGo7VcRmaaqmeHDU6K2RugCLFLVJT6AUUB/YE7INP2Bob77XeApERG1V8saY2KoQfUKvH9jdx76aC6zV20lV/eRk6vk5Cq5qvu7c1TJzSWk2/3NyXXd+0KmLy11FkT4TdJKEujetCbPXtYpKuuMZuKpB6wI6V8JHFfQNKq6T0S2ADWA9aETici1wLUADRs2jFa8xpgEVqFMCg+d3a7Ylqfqks9ByUuVnJz8khYHulXZl3PwPLnhiU+VnNxccnIPXn5oksxVnwj3z8tvEmXOb5KqkpMLuao0rlmx2PZFuGgmnmKjqs8Dz4O71RZwOMYYUyQRIVncrS1zsGhWLvgFaBDSX98Py3caEUkBqgDWhKAxxsSxaCaeqUBzEWksImWAAcCYsGnGAAN993nAl/Z8xxhj4lvUbrX5ZzaDgU+BZOBlVZ0tIg8AWao6BngJeE1EFgEbccnJGGNMHIvqMx5VHQeMCxv2l5DuXcD50YzBGGNMyWJvLjDGGBNTlniMMcbElCUeY4wxMWWJxxhjTExF7V1t0SIi2cDPR7iYmoS9HSFB2X5wbD84th8c2w9OceyHRqqaHj6w1CWe4iAiWfm9uC7R2H5wbD84th8c2w9ONPeD3WozxhgTU5Z4jDHGxFSiJp7ngw6ghLD94Nh+cGw/OLYfnKjth4R8xmOMMSY4iVriMcYYExBLPMYYY2Iq7hKPiLwsIutE5KeQYdVF5DMRWej/VvPDRUSeEJFFIjJTRDoGF3l0icgfRGS2iPwkIiNFpJxvsuI7v/1v+eYr4pqIVBWRd0VknojMFZFuBR0f8U5EkkXkRxH50Pcn1PEgIg1EZLyIzPHnxq1+eEIeD3lEpJ+IzPfHwT3RWEfcJR7gVaBf2LB7gC9UtTnwhe8HOAVo7j/XAs/EKMaYEpF6wC1Apqq2xTVTMQD4J/AfVW0GbAKuDi7KmHkc+ERVWwHHAHMp+PiId7fitj9Poh0P+4DbVbU10BW4SURak7jHAyKSDAzDXRtbAxf5fVKs4i7xqOrXuLZ9QvUHhvvu4cBZIcNHqDMFqCoidWITacylAOV9S68VgNXACcC7fnzofolLIlIF+B2uHShUdY+qbqbg4yNuiUh94DTgRd8vJNjxoKqrVfUH370Nl4TrkYDHQ4guwCJVXaKqe4BRuP1RrOIu8RSgtqqu9t1rgNq+ux6wImS6lX5YXFHVX4B/ActxCWcLMA3YrKr7/GRxue1hGgPZwCv+FtOLIlKRgo+PePYYcBeQ6/trkHjHw34ikgF0AL4jMY+HPDG5JiZK4tnPN62dUHXI/T3q/rgLb12gIr+9HZkIUoCOwDOq2gHYQdhtlEQ4PkTkdGCdqk4LOpaSQETSgPeAIaq6NXRcIhwPQUiUxLM27xaa/7vOD/8FaBAyXX0/LN78Hliqqtmquhd4H+iBu7WY1wptvG57qJXASlX9zve/i0tEBR0f8aoHcKaILMPdSjkB9+wr0Y4HRCQVl3TeUNX3/eBEOx5CxeSamCiJZwww0HcPBD4IGX65r93WFdgSUsSOJ8uBriJSwd/LPxGYA4wHzvPThO6XuKSqa4AVItLSD8rbDwUdH3FJVf+oqvVVNQNXyeRLVb2EBDse/LnwEjBXVf8dMiqhjocwU4HmvoZjGdzxMaa4VxJ3by4QkZFAb9wrvdcC9wGjgbeBhrgmFS5Q1Y3+wHsKd9tpJ3ClqmYFEXe0icj9wIW4mjw/Atfg7t2OAqr7YZeq6u7AgowBETkW90C9DLAEuBL3Bew3x0dgQcaQiPQG7lDV00WkCQl0PIhIT+AbYBYHnnX9H+45T0IeDwAiciruGWAy8LKqPlTs64i3xGOMMaZkS5RbbcYYY0oISzzGGGNiyhKPMcaYmLLEY4wxJqYs8RhjjIkpSzym1BGRdBH51r9p+6yQ4R+ISN0C5hkqIndEIZYhIlIhpH/7ESyrrIh8LiLTReTC4onQmJLHEo8pjS4CnsW90HAIgIicAfyoqqtiHMsQ3EtXi0MHAFU9VlXfKmiikLcLGFMqWeIxpdFe3MW+LJDjL8RDgIcjmVlEmorIJyIyTUS+EZFWfvirvn2mSSKyRETO88OTRORp34bPZyIyTkTOE5FbcO++Gy8i40OW/5CIzBCRKSLymxdM+vZeRotrA2qKiLQXkVrA60BnX+JpGjbPBBF5TESygFtFpJOIfOW34dOQV7zc4tuXmSkio/ywoSLymohM9m3MDPLDRUQe8SXHWXmlLBHp7deX127RG/7H1ojIP0KW/y8/LF1E3hORqf7TI9J/pElQqmof+5SqD1AF+AjIwr325hbgiiLmGYr7hT64Nlaa++7jcK+MAdeW0zu4L2Stca+HB/camXF++FG4tmrO8+OWATVD1qPAGb77YeDP+cTyJHCf7z4BmO67ewMfFhD/BOBp350KTALSff+FuF+YA6wCyvruqiHbPgMoj3ujxwpcwjwX+Az3C/XauFcr1fFxbMG9pysJmAz0xL3Bej4Hfniet/w3gZ6+uyHuFTSBHyf2KbkfK7KbUkdVt+Daksl78/Y9wNki8gJQDXhUVSfnN69/E3F34B3/JR5cySnPaFXNBeaElFZ6Au/44WtCSzf52AN86LunASflM01P3EUfVf1SRGqISOXCttnLu/3WEmgLfOa3IRnX3AXATOANERmNe1VUng9U9VfgVx9/Fx/HSFXNwb0Y8yugM7AV+F5VVwKIyHQgA5gC7AJeEtdqad52/h5oHbI/K4tImqoe9vMuE98s8ZjS7l7gIdxzn29xb5x+H+hbwPRJuHZnji1gfOi7yaSAaQqzV1Xz3kOVQ/GeYzv8XwFmq2q3fKY5DdfY3RnAn0SknR8e/m6sot6VFbofcoAUVd0nIl1wpczzgMG4ElsS0FVVd0W8JSah2TMeU2qJSHOgvqpOwD3zycVdUMsXNI+69laWisj5fhkiIscUsaqJwLn+WU9t3K2oPNuASocY+jfAJX79vYH1GtYOTBHmA+ki0s0vI1VE2ohIEtBAVccDd+NuSab5efqLSDkRqeHjn+rjuFBEkkUkHZewvi9opb60WEVVxwF/wDUdDvA/4OaQ6QpK6sYAVuIxpdtDwJ9890jcraV7gL8UMd8lwDMi8mfc85JRuGcgBXmPA00orAB+wD0DAXge+EREVqlqnwjjHgq8LCIzcW9FH1j45AdT1T2+4sMT4przTsG9TXgB8LofJsATqrrZ3wKbiWv2oCbwV1VdJSL/Bbrhtl2Bu1R1TV5li3xUAj4QkXJ++bf54bcAw/z2pABfA9cfyjaZxGJvpzYmAnnPLHyJ4Xugh7r2fUo8ERkKbFfVfwUdizFgJR5jIvWhiFTFtePz19KSdIwpiazEY4wxJqascoExxpiYssRjjDEmpizxGGOMiSlLPMYYY2LKEo8xxpiY+n8/zGjm1Wp7oQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Vc-2UfNwPGR"
      },
      "source": [
        "import os\r\n",
        "import gc\r\n",
        "import multiprocessing\r\n",
        "\r\n",
        "def create_model_and_train(low, high, mem, attr_type = 'add'):\r\n",
        "  ATTRS_DIR = '/content/drive/MyDrive/IG RESULTS/'\r\n",
        "  ATTRS_TSV = ATTRS_DIR+'MEMORY NET/P3/attrs_'+attr_type+'.tsv'\r\n",
        "  \r\n",
        "  saver = tf.train.import_meta_graph(\"/content/drive/MyDrive/IG RESULTS/MEM MODELS/\"+str(essay_set_id)+\"/checkpoints_\"+str(fold_no)+\"-\"+str(step)+\".meta\")  \r\n",
        "  sess = tf.Session()\r\n",
        "  saver.restore(sess,\"/content/drive/MyDrive/IG RESULTS/MEM MODELS/\"+str(essay_set_id)+\"/checkpoints_\"+str(fold_no)+\"-\"+str(step))\r\n",
        "\r\n",
        "  graph = tf.get_default_graph()\r\n",
        "  IG = integrated_gradients(graph, sess, min = 0, batch_size= 20, num_reps=40)\r\n",
        "\r\n",
        "  data = pd.read_csv(ATTRS_DIR+'big_3_'+attr_type+'.csv')\r\n",
        "  essay_list, resolved_scores = load_training_data(data)\r\n",
        "  E = data_utils.vectorize_data(essay_list, word_idx, 395)\r\n",
        "  \r\n",
        "  with open(ATTRS_TSV, 'w') as outf:\r\n",
        "    ans = ''\r\n",
        "    for i,v in enumerate(E[low:high]):\r\n",
        "        tsv_string = ''\r\n",
        "        attrs, words= IG.explain(x = E[i][:395], memory= mem)\r\n",
        "\r\n",
        "        question_attrs = []\r\n",
        "        for ind in range(len(words)):\r\n",
        "          if words[ind] != None and str(attrs[ind])!=None:\r\n",
        "            question_attrs.append(\r\n",
        "                '|'.join([ words[ind], str(attrs[ind]) ])\r\n",
        "                )\r\n",
        "        tsv_string = ['||'.join(question_attrs)]\r\n",
        "        ans += '\\t'.join(tsv_string) + '\\n'\r\n",
        "        del attrs, words, question_attrs, tsv_string\r\n",
        "        gc.collect()\r\n",
        "    \r\n",
        "    outf.write(ans)\r\n",
        "    outf.flush()\r\n",
        "    del ans\r\n",
        "    gc.collect()\r\n",
        "    outf.write('done')\r\n",
        "  print('DONE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6ih-lD8PkYY",
        "outputId": "9bbebd3d-187c-4151-9044-112dd0630030"
      },
      "source": [
        "attr_type = 'normal'\r\n",
        "ranges = [[0,50], [50,100]]\r\n",
        "for r in (ranges):\r\n",
        "  p = multiprocessing.Process(target = create_model_and_train, args = (r[0], r[1], m[0][0], attr_type))\r\n",
        "  p.start()\r\n",
        "  p.join()\r\n",
        "  p.terminate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/IG RESULTS/MEM MODELS/3/checkpoints_1-8190\n",
            "4\n",
            "DONE\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/IG RESULTS/MEM MODELS/3/checkpoints_1-8190\n",
            "4\n",
            "DONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HC-G8dWwPD7",
        "outputId": "8340beab-6527-4863-bdd7-b9c914bca854"
      },
      "source": [
        "attr_type = 'song_beg'\r\n",
        "for r in (ranges):\r\n",
        "  p = multiprocessing.Process(target = create_model_and_train, args = (r[0], r[1], m[0][0], attr_type))\r\n",
        "  p.start()\r\n",
        "  p.join()\r\n",
        "  p.terminate()\r\n",
        "\r\n",
        "attr_type = 'song_end'\r\n",
        "for r in (ranges):\r\n",
        "  p = multiprocessing.Process(target = create_model_and_train, args = (r[0], r[1], m[0][0], attr_type))\r\n",
        "  p.start()\r\n",
        "  p.join()\r\n",
        "  p.terminate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/IG RESULTS/MEM MODELS/3/checkpoints_1-8190\n",
            "4\n",
            "DONE\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/IG RESULTS/MEM MODELS/3/checkpoints_1-8190\n",
            "4\n",
            "DONE\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/IG RESULTS/MEM MODELS/3/checkpoints_1-8190\n",
            "4\n",
            "DONE\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/IG RESULTS/MEM MODELS/3/checkpoints_1-8190\n",
            "4\n",
            "DONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCTiW4UswPBd",
        "outputId": "d1f083fa-9644-4083-f112-8334fe7794b7"
      },
      "source": [
        "attr_type = 'false_beg'\r\n",
        "for r in (ranges):\r\n",
        "  p = multiprocessing.Process(target = create_model_and_train, args = (r[0], r[1], m[0][0], attr_type))\r\n",
        "  p.start()\r\n",
        "  p.join()\r\n",
        "  p.terminate()\r\n",
        "  \r\n",
        "attr_type = 'false_end'\r\n",
        "for r in (ranges):\r\n",
        "  p = multiprocessing.Process(target = create_model_and_train, args = (r[0], r[1], m[0][0], attr_type))\r\n",
        "  p.start()\r\n",
        "  p.join()\r\n",
        "  p.terminate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/IG RESULTS/MEM MODELS/3/checkpoints_1-8190\n",
            "4\n",
            "DONE\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/IG RESULTS/MEM MODELS/3/checkpoints_1-8190\n",
            "4\n",
            "DONE\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/IG RESULTS/MEM MODELS/3/checkpoints_1-8190\n",
            "4\n",
            "DONE\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/IG RESULTS/MEM MODELS/3/checkpoints_1-8190\n",
            "4\n",
            "DONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFfwzTXnock9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39a45907-c516-45cc-cc9b-0c6ca8173a4e"
      },
      "source": [
        "attr_type = 'shuffle'\r\n",
        "for r in (ranges):\r\n",
        "  p = multiprocessing.Process(target = create_model_and_train, args = (r[0], r[1], m[0][0], attr_type))\r\n",
        "  p.start()\r\n",
        "  p.join()\r\n",
        "  p.terminate()\r\n",
        "\r\n",
        "attr_type = 'syn'\r\n",
        "for r in (ranges):\r\n",
        "  p = multiprocessing.Process(target = create_model_and_train, args = (r[0], r[1], m[0][0], attr_type))\r\n",
        "  p.start()\r\n",
        "  p.join()\r\n",
        "  p.terminate()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/IG RESULTS/MEM MODELS/3/checkpoints_1-8190\n",
            "4\n",
            "DONE\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/IG RESULTS/MEM MODELS/3/checkpoints_1-8190\n",
            "4\n",
            "DONE\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/IG RESULTS/MEM MODELS/3/checkpoints_1-8190\n",
            "4\n",
            "DONE\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/IG RESULTS/MEM MODELS/3/checkpoints_1-8190\n",
            "4\n",
            "DONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoIJKxGZoci6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8g2eDJwocg_"
      },
      "source": [
        "def get_w_a(essay_type):\r\n",
        "  ATTRS_DIR = '/content/drive/MyDrive/IG RESULTS/MEMORY NET/P3/'\r\n",
        "\r\n",
        "  ### NORMAL\r\n",
        "  path = ATTRS_DIR + 'attrs_'+essay_type+'.tsv'\r\n",
        "  w = []\r\n",
        "  a = []\r\n",
        "\r\n",
        "  with open(path, 'r') as f:\r\n",
        "    for line in f:\r\n",
        "        line = line.strip()\r\n",
        "        question_attrs = line.split('\\t')[0]\r\n",
        "        question_tokens = []\r\n",
        "        attrs = []\r\n",
        "        for word_attr in question_attrs.split('||'): \r\n",
        "          if word_attr != 'done':\r\n",
        "            word, attr = word_attr.split('|')\r\n",
        "            question_tokens.append(word)\r\n",
        "            attrs.append(float(attr))\r\n",
        "        question_tokens = question_tokens + [None]*(395 - len(attrs))\r\n",
        "        attrs = attrs + [0]*(395 - len(attrs))\r\n",
        "\r\n",
        "        w.append(question_tokens)\r\n",
        "        a.append(attrs)\r\n",
        "  return a[:-1], w[:-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGvkWetRoccD"
      },
      "source": [
        "def save_stats_add(diff, diff_attr, percent, output_filename):\r\n",
        "  result = open(output_filename, 'w')\r\n",
        "  result.write('\\ndiff in scores:  '+str(diff))\r\n",
        "  result.write('\\ndiff in attrs:  '+ str(diff_attr))\r\n",
        "  result.write('\\npercent of top words in added words:  '+ str(percent))\r\n",
        "  result.close()\r\n",
        "\r\n",
        "def save_stats_mod_shuffle(diff, diff_attr, changed_no,output_filename):\r\n",
        "  result = open(output_filename, 'w')\r\n",
        "  result.write('\\ndiff in scores:  '+str(diff))\r\n",
        "  result.write('\\ndiff in attrs:  '+ str(diff_attr))\r\n",
        "  result.write('\\nno of top words which changed attr:  '+ str(changed_no))\r\n",
        "  result.close()\r\n",
        "\r\n",
        "def save_stats_mod_syn(diff, diff_attr, changed_top_no,changed_bottom_no, top_words, bottom_words, output_filename):\r\n",
        "  result = open(output_filename, 'w')\r\n",
        "  result.write('\\n diff in scores:  '+str(diff))\r\n",
        "  result.write('\\n diff in attrs:  '+ str(diff_attr))\r\n",
        "  result.write('\\n no of top words which changed attr:  '+ str(changed_top_no))\r\n",
        "  result.write('\\n no of bottom words which changed attr:  '+ str(changed_bottom_no))\r\n",
        "  result.write('\\n top words which changed attr:  '+ str(top_words))\r\n",
        "  result.write('\\n bottom words which changed attr:  '+ str(bottom_words))\r\n",
        "  result.close()\r\n",
        "\r\n",
        "def subfinder(l, sl):\r\n",
        "    sll=len(sl)\r\n",
        "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\r\n",
        "        if l[ind:ind+sll]==sl:\r\n",
        "            return ind,ind+sll-1\r\n",
        "\r\n",
        "def top_k_attrs(tokens, attrs, sign = None, k=None):\r\n",
        "    k = min(k, len(tokens))\r\n",
        "    \r\n",
        "    if sign != None:\r\n",
        "      tokens_list = []\r\n",
        "      signs_list = []\r\n",
        "      for i in np.argpartition(attrs, -k)[-k:]:\r\n",
        "          tokens_list.append(tokens[i])\r\n",
        "          signs_list.append(sign[i])\r\n",
        "      return  tokens_list , signs_list\r\n",
        "    \r\n",
        "    else:\r\n",
        "      return ([tokens[i] for i in np.argpartition(attrs, -k)[-k:]])\r\n",
        "\r\n",
        "def bottom_k_attrs(tokens, attrs, sign = None, k=None):\r\n",
        "    k = min(k, len(tokens))\r\n",
        "\r\n",
        "    if sign != None:\r\n",
        "      tokens_list = []\r\n",
        "      signs_list = []\r\n",
        "      for i in np.argpartition(attrs, k)[:k]:\r\n",
        "          tokens_list.append(tokens[i])\r\n",
        "          signs_list.append(sign[i])\r\n",
        "      return  tokens_list , signs_list\r\n",
        "    \r\n",
        "    else:\r\n",
        "      return ([tokens[i] for i in np.argpartition(attrs, k)[:k]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCTemSHEocZ2"
      },
      "source": [
        "def save_big_attrs_pdf(memory, essay_type, type_add = False, type_mod = False, type_gen = False):\r\n",
        "  ATTRS_DIR = '/content/drive/MyDrive/IG RESULTS/MEMORY NET/P3/'\r\n",
        "  dir =  ATTRS_DIR\r\n",
        "  if not os.path.exists(dir):\r\n",
        "    os.makedirs(dir)\r\n",
        "\r\n",
        "  pattern_none = [None, None, None, None, None]\r\n",
        "  \r\n",
        "  if type_add:\r\n",
        "    diff_array = [] \r\n",
        "    diff_attr_array = []\r\n",
        "    percent_array = []\r\n",
        "\r\n",
        "  if type_mod:\r\n",
        "    diff_array = []\r\n",
        "    diff_attr_array = []\r\n",
        "    changed_count_array = []\r\n",
        "    changed_count_top_array = []\r\n",
        "    changed_count_bottom_array = []\r\n",
        "    changed_top_words = {}\r\n",
        "    changed_bottom_words = {}\r\n",
        "\r\n",
        "  ### NORMAL\r\n",
        "  a, w = get_w_a('normal')\r\n",
        "  ### ADV\r\n",
        "  a_new, w_new = get_w_a(essay_type)\r\n",
        "  \r\n",
        "  for i,essay in enumerate(w_new):\r\n",
        "    attrs, words= a_new[i], essay\r\n",
        "    E = vectorize_data([words], word_idx, 359)\r\n",
        "    E_normal = vectorize_data([w[i]], word_idx, 359)\r\n",
        "    # assert (E_normal[0]) == (E[0])\r\n",
        "    labels_new = (IG.predict([E[0], memory[0],1.0]))\r\n",
        "    labels_orig =(IG.predict([E_normal[0], memory[0],1.0]))\r\n",
        "    if type_add:\r\n",
        "        try:\r\n",
        "          loc = subfinder(w[i], pattern_none)[0]\r\n",
        "        except Exception as e:\r\n",
        "          loc = len(w[i]) \r\n",
        "        try: \r\n",
        "          loc2 = subfinder(words, pattern_none)[0]\r\n",
        "        except Exception as e:\r\n",
        "          loc2 = len(words)\r\n",
        "        # print(loc, loc2, w[i])\r\n",
        "\r\n",
        "        if essay_type == 'song_beg' or essay_type =='false_beg' :\r\n",
        "          pattern = w[i][:5]\r\n",
        "          try:\r\n",
        "            loc_patt = subfinder(words, pattern)[0]\r\n",
        "          except Exception as e:\r\n",
        "            loc_patt = 0\r\n",
        "          new_w = words[:loc_patt]\r\n",
        "          new_a = attrs[:loc_patt]\r\n",
        "          other_a = attrs[loc_patt:loc2]\r\n",
        "\r\n",
        "        elif essay_type == 'song_end' or essay_type =='false_end':\r\n",
        "          new_w = words[loc:loc2]\r\n",
        "          new_a = attrs[loc:loc2]\r\n",
        "          other_a = attrs[:loc]\r\n",
        "\r\n",
        "        if (new_w) !=[]:\r\n",
        "          top_words_orig = top_k_attrs(w[i], a[i], k = int(0.2*loc))\r\n",
        "          top_words = top_k_attrs(words , attrs, k = int(0.2*loc2))\r\n",
        "\r\n",
        "          top_words_final = [x for x in top_words if x in new_w]\r\n",
        "\r\n",
        "          diff = int(labels_new) - int(labels_orig)\r\n",
        "          diff_array.append(abs(diff))\r\n",
        "\r\n",
        "          diff_attr_frac = []\r\n",
        "          for i_attrs in range(0,len(other_a), len(new_a)):\r\n",
        "            if i_attrs+len(new_a) < len(other_a):\r\n",
        "              diff_attr_frac.append(   sum(other_a[   i_attrs:i_attrs+len(new_a)  ])  )\r\n",
        "            else:\r\n",
        "              diff_attr_frac.append(   sum(other_a[   i_attrs:len(other_a)  ])  )\r\n",
        "              break\r\n",
        "\r\n",
        "          new_diff_frac = sum(diff_attr_frac) / len(diff_attr_frac)\r\n",
        "\r\n",
        "          diff_attr = sum(new_a)/new_diff_frac\r\n",
        "          diff_attr_array.append(diff_attr)\r\n",
        "\r\n",
        "          percent = len(top_words_final)/len(new_w)\r\n",
        "          percent_array.append(percent)\r\n",
        "    \r\n",
        "    elif type_mod:\r\n",
        "        diff = int(labels_new) - int(labels_orig)\r\n",
        "        diff_array.append(abs(diff))\r\n",
        "\r\n",
        "        diff_attr = sum(attrs) - sum(a[i])\r\n",
        "        diff_attr_array.append(diff_attr)\r\n",
        "\r\n",
        "        ###### SYN\r\n",
        "\r\n",
        "        if essay_type == 'syn':\r\n",
        "          # normal\r\n",
        "          loc = subfinder(w[i], pattern_none)[0]\r\n",
        "          \r\n",
        "          attrs_abs=[]\r\n",
        "          attrs_sign_orig =[]\r\n",
        "          for x in a[i]:\r\n",
        "            attrs_abs.append(abs(x))\r\n",
        "            if x>0:\r\n",
        "              attrs_sign_orig.append('+')\r\n",
        "            else:\r\n",
        "              attrs_sign_orig.append('-')\r\n",
        "\r\n",
        "          top_words_orig, token_signs_orig = top_k_attrs(w[i][:loc], attrs_abs[:loc], attrs_sign_orig[:loc], k = int(0.2*loc))\r\n",
        "          bottom_words_orig, bottom_token_signs_orig = bottom_k_attrs(w[i][:loc], attrs_abs[:loc], attrs_sign_orig[:loc], k = int(0.2*loc))\r\n",
        "          \r\n",
        "          # adv\r\n",
        "          loc2 = subfinder(words, pattern_none)[0]\r\n",
        "          attrs_abs=[]\r\n",
        "          attrs_sign =[]\r\n",
        "          for x in attrs:\r\n",
        "            attrs_abs.append(abs(x))\r\n",
        "            if x>0:\r\n",
        "              attrs_sign.append('+')\r\n",
        "            else:\r\n",
        "              attrs_sign.append('-')\r\n",
        "          \r\n",
        "          top_words, token_signs = top_k_attrs(words[:loc2], attrs_abs[:loc2], attrs_sign[:loc2], k = int(0.2*loc2))\r\n",
        "          bottom_words, bottom_token_signs = bottom_k_attrs(words[:loc2], attrs_abs[:loc2], attrs_sign[:loc2], k = int(0.2*loc2))\r\n",
        "\r\n",
        "          changed_count_top = 0\r\n",
        "          changed_count_bottom = 0\r\n",
        "          \r\n",
        "          # top    \r\n",
        "          for orig_index,t in enumerate(top_words_orig):\r\n",
        "            try:\r\n",
        "              ind = w[i].index(t)\r\n",
        "            except Exception as e:\r\n",
        "              ind = -1\r\n",
        "\r\n",
        "            if ind!=-1:\r\n",
        "              if attrs_sign[ind] != attrs_sign_orig[ind]:\r\n",
        "                  changed_count_top+=1\r\n",
        "                  changed_top_words[t] = words[ind]\r\n",
        "          \r\n",
        "          # bottom\r\n",
        "          for orig_index,t in enumerate(bottom_words_orig):\r\n",
        "            try:\r\n",
        "              ind = w[i].index(t)\r\n",
        "            except Exception as e:\r\n",
        "              ind = -1\r\n",
        "\r\n",
        "            if ind!=-1:\r\n",
        "              if attrs_sign[ind] != attrs_sign_orig[ind]:\r\n",
        "                  changed_count_bottom+=1\r\n",
        "                  changed_bottom_words[t] = words[ind]\r\n",
        "          \r\n",
        "          changed_count_top_array.append(changed_count_top)\r\n",
        "          changed_count_bottom_array.append(changed_count_bottom)\r\n",
        "\r\n",
        "        ###### SHUFFLE \r\n",
        "        if essay_type == 'shuffle':\r\n",
        "          loc = subfinder(w[i], pattern_none)[0]\r\n",
        "          attrs_abs=[]\r\n",
        "          attrs_sign =[]\r\n",
        "          for x in a[i]:\r\n",
        "            attrs_abs.append(abs(x))\r\n",
        "            if x>0:\r\n",
        "              attrs_sign.append('+')\r\n",
        "            else:\r\n",
        "              attrs_sign.append('-')\r\n",
        "\r\n",
        "          top_words_orig, token_signs_orig = top_k_attrs(w[i], attrs_abs, attrs_sign, k = int(0.2*loc))\r\n",
        "          \r\n",
        "          loc = subfinder(words, pattern_none)[0]\r\n",
        "          attrs_abs=[]\r\n",
        "          attrs_sign =[]\r\n",
        "          for x in attrs:\r\n",
        "            attrs_abs.append(abs(x))\r\n",
        "            if x>0:\r\n",
        "              attrs_sign.append('+')\r\n",
        "            else:\r\n",
        "              attrs_sign.append('-')\r\n",
        "          \r\n",
        "          top_words, token_signs = top_k_attrs(words, attrs_abs, attrs_sign, k = int(0.2*loc))\r\n",
        "          \r\n",
        "          changed_count = 0\r\n",
        "\r\n",
        "          for orig_index,t in enumerate(top_words_orig):\r\n",
        "            try:\r\n",
        "              ind = top_words.index(t)\r\n",
        "            except Exception as e:\r\n",
        "              ind = -1\r\n",
        "\r\n",
        "            if ind!=-1:\r\n",
        "              if token_signs_orig[orig_index] != token_signs[ind]:\r\n",
        "                  changed_count+=1\r\n",
        "          changed_count_array.append(changed_count)\r\n",
        "\r\n",
        "  def Average(lst): \r\n",
        "    return sum(lst) / len(lst) \r\n",
        "  \r\n",
        "  if type_add:\r\n",
        "    save_stats_add(Average(diff_array), Average(diff_attr_array), Average(percent_array),       dir+'stats_'+essay_type+'.txt')\r\n",
        "\r\n",
        "  if type_mod and essay_type == 'syn':\r\n",
        "    save_stats_mod_syn(Average(diff_array), Average(diff_attr_array), Average(changed_count_top_array), \\\r\n",
        "                   Average(changed_count_bottom_array), set(changed_top_words),  set(changed_bottom_words), \\\r\n",
        "                   dir+'stats_'+essay_type+'.txt')\r\n",
        "\r\n",
        "  if type_mod and essay_type == 'shuffle':\r\n",
        "    save_stats_mod_shuffle(Average(diff_array), Average(diff_attr_array), Average(changed_count_array), dir+'stats_'+essay_type+'.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeMM0DhTocXa"
      },
      "source": [
        "save_big_attrs_pdf(m[0], 'song_beg', type_add=True)\r\n",
        "save_big_attrs_pdf(m[0], 'song_end', type_add=True)\r\n",
        "save_big_attrs_pdf(m[0], 'false_beg', type_add=True)\r\n",
        "save_big_attrs_pdf(m[0], 'false_end', type_add=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LT6hcWoDocU8"
      },
      "source": [
        "save_big_attrs_pdf(m[0], 'shuffle', type_mod=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpxUGo1VocS1"
      },
      "source": [
        "save_big_attrs_pdf(m[0], 'syn', type_mod=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfXbPBpbKUpG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7mCVQXPP-ee",
        "outputId": "afd926ab-333a-469f-d94c-906e044e5474"
      },
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "for i,v in enumerate(orig_array):\n",
        "      orig_array[i] = v+0\n",
        "orig_acc = cohen_kappa_score(test_scores, orig_array, weights='quadratic')\n",
        "orig_acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6650977415069272"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bG-pYNLOFrp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-cJroxnHWvc",
        "outputId": "243b202f-045e-49c3-bdb7-cfb6a1130193"
      },
      "source": [
        "curve_data = {}\n",
        "question_lengths = {}\n",
        "all_accs = []\n",
        "counts_list,_ = get_counts_list(top_k = None)\n",
        "counts_list = [item for sublist in counts_list for item in sublist]\n",
        "c = (counts_list)\n",
        "with open('/content/drive/My Drive/IG RESULTS/MEM MODELS/tokenizer.pkl', 'rb') as f:\n",
        "    tokenizer = pickle.load(f)\n",
        "for K in np.append(0, np.unique(np.floor(np.geomspace(1, len(c), 10)))):\n",
        "    # take K most top attributed words\n",
        "    if K in curve_data:\n",
        "        continue\n",
        "    \n",
        "    counts_list,_ = get_counts_list(int(K))\n",
        "    counts_list = [item for sublist in counts_list for item in sublist]\n",
        "\n",
        "    whitelist = set([tokenizer[w] for w in counts_list[:int(K)]]) if K > 0 else set()\n",
        "    print('wh len', len(whitelist))\n",
        "    num_batches = 0\n",
        "    avg_question_length_orig = 0\n",
        "    avg_question_length_new = 0\n",
        "    num_questions = 0\n",
        "    pred_array = []\n",
        "    # iterator over the validation dataset\n",
        "    for test,mem in (zip(t,m)):\n",
        "\n",
        "        test = np.array(test)\n",
        "        new_test = np.zeros(test.shape)\n",
        "        curr_batch_size = int(test.shape[0])\n",
        "        for batch_i in range(curr_batch_size):\n",
        "            len_counter = 0\n",
        "            # avg_question_length_orig += int(test[batch_i])\n",
        "            for word_i, w in enumerate(test[batch_i]):\n",
        "                if int(w) in whitelist:\n",
        "                    new_test[batch_i, len_counter] = int(w)\n",
        "                    len_counter += 1\n",
        "            if len_counter == 0:\n",
        "                len_counter = 1\n",
        "            avg_question_length_new += int(len_counter)\n",
        "            num_questions += 1\n",
        "            \n",
        "        feed = {}\n",
        "        input_df = [new_test, mem, 1]\n",
        "        for i, key in enumerate(INPUT_TENSORS):\n",
        "            feed[key.name] = input_df[i]\n",
        "        pred = sess.run(PRED_TENSOR,feed)\n",
        "        pred_array.extend(pred)\n",
        "\n",
        "    # print(\"avg question length orig: \", float(avg_question_length_orig)/num_questions)\n",
        "    print(\"avg question length new: \", float(avg_question_length_new)/num_questions)\n",
        "    for i,v in enumerate(pred_array):\n",
        "      pred_array[i] = v+0\n",
        "    acc = cohen_kappa_score(pred_array, test_scores, weights='quadratic')\n",
        "    print(\"accuracy for \", K, \" is\", acc)\n",
        "    curve_data[K] = acc\n",
        "    question_lengths[K] = float(avg_question_length_new)/num_questions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wh len 0\n",
            "avg question length new:  1.0\n",
            "accuracy for  0.0  is 0.0\n",
            "wh len 1\n",
            "avg question length new:  1.0144508670520231\n",
            "accuracy for  1.0  is 0.0\n",
            "wh len 3\n",
            "avg question length new:  1.106936416184971\n",
            "accuracy for  3.0  is 0.0\n",
            "wh len 8\n",
            "avg question length new:  5.054913294797688\n",
            "accuracy for  10.0  is 0.0\n",
            "wh len 21\n",
            "avg question length new:  19.320809248554912\n",
            "accuracy for  33.0  is 0.004930156121610518\n",
            "wh len 62\n",
            "avg question length new:  44.0635838150289\n",
            "accuracy for  108.0  is 0.3178705330584801\n",
            "wh len 175\n",
            "avg question length new:  69.46531791907515\n",
            "accuracy for  351.0  is 0.4944507532742827\n",
            "wh len 370\n",
            "avg question length new:  87.9364161849711\n",
            "accuracy for  1134.0  is 0.5764614557831254\n",
            "wh len 727\n",
            "avg question length new:  99.91618497109826\n",
            "accuracy for  3663.0  is 0.6421445155843242\n",
            "wh len 1191\n",
            "avg question length new:  106.61849710982659\n",
            "accuracy for  11830.0  is 0.6770729068037333\n",
            "wh len 1504\n",
            "avg question length new:  109.23699421965318\n",
            "accuracy for  38205.0  is 0.6678660132912667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zvYTQlzTPRk"
      },
      "source": [
        "curve_data = {}\n",
        "question_lengths = {}\n",
        "all_accs = []\n",
        "counts_list,_ = get_counts_list(top_k = None)\n",
        "counts_list = [item for sublist in counts_list for item in sublist]\n",
        "c = (counts_list)\n",
        "with open('/content/drive/My Drive/IG RESULTS/MEM MODELS/tokenizer.pkl', 'rb') as f:\n",
        "    tokenizer = pickle.load(f)\n",
        "for K in np.append(0, np.unique(np.floor(np.geomspace(1, len(c), 10)))):\n",
        "    # take K most top attributed words\n",
        "    if K in curve_data:\n",
        "        continue\n",
        "    \n",
        "    counts_list,_ = get_counts_list(int(K))\n",
        "    counts_list = [item for sublist in counts_list for item in sublist]\n",
        "\n",
        "    whitelist = set([tokenizer[w] for w in counts_list[:int(K)]]) if K > 0 else set()\n",
        "    print('wh len', len(whitelist))\n",
        "    num_batches = 0\n",
        "    avg_question_length_orig = 0\n",
        "    avg_question_length_new = 0\n",
        "    num_questions = 0\n",
        "    pred_array = []\n",
        "    # iterator over the validation dataset\n",
        "    for test,mem in (zip(t,m)):\n",
        "\n",
        "        test = np.array(test)\n",
        "        new_test = np.zeros(test.shape)\n",
        "        curr_batch_size = int(test.shape[0])\n",
        "        for batch_i in range(curr_batch_size):\n",
        "            len_counter = 0\n",
        "            # avg_question_length_orig += int(test[batch_i])\n",
        "            for word_i, w in enumerate(test[batch_i]):\n",
        "                if int(w) in whitelist:\n",
        "                    new_test[batch_i, len_counter] = int(w)\n",
        "                    len_counter += 1\n",
        "            if len_counter == 0:\n",
        "                len_counter = 1\n",
        "            avg_question_length_new += int(len_counter)\n",
        "            num_questions += 1\n",
        "            \n",
        "        feed = {}\n",
        "        input_df = [new_test, mem, 1]\n",
        "        for i, key in enumerate(INPUT_TENSORS):\n",
        "            feed[key.name] = input_df[i]\n",
        "        pred = sess.run(PRED_TENSOR,feed)\n",
        "        pred_array.extend(pred)\n",
        "\n",
        "    # print(\"avg question length orig: \", float(avg_question_length_orig)/num_questions)\n",
        "    print(\"avg question length new: \", float(avg_question_length_new)/num_questions)\n",
        "    for i,v in enumerate(pred_array):\n",
        "      pred_array[i] = v+0\n",
        "    acc = cohen_kappa_score(pred_array, test_scores, weights='quadratic')\n",
        "    print(\"accuracy for \", K, \" is\", acc)\n",
        "    curve_data[K] = acc\n",
        "    question_lengths[K] = float(avg_question_length_new)/num_questions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clVXkP1MTRp_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GTCRcwVTRwt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMX_9euNTRuS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8559yiiuTRmz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkaUu2Q_TPOE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qm2gRnkxA5cv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ld02e60Tvc4o",
        "outputId": "c810ae21-d4fa-4e3d-f504-7dfc12c3159c"
      },
      "source": [
        "s= {}\n",
        "l = len(attrs)\n",
        "for i in range(0, len(attrs), l//5):\n",
        "  s[(i)] = sum(attrs[i: i+ l//5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 0.47054603821015917, 79: 0.0447394315560814, 158: 0.0, 237: 0.0, 316: 0.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75IMH0jewawz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2rX4m-iweyo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSsro48jaEWT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9956b51d-3de7-4031-b03e-f272329d36c3"
      },
      "source": [
        "for name in names:\n",
        "  e = E_list[name]\n",
        "  mem = M_list[name]\n",
        "  save_attrs_pdf(e, mem, labels_orig, name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prediction is 0.6202964782714844\n",
            "baseline_prediction is 0.01159109827131033\n",
            "delta_prediction is 0.6087054014205933\n",
            "sum_attributions are 0.6236211061477661\n",
            "Error percentage is -2.450397958086566\n",
            "prediction is 0.9031385183334351\n",
            "baseline_prediction is 0.01159109827131033\n",
            "delta_prediction is 0.891547441482544\n",
            "sum_attributions are 0.9108295440673828\n",
            "Error percentage is -2.1627679793208627\n",
            "prediction is 0.9022407531738281\n",
            "baseline_prediction is 0.8223961591720581\n",
            "delta_prediction is 0.07984459400177002\n",
            "sum_attributions are -0.6018960475921631\n",
            "Error percentage is 853.8344394096613\n",
            "prediction is 0.8502042293548584\n",
            "baseline_prediction is 0.8223961591720581\n",
            "delta_prediction is 0.027808070182800293\n",
            "sum_attributions are -0.4673089385032654\n",
            "Error percentage is 1780.4795709711022\n",
            "prediction is 0.873424232006073\n",
            "baseline_prediction is 0.01159109827131033\n",
            "delta_prediction is 0.8618331551551819\n",
            "sum_attributions are 0.8801319003105164\n",
            "Error percentage is -2.1232352278254596\n",
            "prediction is 0.8385828137397766\n",
            "baseline_prediction is 0.01159109827131033\n",
            "delta_prediction is 0.8269917368888855\n",
            "sum_attributions are 0.8444075584411621\n",
            "Error percentage is -2.1059244942149404\n",
            "prediction is 0.6052603125572205\n",
            "baseline_prediction is 0.01159109827131033\n",
            "delta_prediction is 0.5936692357063293\n",
            "sum_attributions are 0.607475757598877\n",
            "Error percentage is -2.325625291349489\n",
            "prediction is 0.9135856032371521\n",
            "baseline_prediction is 0.8223961591720581\n",
            "delta_prediction is 0.091189444065094\n",
            "sum_attributions are -0.63857501745224\n",
            "Error percentage is 800.2729581248509\n",
            "prediction is 0.5831519961357117\n",
            "baseline_prediction is 0.8223961591720581\n",
            "delta_prediction is -0.23924416303634644\n",
            "sum_attributions are -0.8390330076217651\n",
            "Error percentage is -250.70155817941423\n",
            "prediction is 0.7583040595054626\n",
            "baseline_prediction is 0.01159109827131033\n",
            "delta_prediction is 0.7467129826545715\n",
            "sum_attributions are 0.7615498304367065\n",
            "Error percentage is -1.98695457649469\n",
            "prediction is 0.9028779864311218\n",
            "baseline_prediction is 0.01159109827131033\n",
            "delta_prediction is 0.8912869095802307\n",
            "sum_attributions are 0.9114499688148499\n",
            "Error percentage is -2.2622411501718718\n",
            "prediction is 0.7778496146202087\n",
            "baseline_prediction is 0.8223961591720581\n",
            "delta_prediction is -0.044546544551849365\n",
            "sum_attributions are -0.7142359614372253\n",
            "Error percentage is -1503.3476190412475\n",
            "prediction is 0.7584964632987976\n",
            "baseline_prediction is 0.8223961591720581\n",
            "delta_prediction is -0.0638996958732605\n",
            "sum_attributions are -0.5892512202262878\n",
            "Error percentage is -822.1502734457712\n",
            "prediction is 0.9793702960014343\n",
            "baseline_prediction is 0.01159109827131033\n",
            "delta_prediction is 0.9677792191505432\n",
            "sum_attributions are 0.9860724806785583\n",
            "Error percentage is -1.89023086733272\n",
            "prediction is 0.9089457988739014\n",
            "baseline_prediction is 0.01159109827131033\n",
            "delta_prediction is 0.8973547220230103\n",
            "sum_attributions are 0.9120920300483704\n",
            "Error percentage is -1.6423057307968574\n",
            "prediction is 0.6409288048744202\n",
            "baseline_prediction is 0.01159109827131033\n",
            "delta_prediction is 0.629337728023529\n",
            "sum_attributions are 0.6421114802360535\n",
            "Error percentage is -2.02971340247487\n",
            "prediction is 0.909771203994751\n",
            "baseline_prediction is 0.8223961591720581\n",
            "delta_prediction is 0.08737504482269287\n",
            "sum_attributions are -0.6276172399520874\n",
            "Error percentage is 818.30262430845\n",
            "prediction is 0.6187930703163147\n",
            "baseline_prediction is 0.01159109827131033\n",
            "delta_prediction is 0.6072019934654236\n",
            "sum_attributions are 0.627358078956604\n",
            "Error percentage is -3.3195025227347488\n",
            "prediction is 0.687543511390686\n",
            "baseline_prediction is 0.8223961591720581\n",
            "delta_prediction is -0.13485264778137207\n",
            "sum_attributions are -0.25779929757118225\n",
            "Error percentage is -91.17110550853675\n",
            "prediction is 0.9420578479766846\n",
            "baseline_prediction is 0.8223961591720581\n",
            "delta_prediction is 0.11966168880462646\n",
            "sum_attributions are 0.03648883104324341\n",
            "Error percentage is 69.5066721790804\n",
            "prediction is 0.8955467343330383\n",
            "baseline_prediction is 0.8223961591720581\n",
            "delta_prediction is 0.07315057516098022\n",
            "sum_attributions are -0.04828750342130661\n",
            "Error percentage is 166.01110560654072\n",
            "prediction is 0.8482548594474792\n",
            "baseline_prediction is 0.8223961591720581\n",
            "delta_prediction is 0.025858700275421143\n",
            "sum_attributions are -0.1143006980419159\n",
            "Error percentage is 542.020274895871\n",
            "prediction is 0.6563226580619812\n",
            "baseline_prediction is 0.04815390333533287\n",
            "delta_prediction is 0.6081687808036804\n",
            "sum_attributions are 0.624713659286499\n",
            "Error percentage is -2.7204419241900157\n",
            "prediction is 0.8801025152206421\n",
            "baseline_prediction is 0.8223961591720581\n",
            "delta_prediction is 0.057706356048583984\n",
            "sum_attributions are -0.10370084643363953\n",
            "Error percentage is 279.70437493286175\n",
            "prediction is 0.9278786182403564\n",
            "baseline_prediction is 0.8223961591720581\n",
            "delta_prediction is 0.10548245906829834\n",
            "sum_attributions are 0.09418882429599762\n",
            "Error percentage is 10.70664722083153\n",
            "prediction is 0.8892634510993958\n",
            "baseline_prediction is 0.8223961591720581\n",
            "delta_prediction is 0.06686729192733765\n",
            "sum_attributions are -0.1264103651046753\n",
            "Error percentage is 289.0466347015235\n",
            "prediction is 0.8678151965141296\n",
            "baseline_prediction is 0.8223961591720581\n",
            "delta_prediction is 0.04541903734207153\n",
            "sum_attributions are -0.15264123678207397\n",
            "Error percentage is 436.0732541125058\n",
            "prediction is 0.9032529592514038\n",
            "baseline_prediction is 0.8223961591720581\n",
            "delta_prediction is 0.0808568000793457\n",
            "sum_attributions are 0.10722216963768005\n",
            "Error percentage is -32.60748574326675\n",
            "prediction is 0.8585196137428284\n",
            "baseline_prediction is 0.8223961591720581\n",
            "delta_prediction is 0.036123454570770264\n",
            "sum_attributions are 0.03510083258152008\n",
            "Error percentage is 2.8309086199016256\n",
            "prediction is 0.8578868508338928\n",
            "baseline_prediction is 0.8223961591720581\n",
            "delta_prediction is 0.03549069166183472\n",
            "sum_attributions are 0.04134221374988556\n",
            "Error percentage is -16.487483940312543\n",
            "prediction is 0.8856803178787231\n",
            "baseline_prediction is 0.8223961591720581\n",
            "delta_prediction is 0.06328415870666504\n",
            "sum_attributions are 0.05023013800382614\n",
            "Error percentage is 20.627627781775438\n",
            "prediction is 0.902135968208313\n",
            "baseline_prediction is 0.8223961591720581\n",
            "delta_prediction is 0.07973980903625488\n",
            "sum_attributions are 0.06708110868930817\n",
            "Error percentage is 15.875007101147247\n",
            "prediction is 0.869430661201477\n",
            "baseline_prediction is 0.8223961591720581\n",
            "delta_prediction is 0.047034502029418945\n",
            "sum_attributions are 0.037230703979730606\n",
            "Error percentage is 20.843843618364026\n",
            "prediction is 0.8633621335029602\n",
            "baseline_prediction is 0.8223961591720581\n",
            "delta_prediction is 0.0409659743309021\n",
            "sum_attributions are 0.018949806690216064\n",
            "Error percentage is 53.74257051193447\n",
            "prediction is 0.8561506867408752\n",
            "baseline_prediction is 0.8223961591720581\n",
            "delta_prediction is 0.03375452756881714\n",
            "sum_attributions are 0.02414456009864807\n",
            "Error percentage is 28.47015841230993\n",
            "prediction is 0.8658022880554199\n",
            "baseline_prediction is 0.8223961591720581\n",
            "delta_prediction is 0.043406128883361816\n",
            "sum_attributions are 0.02774590067565441\n",
            "Error percentage is 36.078380197848496\n",
            "prediction is 0.5498675107955933\n",
            "baseline_prediction is 0.04815390333533287\n",
            "delta_prediction is 0.5017136335372925\n",
            "sum_attributions are 0.5160282254219055\n",
            "Error percentage is -2.853139904468837\n",
            "prediction is 0.9448996186256409\n",
            "baseline_prediction is 0.8223961591720581\n",
            "delta_prediction is 0.12250345945358276\n",
            "sum_attributions are 0.030634399503469467\n",
            "Error percentage is 74.99303314362562\n",
            "prediction is 0.7698799967765808\n",
            "baseline_prediction is 0.8223961591720581\n",
            "delta_prediction is -0.052516162395477295\n",
            "sum_attributions are -0.18641877174377441\n",
            "Error percentage is -254.9740941463553\n",
            "prediction is 0.5920398235321045\n",
            "baseline_prediction is 0.8223961591720581\n",
            "delta_prediction is -0.2303563356399536\n",
            "sum_attributions are -0.4075605273246765\n",
            "Error percentage is -76.92612021823989\n",
            "prediction is 0.653647243976593\n",
            "baseline_prediction is 0.04815390333533287\n",
            "delta_prediction is 0.6054933667182922\n",
            "sum_attributions are 0.6266076564788818\n",
            "Error percentage is -3.4871215641926416\n",
            "prediction is 0.7853673696517944\n",
            "baseline_prediction is 0.8223961591720581\n",
            "delta_prediction is -0.03702878952026367\n",
            "sum_attributions are -0.21961987018585205\n",
            "Error percentage is -493.10572403579937\n",
            "prediction is 0.894251823425293\n",
            "baseline_prediction is 0.8223961591720581\n",
            "delta_prediction is 0.07185566425323486\n",
            "sum_attributions are 0.02734389714896679\n",
            "Error percentage is 61.946079779318445\n",
            "prediction is 0.8067879676818848\n",
            "baseline_prediction is 0.8223961591720581\n",
            "delta_prediction is -0.01560819149017334\n",
            "sum_attributions are -0.24075499176979065\n",
            "Error percentage is -1442.4912740298325\n",
            "prediction is 0.7307301759719849\n",
            "baseline_prediction is 0.8223961591720581\n",
            "delta_prediction is -0.09166598320007324\n",
            "sum_attributions are -0.3393963873386383\n",
            "Error percentage is -270.25336497821706\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiP48YSoaEOU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Oh0h0v3aDUe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CE7rgm5jaDb8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DnsM7fhaDYm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WItHEh-0aDRF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KG3dMoYc4C3"
      },
      "source": [
        "# # ATTRS_TSV = '/content/drive/My Drive/IG RESULTS/P1/MEMORY NET/attrs_1_new.tsv'\n",
        "# # with open(ATTRS_TSV, 'a') as outf:\n",
        "# c = 1\n",
        "# ans = ''\n",
        "# for test,mem in tqdm(zip(t,m)): \n",
        "#   # IG = integrated_gradients(graph, sess, min = 1, batch_size= 20, num_reps=20)\n",
        "#   for j in range(len(test)):   \n",
        "#       if c<= 271:\n",
        "#         c+=1\n",
        "#         continue  \n",
        "\n",
        "#       question_attrs = []\n",
        "#       tsv_string= ''\n",
        "#       attrs, words= IG.explain(x = test[j], memory= mem[j], debug=False)\n",
        "#       for ind in range(len(words)):\n",
        "#         if words[ind] is not None:\n",
        "#             question_attrs.append(\n",
        "#                 '|'.join([ words[ind], str(attrs[ind]) ])\n",
        "#                 )\n",
        "#       tsv_string = ['||'.join(question_attrs)]\n",
        "#       ans += '\\t'.join(tsv_string) + '\\n'\n",
        "#       del attrs, words, question_attrs, tsv_string\n",
        "#       gc.collect()\n",
        "#       c+=1\n",
        "      \n",
        "#   if  c>271:\n",
        "#     with open(ATTRS_TSV, 'a') as outf:\n",
        "#       outf.write(ans)\n",
        "#     print(c)\n",
        "#   # del ans, IG\n",
        "#   # gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noETXZGLwM5W"
      },
      "source": [
        "ATTRS_TSV = '/content/drive/My Drive/IG RESULTS/MEMORY NET/P7/attrs.tsv'\n",
        "ATTRS_DIR = '/content/drive/My Drive/IG RESULTS/MEMORY NET/P7/'\n",
        "from collections import Counter\n",
        "\n",
        "counts_list = []\n",
        "top_k = 10\n",
        "with open(ATTRS_TSV) as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        question_attrs = line.split('\\t')[0]\n",
        "        question_tokens = []\n",
        "        attrs = []\n",
        "        for word_attr in question_attrs.split('||'): \n",
        "            word, attr = word_attr.split('|')\n",
        "            question_tokens.append(word)\n",
        "            attrs.append(float(attr))\n",
        "        k = min(top_k, len(question_tokens))\n",
        "        # get top k words by attribution \n",
        "        counts_list.append([question_tokens[i].strip() for i in np.argpartition(attrs, -k)[-k:]])\n",
        "\n",
        "flat_counts_list = [item for sublist in counts_list for item in sublist]\n",
        "frequent_attributions = Counter(flat_counts_list)\n",
        "\n",
        "with open(ATTRS_DIR+'highest_attrs.txt', 'w') as f:\n",
        "  attr_to_save = frequent_attributions.most_common(10)\n",
        "  f.write(str(attr_to_save))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DWI4mZbwV7W"
      },
      "source": [
        "frequent_attributions = Counter(counts_list)\n",
        "frequent_attributions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPR744Ib1f8h"
      },
      "source": [
        "w1 = graph.get_tensor_by_name(\"input/question:0\")\n",
        "w2 = graph.get_tensor_by_name(\"input/memory_key:0\")\n",
        "w3 = graph.get_tensor_by_name(\"input/keep_prob:0\")\n",
        "INPUT_TENSORS = [w1,w2,w3]\n",
        "PRED_TENSOR = graph.get_tensor_by_name('prediction/predict_op:0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyQX61U_5m5_"
      },
      "source": [
        "orig_array =[] \n",
        "for test,mem in (zip(t,m)):\n",
        "\n",
        "        test = np.array(test)\n",
        "        feed = {}\n",
        "        input_df = [test, mem, 1]\n",
        "        # baseline = np.zeros(test.shape)\n",
        "        # baseline_mem = np.zeros(np.array(mem).shape)\n",
        "        # input_df = [baseline, np.array(mem), 1]\n",
        "      \n",
        "        for i, key in enumerate(INPUT_TENSORS):\n",
        "            feed[key.name] = input_df[i]\n",
        "        pred = sess.run(PRED_TENSOR,feed)\n",
        "        orig_array.extend(pred)\n",
        "# orig_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYFY5NRs4qaX"
      },
      "source": [
        "from sklearn.metrics import cohen_kappa_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORvNxWLg5xRA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "59a847b7-0512-49c5-f6b9-7cd4fc28a784"
      },
      "source": [
        "for i,v in enumerate(orig_array):\n",
        "      orig_array[i] = v+2\n",
        "orig_acc = cohen_kappa_score(test_scores, orig_array, weights='quadratic')\n",
        "orig_acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7867273787473417"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDDRU4le0v8C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "outputId": "975e5aa3-2ea2-44c7-bcff-19cb1e9a697a"
      },
      "source": [
        "curve_data = {}\n",
        "question_lengths = {}\n",
        "all_accs = []\n",
        "with open('/content/drive/My Drive/IG RESULTS/MEM MODELS/tokenizer.pkl', 'rb') as f:\n",
        "    tokenizer = pickle.load(f)\n",
        "for K in np.append(0, np.unique(np.floor(np.geomspace(1, len(Counter(counts_list)), 10)))):\n",
        "    # take K most top attributed words\n",
        "    if K in curve_data:\n",
        "        continue\n",
        "    whitelist = set([tokenizer[w] for w in counts_list[:int(K)]]) if K > 0 else set()\n",
        "    print('wh len', len(whitelist))\n",
        "    num_batches = 0\n",
        "    avg_question_length_orig = 0\n",
        "    avg_question_length_new = 0\n",
        "    num_questions = 0\n",
        "    pred_array = []\n",
        "    # iterator over the validation dataset\n",
        "    for test,mem in (zip(t,m)):\n",
        "\n",
        "        test = np.array(test)\n",
        "        new_test = np.zeros(test.shape)\n",
        "        curr_batch_size = int(test.shape[0])\n",
        "        for batch_i in range(curr_batch_size):\n",
        "            len_counter = 0\n",
        "            # avg_question_length_orig += int(test[batch_i])\n",
        "            for word_i, w in enumerate(test[batch_i]):\n",
        "                if int(w) in whitelist:\n",
        "                    new_test[batch_i, len_counter] = int(w)\n",
        "                    len_counter += 1\n",
        "            if len_counter == 0:\n",
        "                len_counter = 1\n",
        "            avg_question_length_new += int(len_counter)\n",
        "            num_questions += 1\n",
        "            \n",
        "        feed = {}\n",
        "        input_df = [new_test, mem, 1]\n",
        "        for i, key in enumerate(INPUT_TENSORS):\n",
        "            feed[key.name] = input_df[i]\n",
        "        pred = sess.run(PRED_TENSOR,feed)\n",
        "        pred_array.extend(pred)\n",
        "\n",
        "    # print(\"avg question length orig: \", float(avg_question_length_orig)/num_questions)\n",
        "    print(\"avg question length new: \", float(avg_question_length_new)/num_questions)\n",
        "    for i,v in enumerate(pred_array):\n",
        "      pred_array[i] = v+2\n",
        "    acc = cohen_kappa_score(pred_array, test_scores, weights='quadratic')\n",
        "    print(\"accuracy for \", K, \" is\", acc)\n",
        "    curve_data[K] = acc\n",
        "    question_lengths[K] = float(avg_question_length_new)/num_questions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wh len 0\n",
            "avg question length new:  1.0\n",
            "accuracy for  0.0  is 0.0\n",
            "wh len 1\n",
            "avg question length new:  1.0\n",
            "accuracy for  1.0  is 0.0\n",
            "wh len 2\n",
            "avg question length new:  1.0336134453781514\n",
            "accuracy for  2.0  is 0.0\n",
            "wh len 5\n",
            "avg question length new:  2.316526610644258\n",
            "accuracy for  5.0  is 0.00015103991478160328\n",
            "wh len 13\n",
            "avg question length new:  20.142857142857142\n",
            "accuracy for  13.0  is 0.010327873085193895\n",
            "wh len 26\n",
            "avg question length new:  66.57422969187675\n",
            "accuracy for  30.0  is 0.17700315687596768\n",
            "wh len 49\n",
            "avg question length new:  94.96918767507003\n",
            "accuracy for  72.0  is 0.1964733462440118\n",
            "wh len 81\n",
            "avg question length new:  122.70868347338936\n",
            "accuracy for  170.0  is 0.2035830142911419\n",
            "wh len 151\n",
            "avg question length new:  228.4173669467787\n",
            "accuracy for  400.0  is 0.49563840479396215\n",
            "wh len 256\n",
            "avg question length new:  264.9495798319328\n",
            "accuracy for  942.0  is 0.5342666714044233\n",
            "wh len 425\n",
            "avg question length new:  293.8347338935574\n",
            "accuracy for  2218.0  is 0.5900155373910694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SitlrRJU08Zv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "807eac5e-cfc2-44cf-d124-0ff72082c214"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "OVERSTABILITY_CURVE_FILE = 'over_1.eps'\n",
        "plt.plot(list(curve_data.keys()), list(curve_data.values())/orig_acc)\n",
        "plt.xscale('symlog')\n",
        "plt.xlabel('num. words in vocab')\n",
        "plt.ylabel('relative accuracy')\n",
        "plt.savefig(OVERSTABILITY_CURVE_FILE, format='eps')\n",
        "plt.savefig(OVERSTABILITY_CURVE_FILE.replace('eps','png'), format='png')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZhcdZ3v8fenOxvZt06A7JCNBFlCE1RAExYNVwVHUMGZ6zg68uCIMjozDo4zyEXnjsvo3HHkqmGRRR1EXG5mjDI6SQggSxJWE7ohdLaOhO4knT3p9Xv/qOpQNJ109XK6ts/refrpOqdOnfp2Vac/+Z7zq/NTRGBmZpZvynJdgJmZWWccUGZmlpccUGZmlpccUGZmlpccUGZmlpcG5LqA7ho/fnxMnz4912WYmVkfWbdu3c6IqOi4vuACavr06axduzbXZZiZWR+RtKWz9T7EZ2ZmeckBZWZmeckBZWZmeckBZWZmeckBZWZmeckBZWZmeckBZWZmeckBZWZmPdLS2pbo/h1QZmbWbU/U7OId/7Kax2t2JfYcBXclCTMzy50DjS189VdV3Pv4FqaOHcqAMiX2XA4oMzPLykMv1vN3P3ueP+w9zMcumMFfvWM2QwclFyMOKDMzO669h5r58i838JN1tZxaMYwHrnsr50wbk/jzOqDMzOyY/mv9Dr7wi9+z+2ATn1x8Kp+6aBZDBpb3y3M7oMzM7A12HWjki8vW85/PvcJpJ43k+x85l9MnjerXGhxQZmZ2VETwH8+9ws3L1rP/SDN/delsrlt0KgPL+3/QtwPKzMwAeHXfEb7w89/z2xde5cwpo/n6VWcwe+KInNXjgDIzK3ERwU/W1vKlX26gqaWNL/yP0/joBTMoT3AIeTYcUGZmJWzb7kP83c+f5+GXdrJwxli+euUZzBg/LNdlAQ4oM7OS1NYW/OCJLXzlV1UI+NIV8/nj86ZRluOuKZMDysysxNTUH+DGnz7Pk5t3c+Gs8fzT+97E5DFDc13WGyQaUJKWAP8KlAO3R8RXOtz/L8Di9OJQYEJEjE6yJjOzUtXS2sYdj2zim795kcEDyvjaVWfw/nMmI+VP15QpsYCSVA7cClwK1AJrJC2LiA3t20TEZzK2/xRwdlL1mJmVsuod+/ncA8/ybO1eLp03kS+/93QmjhyS67KOK8kOaiGwMSJqACTdB1wBbDjG9tcAX0ywHjOzktPU0sZ3Vr3Mt1e+xIghA/m3a87m3WeclLddU6YkA2oSsC1juRY4r7MNJU0DZgArjnH/tcC1AFOnTu3bKs3MitTztXv5mweepWrHfi4/82S++J55jBs+ONdlZS1fBklcDTwQEa2d3RkRS4GlAJWVldGfhZmZFZojza3863+/xNLVNYwbNojbPlzJpfMm5rqsbksyoLYDUzKWJ6fXdeZq4JMJ1mJmVhLWbt7N5376HDX1B/lA5WS+8K55jDphYK7L6pEkA2oNMEvSDFLBdDXwoY4bSZoLjAEeS7AWM7Oidqipha/9upq7H9vMyaNO4J6PLuRtsytyXVavJBZQEdEi6XrgQVLDzO+MiPWSbgHWRsSy9KZXA/dFhA/dmZn1wKMbd3Ljz55j2+7DfPgt0/jckrkMH5wvZ3B6LtGfICKWA8s7rLupw/LNSdZgZlas1m1p4LbVNfx6/Q6mjxvKj699M+edMi7XZfWZwo9YM7MS0toW/PaFV1m6uoZ1WxoYdcJAPn3RTD6xaCYnDOqfiQT7iwPKzKwAHGlu5YF1tdzxyCY27TzI5DEncPN75vH+yikMK4LDeZ0pzp/KzKxI7DrQyD2PbeHex7ew+2ATZ04exbc/dDZL5p/IgBxMItifHFBmZnmopv4Atz+yiZ+uq6WxpY1LTpvAxy88hYUzxhbEVSD6ggPKzCxPRATrtjSwdHUNv3nhVQaWl3Hlgkl87IJTmDlheK7L63cOKDOzHGttC/5r/Q6WPlzD01v3MHroQD61eCb/8y3TqRhROJcm6msOKDOzHDnU1MID62q5/eFNbN19iKljh3LLFfO56pzJDB3kP89+BczM+ln9/kbueWwz9z6+hT2Hmjlrymg+f9lc3jH/RMrzaEbbXHNAmZn1k411B7jjkRp++tR2mlvbuPS0iVz7tlM4Z9qYkhn40B0OKDOzBEUET27azW0P1/DbF+oYPKCM958zmY9dMINTKkpv4EN3OKDMzBLQ0trGr9fv4LbVNTxbu5exwwZxw8Wz+PBbphXUnEy55IAyM+tDBxtbuH/tNu54ZBO1DYeZMX4YX37v6Vy5YHLRXYooaQ4oM7M+UL+/kbt+t4kfPL6VvYebqZw2hn949zwuOW2iBz70kAPKzKyX2tqCK7/zO7Y1HGLJ/BP58wtTAx+sdxxQZma99Ps/7GXr7kN87coz+MC5U7p+gGWluK80aGbWD1ZU1SHBxadNyHUpRcUBZWbWSyur6zlrymiPzutjDigzs17YeaCR52r3sHiOu6e+5oAyM+uFVdX1RMBFcx1QfS3RgJK0RFK1pI2SbjzGNh+QtEHSekk/SrIeM7O+trK6jgkjBjP/5JG5LqXoJDaKT1I5cCtwKVALrJG0LCI2ZGwzC/g8cH5ENEjyf0HMrGA0t7ax+sV6Ljv9RF9LLwFJdlALgY0RURMRTcB9wBUdtvk4cGtENABERF2C9ZiZ9amntjSw/0iLD+8lJMmAmgRsy1iuTa/LNBuYLelRSY9LWtLZjiRdK2mtpLX19fUJlWtm1j0rqusYWC7Onzk+16UUpVwPkhgAzAIWAdcAt0ka3XGjiFgaEZURUVlRUdHPJZqZdW5VVT3nTh/LiCEDc11KUUoyoLYDmR+pnpxel6kWWBYRzRGxCXiRVGCZmeW17XsOU/3qfh/eS1CSAbUGmCVphqRBwNXAsg7b/IJU94Sk8aQO+dUkWJOZWZ9YUZU6Zb7In39KTGIBFREtwPXAg8ALwP0RsV7SLZIuT2/2ILBL0gZgJfA3EbErqZrMzPrKqqo6po4dyqkVw3JdStFK9GKxEbEcWN5h3U0ZtwP4bPrLzKwgHGlu5dGXd/LByikeXp6gXA+SMDMrOI/X7OJIcxuLff4pUQ4oM7NuWllVx5CBZbz5lHG5LqWoOaDMzLohIlhRXcf5p45nyEBP4Z4kB5SZWTe8XH+QbbsP+/BeP3BAmZl1w8r08HIHVPIcUGZm3bCyuo45E0cwafQJuS6l6DmgzMyytP9IM09u2s2iub7kWn9wQJmZZenRjTtpaQsu8tUj+oUDyswsSyuq6hgxZAALpo3JdSklwQFlZpaFtrZgZXU9b5tdwcBy/+nsD36VzcyysOGVfdTvb/ThvX7kgDIzy8KKqjokePscD5DoLw4oM7MsrKyu44zJoxk/fHCuSykZDigzsy7sOtDIM9v2sNjdU79yQJmZdWH1S/VE4Nlz+5kDysysCyuq6hk/fDCnnzwq16WUlC4DStI3JM3vj2LMzPJNS2sbD1XXsWhOBWVlnpywP2XTQb0ALJX0hKTrJPm/EGZWMp7etod9R1p8eC8HugyoiLg9Is4HPgxMB56T9CNJi5Muzsws11ZU1TGgTFwwa3yuSyk5WZ2DklQOzE1/7QSeBT4r6b4uHrdEUrWkjZJu7OT+j0iql/RM+uvPe/AzmJklZmVVHZXTxzByyMBcl1JyBnS1gaR/Ad4NrAD+d0Q8mb7rq5Kqj/O4cuBW4FKgFlgjaVlEbOiw6Y8j4voeVW9mlqA/7DlM1Y79fP6yubkupSR1GVDAc8DfR8TBTu5beJzHLQQ2RkQNQLrbugLoGFBmZnlpZXVqckKff8qNbA7x7SEjyCSNlvRegIjYe5zHTQK2ZSzXptd1dKWk5yQ9IGlKZzuSdK2ktZLW1tfXZ1GymVnvrayqZ/KYE5g5YXiuSylJ2QTUFzODKCL2AF/so+f/D2B6RJwB/Aa4u7ONImJpRFRGRGVFhT/JbWbJO9LcyqMbd7J4zgQkDy/PhWwCqrNtsjk0uB3I7Igmp9cdFRG7IqIxvXg7cE4W+zUzS9yTm3ZzuLnVh/dyKJuAWivpm5JOTX99E1iXxePWALMkzZA0CLgaWJa5gaSTMhYvJ/WZKzOznFtRVcfgAWW8+ZRxuS6lZGUTUJ8CmoAfp78agU929aCIaAGuBx4kFTz3R8R6SbdIujy92aclrZf0LPBp4CPd/xHMzPrequo63nrqOE4YVJ7rUkpWl4fq0qP33vAZpmxExHJgeYd1N2Xc/jzw+Z7s28wsKTX1B9i86xAfvWBGrkspadl8DqoC+BwwHxjSvj4iLkqwLjOznFlRlRpevtiz5+ZUNof4fghUATOA/wVsJnV+ycysKK2qrmfWhOFMGTs016WUtGwCalxE3AE0R8RDEfFRwN2TmRWlA40tPLFpF4s9ei/nshku3pz+/oqkdwF/AMYmV5KZWe48unEnza3hw3t5IJuA+nJ6io2/Av4NGAl8JtGqzMxyZGVVHSMGD6By+phcl1LyjhtQ6Qu+zoqI/wT2Ap5iw8yKVkSwsrqOC2ePZ2C5JxzPteO+AxHRClzTT7WYmeXUhlf28eq+Rhb58F5eyOYQ36OSvk3qQ7pHr2geEU8lVpWZWQ6sTA8vXzTH1/zMB9kE1Fnp77dkrAs8ks/MiszK6nrOmDyKCSOGdL2xJS6bK0n4vJOZFb2Gg008vbWB6y+aletSLC2bK0nc1Nn6iLils/VmZoVo9Uv1tIUnJ8wn2Rziy5xJdwip6d991XEzKyorquoYN2wQZ0waletSLC2bQ3zfyFyW9M+krlBuZlYUWtuCh16s56K5Eygr8+SE+aInA/2Hkpp80MysKDyzrYE9h5p9eC/PZHMO6nlSo/YAyoEKXj+iz8ysoK2oqqO8TFw4y8PL80k256DenXG7BXg1PRmhmVlRWFlVzznTxjDqhIG5LsUyZHOI7yRgd0RsiYjtwAmSzku4LjOzfrFj7xE2vLLPF4fNQ9kE1HeAAxnLB9PrzMwK3qrq1NUjfP4p/2QTUIqI9nNQREQb2R0aNDPLeyuq6jh51BBmTxye61Ksg2wCqkbSpyUNTH/dANRks3NJSyRVS9oo6cbjbHelpJBUmW3hZma91djSyiMbd7J47gQkDy/PN9kE1HXAW4HtQC1wHnBtVw9KT9VxK3AZMA+4RtK8TrYbAdwAPJF92WZmvbdmUwOHmlp9eC9PZfNB3Trg6h7seyGwMSJqACTdB1wBbOiw3ZeArwJ/04PnMDPrsRVVdQwaUMZbTh2X61KsE112UJLuljQ6Y3mMpDuz2PckYFvGcm16Xea+FwBTIuKXXdRwraS1ktbW19dn8dRmZl1bVV3HW04Zx9BBPq2ej7I5xHdGROxpX4iIBuDs3j6xpDLgm6Smkj+uiFgaEZURUVlR4Q/SmVnvbd55kJqdB1nsuZ/yVjYBVSZpTPuCpLFkN4pvOzAlY3lyel27EcDpwCpJm4E3A8s8UMLM+sOKqvbh5RNzXIkdSzZB8w3gMUk/AQRcBfxjFo9bA8ySNINUMF0NfKj9zojYC4xvX5a0CvjriFibdfVmZj20srqOUyqGMXXc0FyXYsfQZQcVEfcAVwKvAjuA90XEvVk8rgW4ntSVz18A7o+I9ZJukXR578o2M+u5g40tPFGzm4t89Yi8ltWZwXSw1JOaDwpJUyNiaxaPWw4s77DuWBMgLsqmFjOz3vrdy7toam3z8PI8l80ovsslvQRsAh4CNgO/SrguM7PErKiqY/jgAVROH5vrUuw4shkk8SVSAxhejIgZwMXA44lWZWaWkIhgVXUdF8wcz6ABPZkSz/pLNu9Oc0TsIjWarywiVgIeaWdmBalqx35e2XuExXM9vDzfZXMOao+k4cBq4IeS6khd0dzMrOC0Dy/39Br5L5sO6grgEPAZ4NfAy8B7kizKzCwpq6rrOH3SSCaMHJLrUqwL2VyLr71bagPuTrYcM7Pk7DnUxLotDXxy8cxcl2JZ8BlCMysZq1/aSVvAYg8vLwgOKDMrGSur6hg7bBBnTh7d9caWc1kFlKQTJM1Juhgzs6S0tqWGl799dgXlZZ6csBBk80Hd9wDPkBoggaSzJC1LujAzs770bO0eGg41s8hXLy8Y2XRQN5OafHAPQEQ8A8xIsCYzsz63sqqOMsHbZzugCkW2H9Td22FdJFGMmVlSVlbXcc60MYweOijXpViWsgmo9ZI+BJRLmiXp34DfJVyXmVmfqdt3hN9v38cifzi3oGQTUJ8C5gONwI+AvcBfJlmUmVlfWlVdD+CrlxeYbC51NDcivgB8IelizMySsKKqjpNGDWHuiSNyXYp1QzYd1DckvSDpS5JOT7wiM7M+1NTSxiMbd7JozgQkDy8vJNnMqLsYWAzUA9+T9Lykv0+8MjOzPrB2824ONLaw2MPLC05WH9SNiB0R8S3gOlKfiep0Vlwzs3yzoqqOQeVlnD9zfK5LsW7K5oO6p0m6WdLzQPsIvsmJV2Zm1gdWVtdx3iljGTY4m1Pulk+y6aDuJPUh3XdGxKKI+E5E1GWzc0lLJFVL2ijpxk7uvy59yPAZSY9ImtfN+s3MjmnrrkO8XH/Qcz8VqGym23hLT3YsqRy4FbgUqAXWSFoWERsyNvtRRHw3vf3lwDeBJT15PjOzjlZUvQp4eHmhOmZASbo/Ij6QPrSXeeUIARERZ3Sx74XAxoioSe/vPlKTHx4NqIjYl7H9MHyFCjPrQyur65kxfhjTxw/LdSnWA8froG5If393D/c9CdiWsVwLnNdxI0mfBD4LDAIu6mxHkq4FrgWYOnVqD8sxs1JyqKmFx2p28SfnTct1KdZDxzwHFRGvpG/+RURsyfwC/qKvCoiIWyPiVOBvgU6Hr0fE0oiojIjKigoPFTWzrj328i6aWtp8eK+AZTNI4tJO1l2WxeO2A1Mylien1x3LfcB7s9ivmVmXVlTVMXRQOefOGJPrUqyHjhlQkj6RPv80R9JzGV+bgOey2PcaYJakGZIGAVcDr5tHStKsjMV3AS91/0cwM3u9iGBVdT0XzBzP4AHluS7Heuh456B+BPwK+Ccgc4j4/ojY3dWOI6JF0vXAg0A5cGdErJd0C7A2IpYB10u6BGgGGoA/7eHPYWZ21IuvHmD7nsNcf9HMXJdivXDMgErPAbUXuAZA0gRgCDBc0vCI2NrVziNiObC8w7qbMm7f8IYHmZn10oqq1Ec1/fmnwpbVlO+SXgI2AQ8Bm0l1VmZmeWlldR2nnTSSE0cNyXUp1gvZDJL4MvBm4MWImAFcDDyeaFVmZj2091Az67Y0cNFcj/gtdNlO+b4LKJNUFhErgcqE6zIz65GHN9bT2hYeXl4Esrl64h5Jw4HVwA8l1QEHky3LzKxnVlTVMXroQM6a4uHlhS6bDuoK4DDwGeDXwMvAe5IsysysJ9ragoeq63n77ArKyzw5YaHL5mKxmd3S3QnWYmbWqaaWNhoONbHrQBO7Dzax62Bjxu0mdh1oZPfBJnYeaGTXwSaP3isSx7tY7H46uUgsr10sdmTCtZlZkWoPnJ3pYNl9MBU+uw42ZtxuX9/IviMtne6nvEyMGTqIccMGMW74IE6fNIp3zj+Rd84/sZ9/IkvC8T4HNaI/CzGz4rD7YBNPbWlgy+5D7E53Ou1h097l7M8ycOafPDJ9ezBjhw16w+1RJwykzIfyilZWU0xKugCYFRHflzQeGBERm5ItzczyXWtbUL1jP09tbUh9bWlg865DR+9vD5zxwwcxdlgqcManA2bssPb1g4/eHjnEgWOv6TKgJH2R1LDyOcD3SU2L8QPg/GRLM7N8s+dQE09v3XM0kJ7ZuoeDTa0AjB8+iLOnjuGD505lwdTRzJ44wh2O9Uo2HdQfAWcDTwFExB8k+fCfWZFraws21h9g3ZZUZ/TU1gZerk+NmSovE3NPHMH7FkxmwbTRLJg6hqljhyI5jKzvZBNQTRERkgJAkqemNCtC+44088zWPalA2trAM9v2HD1XNGboQBZMHcP7Fkzm7KmjOXPyaIYNzuoMgVmPZfMbdr+k7wGjJX0c+ChwW7JlmVmS2tqCmp0HeWprA09vbWDdlgZeqjtABEgwZ+II3nPmySyYOoYFU0czY/wwd0fW744bUEr9Rv4YmAvsI3Ue6qaI+E0/1GZmfeRAYwvPbttz9FDdU1v3sPdwMwAjhwzg7KljeNebTuacaWM4c8ooRgwZmOOKzboIqPShveUR8SbAoWRWQJ7a2sBP19Xy1NY9VO/YR1v6U42zJgxnyfwTWTBtNOdMG8Mp44d7IIPlpWwO8T0l6dyIWJN4NWbWJ/YeauZP73iSAM6eOppLL5rFgqmjOXvKGEYNdXdkhSGbgDoP+GNJW0hdJLb9ShJnJFqZmfXYHY9uYn9jC7+64UJOO8kXfbHClE1AvTPxKsysz+w91Mz3H9nEkvknOpysoGVzsdgt/VGImfWN9u7p0xfPynUpZr2SzXQbPSZpiaRqSRsl3djJ/Z+VtEHSc5L+W9K0JOsxK3Z7Dzfz/UdT3dO8k909WWFLLKAklQO3ApcB84BrJM3rsNnTQGX6fNYDwNeSqsesFNz5yCb2H3H3ZMUhyQ5qIbAxImoiogm4j9Tkh0dFxMqIaL+y5OPA5ATrMStqew83c+ejm3jn/InunqwoJBlQk4BtGcu16XXH8jHgV53dIelaSWslra2vr+/DEs2Kh7snKzaJnoPKlqQ/IXXF9K93dn9ELI2IyoiorKio6N/izApAZvc0/+RRuS7HrE8kebXH7cCUjOXJ6XWvI+kS4AvA2yOiMcF6zIrW9x9192TFJ8kOag0wS9IMSYOAq4FlmRtIOhv4HnB5RNQlWItZ0dp7uJk7HtnEO+a5e7LiklhARUQLcD3wIPACcH9ErJd0i6TL05t9HRgO/ETSM5KWHWN3ZnYM7p6sWCU6oUtELAeWd1h3U8btS5J8frNit/dwM3emu6fTJ7l7suKSF4MkzKxn7np0M/vcPVmRckCZFah9R5q545EaLnX3ZEXKAWVWoNq7pxvcPVmRckCZFaB9R5q5/WF3T1bcHFBmBcjdk5UCB5RZgWnvni45zd2TFTcHlFmBuTvdPf3lJe6erLg5oMwKyL4jzdz+yCZ3T1YSHFBmBeTuRzez93Czzz1ZSXBAmRWI/Ue7pwm8abK7Jyt+DiizAnH379q7p9m5LsWsXzigzArA/iPN3PbwJi6e6+7JSocDyqwAHO2ePHLPSogDyizPtZ97unjuBM6YPDrX5Zj1GweUWZ6757Et7Dnk7slKjwPKLI+lzj3VcJG7JytBDiizPHa0e/LnnqwEOaDM8tSBxpaj3dOZU9w9WelxQJnlqbt/t9ndk5W0RANK0hJJ1ZI2Srqxk/vfJukpSS2SrkqyFrNC0t49LZ5T4e7JSlZiASWpHLgVuAyYB1wjaV6HzbYCHwF+lFQdZoXoaPd0ia8aYaVrQIL7XghsjIgaAEn3AVcAG9o3iIjN6fvaEqzDrKAcaGzh9nT3dJa7JythSR7imwRsy1iuTa/rNknXSloraW19fX2fFGeWr+55bDMN7p7MCmOQREQsjYjKiKisqKjIdTlmiTnY2MJtq2tY5O7JLNGA2g5MyVienF5nZsdwz2NbUt2TR+6ZJRpQa4BZkmZIGgRcDSxL8PnMCtrBxhaWrn6ZRXMqOHvqmFyXY5ZziQVURLQA1wMPAi8A90fEekm3SLocQNK5kmqB9wPfk7Q+qXrM8p27J7PXS3IUHxGxHFjeYd1NGbfXkDr0Z1bS2runt89292TWriAGSZgVu3sfT3dPvmK52VEOKLMcS3VPNbx9dgUL3D2ZHeWAMsuxex/fwu6DTe6ezDpwQJnlUHv39DZ3T2Zv4IAyy6EftHdPHrln9gYOKLMcOdTUwvfS3dM509w9mXXkgDLLkXsfc/dkdjwOKLMcONSUOvd04azx7p7MjsEBZZYDP3h8C7sONvGXHrlndkwOKLN+dqiphe891N49jc11OWZ5ywFl1s/auyefezI7PgeUWT/KPPdUOd3dk9nxOKDM+tEPH9/KzgPunsyy4YAy6yepzz29zAUz3T2ZZcMBZdZPjnZPHrlnlhUHlFk/ONzUerR7Otfdk1lWHFBm/eCHT2xx92TWTQ4os4Qdbmrluw+9zPkzx7l7MuuGRKd8NysFR5pb2bH3CDv2HXn99/Tt2obD7DzQxP+9eHauSzUrKIkGlKQlwL8C5cDtEfGVDvcPBu4BzgF2AR+MiM1J1mSWrYhg7+HmNwROx+97DjW/4bEjBg9g4qghnDRqCIvmVLBw+lgWznD3ZNYdiQWUpHLgVuBSoBZYI2lZRGzI2OxjQENEzJR0NfBV4INJ1WTWrrUtqN/fmA6aw+zYe4RX9h3h1Q7hc6S57XWPk2DcsMGcNGoIk8cMpXL6GE4adQITR6bCaOLIIZw4agjDB/vghFlvJfmvaCGwMSJqACTdB1wBZAbUFcDN6dsPAN+WpIiIpIr6/fa97DrYlNTuLUPm2xgdbgQZ90WH753sIzpsm7llx8dlbtPU2sqr+xqPdkDtIVS3/whtHX7LBpWXMWFkKnxOnzSKS+dNPBo47eEzYcQQBg3wqVuz/pBkQE0CtmUs1wLnHWubiGiRtBcYB+zM3EjStcC1AFOnTu1VUf/nty/y2xfqerUPKzyZh9xmTRh/NHAyu56xQwdRVqZcl2pmaQVxHCIilgJLASorK3vVXd142Wl8YtHMPqnLuqaMv/c6uk6vW87cTum16iQnOm7zunXHuG9AmZgw0ofczApRkv9qtwNTMpYnp9d1tk2tpAHAKFKDJRIzc8LwJHdvZmZ9JMmD6WuAWZJmSBoEXA0s67DNMuBP07evAlYkef7JzMwKR2IdVPqc0vXAg6SGmd8ZEesl3QKsjYhlwB3AvZI2ArtJhZiZmVmy56AiYjmwvMO6mzJuHwHen2QNZmZWmDxe1szM8pIDyszM8pIDyszM8pIDyszM8pIDyszM8pIK7WNHkuqBLb3czXg6XE6pABRizaXC703f8OvYe4X6Gk6LiIqOKwsuoPqCpLURUZnrOrqjEGsuFX5v+oZfx94rttfQh/jMzCwvOaDMzCwvlWpALc11AT1QiDWXCr83fcOvY+8V1WtYkuegzMws/5VqB2VmZnnOAeFvcVcAAAXqSURBVGVmZnnJAWVmZnnJAWVmZnmppAJK0hJJ1ZI2Srox1/X0hqRhku6WdJukP851PfYaSadIukPSA7mupVBJem/6d/vHkt6R63oKlaTTJH1X0gOSPpHrerqrZAJKUjlwK3AZMA+4RtK83Fb1epLulFQn6fcd1ncWrO8DHoiIjwOX93uxJaY7701E1ETEx3JTaf7q5mv4i/Tv9nXAB3NRb77q5uv4QkRcB3wAOD8X9fZGyQQUsBDYmP7j0QTcB1yR45o6ugtYkrniOME6GdiW3qy1H2ssVXeR/XtjnbuL7r+Gf5++315zF914HSVdDvySDrObF4JSCqhJvPYHHaA2vS5vRMRqYHeH1ccK1lpSIQWl9T7mRDffG+tEd15DpXwV+FVEPNXfteaz7v4uRsSyiLgMKLhTAf7Dlv+OFaw/A66U9B3gP3JRmHX+3kgaJ+m7wNmSPp+b0grGsX6/PwVcAlwl6bpcFFZgjvW7uEjStyR9jwLsoAbkuoB+tB2YkrE8Ob2uIEXEQeDPcl2HvVFE7CJ17sR6KCK+BXwr13UUuohYBazKcRk9Vkod1BpglqQZkgYBVwPLclxTNooqWIuM35ve82vYN4rydSyZgIqIFuB64EHgBeD+iFif26qyUqjBWgr83vSeX8O+UZSvY8kEFEBELI+I2RFxakT8Y67r6UjSvwOPAXMk1Ur6WAEHa1Hxe9N7fg37Rim9jr6auZmZ5aWS6qDMzKxwOKDMzCwvOaDMzCwvOaDMzCwvOaDMzCwvOaDMzCwvOaDM8pCkuyRdleW2l+fT/Gbdqd3seErpWnxmeUnSgPQHLXskIpZRBFcNMOvIHZQVDUnTJb2Qnol1vaT/knRC+r5VkirTt8dL2py+/RFJv5D0G0mbJV0v6bOSnpb0uKSxXTznLyWdkb79tKSb0rdvkfTx9LQRX5f0e0nPS/pg+v5Fkh6WtAzYkN7u2+kJ534LTMh4jq9I2iDpOUn/3EkNH5H07fTtu9JXr/6dpJrOOpn0/j6ZsXyzpL8+Vq3pbf42ve5ZSV9Jr/u4pDXpdT+VNDTjaS6RtFbSi5Leffx3zqxzDigrNrOAWyNiPrAHuDKLx5xOaobic4F/BA5FxNmkLifz4S4e+zBwoaRRQAuvzVp6IbA6vd+zgDNJTR/xdUknpbdZANwQEbOBPwLmkJps7sPAWwEkjUvfNz8izgC+nMXPcxJwAfBu4Cud3P9jUjOstvtAel2ntUq6jNTcQudFxJnA19KP+1lEnJte9wKQOYvwdFJzFL0L+K6kIVnUbfY6DigrNpsi4pn07XWk/lB2ZWVE7I+IemAvr82v9XwWj38YeBupYPolMDzdScyIiGpSQfHvEdEaEa8CD5EKQoAnI2JT+vbbMrb7A7AivX4vcAS4Q9L7gENZ/Dy/iIi2iNgATOx4Z0Q8DUyQdLKkM4GGiNh2nFovAb4fEYfSj2+fLO/0dBf4PKnJ8OZnPM396RpeAmqAuVnUbfY6PgdlxaYx43YrcEL6dguv/Yes4//mMx/TlrHcRtf/RtYAlaT+CP8GGA98nFQ4duVgVxtERIukhcDFwFWkLgh6URcPy/x5dIxtfpLe34mkuqeeuAt4b0Q8K+kjwKKM+zpe5NMX/bRucwdlpWIzcE76dp+NMEtPr70NeD+pQ4IPA39N6vAe6eUPSiqXVEGqU3qyk12tztjuJGAxgKThwKiIWA58htTht77wY1JTMlxFKqyOV+tvgD9rP8eUcV5uBPCKpIG8cTrx90sqk3QqcApQ3Ud1WwlxB2Wl4p+B+yVdS+pQXLcoPe14RHy3k7sfBi6OiMOSHiY1WdzD6ft+DrwFeJZUF/G5iNghqeMhr5+T6ow2AFtJhR2kQuD/pc/hCPhsd2vvTESslzQC2B4RrxyvVuDXks4C1kpqIjV1+N8B/wA8AdSnv4/IeIqtpMJtJHBdRBzpi7qttHi6DTMzy0s+xGdmZnnJAWVmZnnJAWVmZnnJAWVmZnnJAWVmZnnJAWVmZnnJAWVmZnnp/wNUvQ5Tjz9bpAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgHguiRQ8hf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4704c011-5b4c-4f35-e5e1-12ec63b2456e"
      },
      "source": [
        "ATTRS_TSV = '/content/drive/My Drive/IG RESULTS/P1/MEMORY NET/attrs_1_new.tsv'\n",
        "with open(ATTRS_TSV, 'a') as outf:\n",
        "  c = 1\n",
        "  for test,mem in tqdm(zip(t,m)): \n",
        "    ans = ''\n",
        "    IG = integrated_gradients(graph, sess, min = 2, batch_size= 20, num_reps=20)\n",
        "    for j in range(len(test)):\n",
        "        question_attrs = []\n",
        "        tsv_string= ''\n",
        "        attrs, words= IG.explain(x = test[j], memory= mem[j], debug=False)\n",
        "        for ind in range(len(words)):\n",
        "          if words[ind] is not None:\n",
        "              question_attrs.append(\n",
        "                  '|'.join([ words[ind], str(attrs[ind]) ])\n",
        "                  )\n",
        "        tsv_string = ['||'.join(question_attrs)]\n",
        "        ans += '\\t'.join(tsv_string) + '\\n'\n",
        "        del attrs, words, question_attrs, tsv_string\n",
        "        gc.collect()\n",
        "        c+=1\n",
        "        print(c)\n",
        "    outf.write(ans)\n",
        "    outf.flush()\n",
        "    del ans, IG\n",
        "    gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "1it [01:55, 115.85s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "2it [04:41, 130.90s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "3it [08:19, 157.04s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "4it [12:43, 188.90s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "5it [18:02, 228.18s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "6it [24:22, 273.73s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "7it [31:28, 319.38s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "8it [39:23, 365.93s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "9it [48:24, 418.39s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "10it [58:10, 468.90s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "11it [1:08:46, 518.78s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "12it [1:20:57, 582.42s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "13it [1:34:25, 650.21s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "14it [1:48:30, 708.67s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "15it [2:03:25, 764.66s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "16it [2:19:07, 817.63s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "17it [2:35:40, 870.40s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "18it [2:53:02, 921.83s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "19it [3:11:20, 974.72s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "20it [3:30:31, 1027.61s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfeBL1rEc20k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CACxLRdLWnG_"
      },
      "source": [
        "###### MAIN RUN\n",
        "import time\n",
        "start = time.time()\n",
        "c = 1\n",
        "for test,mem in zip(t,m): \n",
        "  for j in range(len(test)):\n",
        "    out = IG.explain(x = test[j], memory= mem[j], debug=False) ## debug for errors\n",
        "    convert_html_to_pdf(out, ATTRS_DIR+str(fold_no)+'_'+str(c)+'.pdf')\n",
        "    c+=1\n",
        "end = time.time()\n",
        "print(start-end)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rR4C2kCYXnOk"
      },
      "source": [
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "gdd.download_file_from_google_drive\\\n",
        "(file_id='1CIEpiDmzLmJ6LMCVSOmCKw_eOg4ocuS4', dest_path='/content/AES.zip', unzip=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJd8Q5186K-w"
      },
      "source": [
        "import nltk\n",
        "from nltk import tokenize\n",
        "import pickle\n",
        "from random import shuffle\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tY1lv95j6K79"
      },
      "source": [
        "def create_adv_data(data_test, file_path):\t\n",
        "    def rindex(lst, value):\n",
        "      lst.reverse()\n",
        "      i = lst.index(value)\n",
        "      lst.reverse()\n",
        "      return len(lst) - i - 1\n",
        "    \n",
        "    with open('/content/calling-out-bluff/'+file_path, 'rb') as handle:\n",
        "        songs= pickle.load(handle)\n",
        "    song_list = []\n",
        "    for i in songs:\n",
        "      song_sents = tokenize.sent_tokenize(i)\n",
        "      song_list.extend(song_sents)\n",
        "    \n",
        "    song_lyrics = []\n",
        "    for x in song_list:\n",
        "      a = []\n",
        "      for w in tokenize.word_tokenize(x):\n",
        "        try:\n",
        "          a.append(tokenizer[w.lower()])\n",
        "        except:\n",
        "          a.append(0)\n",
        "      song_lyrics.append(a)\n",
        "    \n",
        "    songs_beg=[]\n",
        "    mem_beg = []\n",
        "    songs_end=[]\n",
        "    mem_end = []\n",
        "    \n",
        "    for r in data_test:\n",
        "      test = r[0]\n",
        "      mem = r[1]\n",
        "      for i,r in enumerate(test):\n",
        "        if 0 in r:\n",
        "          ind = rindex(r.tolist(), 0)\n",
        "        else:\n",
        "          ind = 0\n",
        "        ar = random.sample(song_lyrics, 20)\n",
        "        ar = [x for sublist in ar for x in sublist]\n",
        "        ar = ar[:ind+1]\n",
        "        songs_beg.append(ar + r[ind+1:].tolist())\n",
        "        songs_end.append(r[ind+1:].tolist() + ar)\n",
        "        mem_beg.append(mem[i])\n",
        "        mem_end.append(mem[i])\n",
        "    \n",
        "    song_beg_data = pad_sequences(songs_beg, maxlen=MAX_SEQUENCE_LENGTH, padding = 'pre', truncating='pre') #padding to max_length\n",
        "    song_end_data = pad_sequences(songs_end, maxlen=MAX_SEQUENCE_LENGTH, padding = 'post', truncating='post') #padding to max_length\n",
        "    return song_beg_data, song_end_data, mem_beg, mem_end"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOmfSpAu6K6I"
      },
      "source": [
        "def predict_and_score_save(model, beg, end, orig, f_name, prompt, data=None):\n",
        "    def npos(orig, new):\n",
        "      count = 0\n",
        "      for i in range(len(orig)):\n",
        "        if new[i]>orig[i]:\n",
        "          count+=1 \n",
        "      return count\n",
        "\n",
        "    def nneg(orig, new):\n",
        "      count = 0\n",
        "      for i in range(len(orig)):\n",
        "        if new[i]<orig[i]:\n",
        "          count+=1 \n",
        "      return count\n",
        "\n",
        "    def nsame(orig, new):\n",
        "      count = 0\n",
        "      for i in range(len(orig)):\n",
        "        if new[i]==orig[i]:\n",
        "          count+=1 \n",
        "      return count\n",
        "\n",
        "    def mu(orig, new):\n",
        "      s=0\n",
        "      n = len(orig)\n",
        "      for i in range(n):\n",
        "        s+=(orig[i] - new[i])\n",
        "      return s/n\n",
        "\n",
        "    def absmu(orig, new):\n",
        "      s=0\n",
        "      n = len(orig)\n",
        "      for i in range(n):\n",
        "        s+=(orig[i] - new[i])\n",
        "      return abs(s)/n\n",
        "\n",
        "    def sd(orig, new):\n",
        "      mu_val = mu(orig, new)\n",
        "      s=0\n",
        "      n = len(orig)\n",
        "      for i in range(n):\n",
        "        s+=(orig[i] - new[i] - mu_val)**2\n",
        "      return (s/n)**(1/2)\n",
        "\n",
        "    def muneg(orig, new):\n",
        "      s=0\n",
        "      n = len(orig)\n",
        "      for i in range(n):\n",
        "        if orig[i] < new[i]:\n",
        "          s+=-(orig[i] - new[i])\n",
        "      return s/n\n",
        "\n",
        "    def mupos(orig, new):\n",
        "      s=0\n",
        "      n = len(orig)\n",
        "      for i in range(n):\n",
        "        if orig[i] > new[i]:\n",
        "          s+=-(orig[i] - new[i])\n",
        "      return s/n\n",
        "    \n",
        "    def get_pred_stats(orig, new):\n",
        "      a = ('kappa', cohen_kappa_score(orig, new, weights='quadratic'))\n",
        "      b = ('NPOS', npos(orig, new))\n",
        "      c = ('NNEG', nneg(orig, new))\n",
        "      d = ('NSAME', nsame(orig, new))\n",
        "      e = ('MU', mu(orig, new))\n",
        "      f = ('ABSMU', absmu(orig, new))\n",
        "      g = ('SD', sd(orig, new))\n",
        "      h = ('MUPOS', mupos(orig, new))\n",
        "      i = ('MUNEG', muneg(orig, new))\n",
        "      return [a,b,c,d,e,f,g,h,i]\n",
        "\n",
        "    beg_preds = []\n",
        "    for test,mem in beg:\n",
        "\n",
        "        feed = {}\n",
        "        input_df = [test, mem, 1]\n",
        "        for i, key in enumerate(INPUT_TENSORS):\n",
        "            feed[key.name] = input_df[i]\n",
        "        pred = sess.run(PRED_TENSOR,feed)\n",
        "        beg_preds.extend(pred)\n",
        "    \n",
        "    end_preds = []\n",
        "    for test,mem in end:\n",
        "\n",
        "        feed = {}\n",
        "        input_df = [test, mem, 1]\n",
        "        for i, key in enumerate(INPUT_TENSORS):\n",
        "            feed[key.name] = input_df[i]\n",
        "        pred = sess.run(PRED_TENSOR,feed)\n",
        "        end_preds.extend(pred)\n",
        "\n",
        "    beg_fin =[a+2 for a in beg_preds.flatten().tolist()]\n",
        "    end_fin =[a+2 for a in end_preds.flatten().tolist()]\n",
        "    # y_pred_fin =  [int(round(a*(range_max-range_min)+range_min)) for a in main_preds.flatten().tolist()]\n",
        "    orig_fin =   [int(a) for a in orig]\n",
        "    return (beg_fin, end_fin, orig_fin)\n",
        "    # print(, orig_fin)\n",
        "\n",
        "    # with open('/content/drive/My Drive/IG RESULTS/P'+prompt+'/SKIPFLOW/ADV/'+f_name+'_results_'+prompt+'.txt', 'w') as f:\n",
        "    #   f.write(str(get_pred_stats(orig_fin, beg_fin)))\n",
        "    #   f.write(str(get_pred_stats(orig_fin, end_fin)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs7iL6BV6K4E"
      },
      "source": [
        "def store_attr(f_name, data, prompt, n =10):\n",
        "  import os\n",
        "  import gc\n",
        "  ATTRS_TSV = './drive/My Drive/IG RESULTS/P'+prompt+'/MEMORY NET/attrs_stor_'+f_name+'.tsv'\n",
        "  ATTRS_DIR = './drive/My Drive/IG RESULTS/P'+prompt+'/MEMORY NET/'+f_name+'/'\n",
        "  if os.path.isdir(ATTRS_DIR):\n",
        "    pass\n",
        "  else:\n",
        "    os.mkdir(ATTRS_DIR)\n",
        "  with open(ATTRS_DIR+f_name+'.pkl', 'wb') as f:\n",
        "    pickle.dump(data,f)\n",
        "  batch = 1\n",
        "  with open(ATTRS_TSV, 'a') as outf:\n",
        "    c=0\n",
        "    while c<n:\n",
        "      ans = ''\n",
        "      for i,v in enumerate(data[c:c+batch]):\n",
        "          tsv_string = ''\n",
        "          attrs = get_attributions(np.array([v]))[0]\n",
        "          words,count = sequence_to_text(v)\n",
        "          assert len(words[count:]) == len(attrs[count:])\n",
        "          question_attrs = []\n",
        "          html = vis(attrs[count:], words[count:])\n",
        "          convert_html_to_pdf(html, ATTRS_DIR+str(c)+'.pdf')\n",
        "          for ind in range(len(words[count:])):\n",
        "            # print( '|'.join([ words[count:][ind], str(attrs[count:][ind]) ]))\n",
        "            if words[count:][ind] != None and str(attrs[count:][ind])!=None:\n",
        "              question_attrs.append(\n",
        "                  '|'.join([ words[count:][ind], str(attrs[count:][ind]) ])\n",
        "                  )\n",
        "          tsv_string = ['||'.join(question_attrs)]\n",
        "          ans += '\\t'.join(tsv_string) + '\\n'\n",
        "          del attrs, words, question_attrs, tsv_string\n",
        "          gc.collect()\n",
        "      c+=batch\n",
        "      outf.write(ans)\n",
        "      outf.flush()\n",
        "      del ans\n",
        "      gc.collect()\n",
        "    outf.write('done')\n",
        "  print('DONE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQvo2e7nw910",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "94eeea67-c90e-41ec-b124-60550d07dbf7"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlgmKziW6K1p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "2aa27ea6-8235-4e72-d172-a4353c4dbf65"
      },
      "source": [
        "songs_beg, songs_end = create_adv_data([t,m], 'song.pickle')\n",
        "speech_beg, speech_end = create_adv_data([t,m], 'speeches.pickle')\n",
        "truth_beg, truth_end = create_adv_data([t,m], 'un_truth.pickle')\n",
        "false_beg, false_end = create_adv_data([t,m], 'un_false.pickle')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-117-0d37cc046de5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msongs_beg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msongs_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_adv_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'song.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mspeech_beg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeech_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_adv_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'speeches.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtruth_beg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruth_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_adv_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'un_truth.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfalse_beg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfalse_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_adv_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'un_false.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-113-74af7ce0d3a2>\u001b[0m in \u001b[0;36mcreate_adv_data\u001b[0;34m(data_test, file_path)\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m           \u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m           \u001b[0mind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'tolist'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5IQLR1-6Kw-"
      },
      "source": [
        "f_names = ['song_beg', 'song_end', 'speech_beg', 'speech_end', 'truth_beg', 'truth_end', 'false_beg', 'false_end']\n",
        "datas = [songs_beg, songs_end, speech_beg, speech_end, truth_beg, truth_end,  false_beg, false_end]\n",
        "for i in range(len(f_names)):\n",
        "  store_attr(f_names[i], datas[i],prompt =str(1),n=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SA6N0-0n6KuZ"
      },
      "source": [
        "range_max =12\n",
        "range_min =2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBt2cbM16Kpa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Syy0VFIO6Kml"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwmcyxqY6Kkd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r544ukJm6Kis"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZDvAq9i6KgX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDF5gwtW6Kcw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avCCTOvBaWfv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvradoLBaWcD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsSuUxUdaWZP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}